{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00bd3040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "import os\n",
    "import cProfile\n",
    "import copy\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import subprocess\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import timm\n",
    "import pandas\n",
    "import seaborn\n",
    "import matplotlib.pyplot\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import math\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import numpy as np\n",
    "import csv\n",
    "import argparse\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import wandb\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.backbone import get_backbone\n",
    "from src.modules import CosineLinear\n",
    "from src.moe_seed import MoE_SEED\n",
    "from src.data import (\n",
    "    CILDataManager,\n",
    "    DILDataManager,\n",
    "    get_dataset,\n",
    "    DATASET_MAP,\n",
    "    make_test_transform_from_args,\n",
    "    make_train_transform_from_args,\n",
    "    update_transforms,\n",
    ")\n",
    "from src.logging import Logger, WandbLogger, ConsoleLogger, TQDMLogger\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from src.support_functions import check_gpu_memory, shrink_dataset, display_profile, log_gpustat, optimize_args\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6a5e9",
   "metadata": {},
   "source": [
    "# Important! Only one dataset at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "364f60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"vtab\"\n",
    "CUDA_VISIBLE_DEVICES = \"4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4df6d7",
   "metadata": {},
   "source": [
    "## Get dataset or class features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "762ebe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def setup_logger(args):\n",
    "    Logger.instance().add_backend(ConsoleLogger())\n",
    "    if args.wandb_project is not None:\n",
    "        Logger.instance().add_backend(\n",
    "            WandbLogger(args.wandb_project, args.wandb_entity, args)\n",
    "        )\n",
    "\n",
    "\n",
    "def update_args(args):\n",
    "    assert args.k >= 1 and args.k <= 12\n",
    "    args.intralayers = [f\"blocks.{11 - i}\" for i in range(args.k)]\n",
    "\n",
    "    args.aug_normalize = bool(args.aug_normalize)\n",
    "\n",
    "    args.target_size = 224\n",
    "    \n",
    "    dataset_T_map = {\n",
    "        \"dil_imagenetr\": {\"T\": 15, \"moe_max_experts\": 2},\n",
    "        \"limited_domainnet\": {\"T\": 6, \"moe_max_experts\": 3},\n",
    "        \"vtab\": {\"T\": 5, \"moe_max_experts\": 3},\n",
    "        \"cddb\": {\"T\": 5, \"moe_max_experts\": 3},\n",
    "    }\n",
    "\n",
    "    if args.dataset in dataset_T_map.keys():\n",
    "        args.T = dataset_T_map[args.dataset][\"T\"]\n",
    "        #args.moe_max_experts = dataset_T_map[args.dataset][\"moe_max_experts\"] #immer 5!\n",
    "        print(f\"Dataset {args.dataset} has T={args.T} and moe_max_experts={args.moe_max_experts}\")\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "def calculate_similarity(feature1, feature2, metric='cosine'):\n",
    "    \"\"\"\n",
    "    Calculates the similarity between two feature vectors.\n",
    "\n",
    "    Args:\n",
    "        feature1 (np.ndarray): First feature vector.\n",
    "        feature2 (np.ndarray): Second feature vector.\n",
    "        metric (str): The similarity metric to use ('cosine' or 'euclidean').\n",
    "\n",
    "    Returns:\n",
    "        float: The similarity score.\n",
    "    \"\"\"\n",
    "    if metric == 'cosine':\n",
    "        return cosine_similarity(feature1.reshape(1, -1), feature2.reshape(1, -1))[0][0]\n",
    "    elif metric == 'euclidean':\n",
    "        return -np.linalg.norm(feature1 - feature2) # Negative for consistency (higher value = more similar)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported similarity metric: {metric}\")\n",
    "\n",
    "def calculate_intra_class_similarity(features, labels, similarity_metric='cosine'):\n",
    "    \"\"\"\n",
    "    Calculates the average intra-class similarity for a dataset.\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): Array of feature vectors.\n",
    "        labels (np.ndarray): Array of corresponding labels.\n",
    "        similarity_metric (str): The similarity metric to use ('cosine' or 'euclidean').\n",
    "\n",
    "    Returns:\n",
    "        tuple: (dict, float) - A dictionary of per-class intra-class similarities\n",
    "               and the overall average intra-class similarity.\n",
    "    \"\"\"\n",
    "    intra_class_similarities = {}\n",
    "    unique_labels = np.unique(labels)\n",
    "    bar = tqdm(enumerate(unique_labels), desc=\"Calculating Intra-Class Similarity\", total=len(unique_labels))\n",
    "    for i, label in bar:\n",
    "        class_features = features[labels == label]\n",
    "        similarities = []\n",
    "        for i in range(len(class_features)):\n",
    "            for j in range(i + 1, len(class_features)):\n",
    "                similarity = calculate_similarity(class_features[i], class_features[j], similarity_metric)\n",
    "                similarities.append(similarity)\n",
    "        if similarities:\n",
    "            intra_class_similarities[label] = np.mean(similarities)\n",
    "        else:\n",
    "            intra_class_similarities[label] = 0.0\n",
    "\n",
    "    overall_intra_class_similarity = np.mean(list(intra_class_similarities.values())) if intra_class_similarities else 0.0\n",
    "    return intra_class_similarities, overall_intra_class_similarity\n",
    "\n",
    "def calculate_inter_class_similarity(features, labels, similarity_metric='cosine'):\n",
    "    \"\"\"\n",
    "    Calculates the average inter-class similarity for a dataset.\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): Array of feature vectors.\n",
    "        labels (np.ndarray): Array of corresponding labels.\n",
    "        similarity_metric (str): The similarity metric to use ('cosine' or 'euclidean').\n",
    "\n",
    "    Returns:\n",
    "        float: The overall average inter-class similarity.\n",
    "    \"\"\"\n",
    "    inter_class_similarities = []\n",
    "    unique_labels = np.unique(labels)\n",
    "    bar = tqdm(enumerate(unique_labels), desc=\"Calculating Inter-Class Similarity\", total=len(unique_labels))\n",
    "    # Iterate through all pairs of classes\n",
    "    for i, _ in bar:\n",
    "        for j in range(i + 1, len(unique_labels)):\n",
    "            label1 = unique_labels[i]\n",
    "            label2 = unique_labels[j]\n",
    "            features_class1 = features[labels == label1]\n",
    "            features_class2 = features[labels == label2]\n",
    "            for feat1 in features_class1:\n",
    "                for feat2 in features_class2:\n",
    "                    similarity = calculate_similarity(feat1, feat2, similarity_metric)\n",
    "                    inter_class_similarities.append(similarity)\n",
    "\n",
    "    overall_inter_class_similarity = np.mean(inter_class_similarities) if inter_class_similarities else 0.0\n",
    "    return overall_inter_class_similarity\n",
    "\n",
    "def calculate_inter_class_similarity_vectorized(features, labels, similarity_metric='cosine'):\n",
    "    \"\"\"\n",
    "    Calculates the average inter-class similarity for a dataset using vectorization.\n",
    "    \"\"\"\n",
    "    inter_class_similarities = []\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_classes = len(unique_labels)\n",
    "    bar = tqdm(range(n_classes), desc=\"Calculating Inter-Class Similarity\", total=n_classes)\n",
    "\n",
    "    for i in bar:\n",
    "        for j in range(i + 1, n_classes):\n",
    "            label1 = unique_labels[i]\n",
    "            label2 = unique_labels[j]\n",
    "            features_class1 = features[labels == label1]\n",
    "            features_class2 = features[labels == label2]\n",
    "\n",
    "            if similarity_metric == 'cosine':\n",
    "                similarity_matrix = cosine_similarity(features_class1, features_class2)\n",
    "                inter_class_similarities.extend(similarity_matrix.flatten())\n",
    "            elif similarity_metric == 'euclidean':\n",
    "                # Calculate pairwise Euclidean distances and negate for consistency\n",
    "                distances = np.linalg.norm(features_class1[:, np.newaxis, :] - features_class2[np.newaxis, :, :], axis=2)\n",
    "                inter_class_similarities.extend((-distances).flatten())\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported similarity metric: {similarity_metric}\")\n",
    "\n",
    "    overall_inter_class_similarity = np.mean(inter_class_similarities) if inter_class_similarities else 0.0\n",
    "    return overall_inter_class_similarity\n",
    "\n",
    "# Expert learned a set of classes and we want to calculate the similarity to all other classes\n",
    "def calculate_selective_inter_class_similarity(features, labels, target_classes, similarity_metric='cosine'):\n",
    "    \"\"\"\n",
    "    Calculates the average inter-class similarity between a target list of classes\n",
    "    and all other classes not in the target list, using vectorization.\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): Array of feature vectors.\n",
    "        labels (np.ndarray): Array of corresponding labels.\n",
    "        target_classes (list): List of class labels for which to compute similarity\n",
    "                               to all other classes (excluding those in this list).\n",
    "        similarity_metric (str, optional): Metric to use for similarity calculation.\n",
    "                                           'cosine' or 'euclidean' are supported.\n",
    "                                           Defaults to 'cosine'.\n",
    "\n",
    "    Returns:\n",
    "        float: The average inter-class similarity between the target classes\n",
    "               and the other classes. Returns 0.0 if no such pairs exist.\n",
    "    \"\"\"\n",
    "    inter_class_similarities = []\n",
    "    unique_labels = np.unique(labels)\n",
    "    target_classes = set(target_classes)  # Convert to set for faster lookups\n",
    "    other_classes = [label for label in unique_labels if label not in target_classes]\n",
    "\n",
    "    bar = tqdm(target_classes, desc=\"Calculating Selective Inter-Class Similarity\", total=len(target_classes))\n",
    "\n",
    "    for label1 in bar:\n",
    "        features_class1 = features[labels == label1]\n",
    "        for label2 in other_classes:\n",
    "            features_class2 = features[labels == label2]\n",
    "\n",
    "            if similarity_metric == 'cosine':\n",
    "                similarity_matrix = cosine_similarity(features_class1, features_class2)\n",
    "                inter_class_similarities.extend(similarity_matrix.flatten())\n",
    "            elif similarity_metric == 'euclidean':\n",
    "                # Calculate pairwise Euclidean distances and negate for consistency\n",
    "                distances = np.linalg.norm(features_class1[:, np.newaxis, :] - features_class2[np.newaxis, :, :], axis=2)\n",
    "                inter_class_similarities.extend((-distances).flatten())\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported similarity metric: {similarity_metric}\")\n",
    "\n",
    "    overall_inter_class_similarity = np.mean(inter_class_similarities) if inter_class_similarities else 0.0\n",
    "    return overall_inter_class_similarity\n",
    "\n",
    "\n",
    "def calculate_entropy(labels):\n",
    "    \"\"\"\n",
    "    Calculates the entropy of a list or NumPy array of labels.\n",
    "\n",
    "    Args:\n",
    "        labels (list or np.ndarray): A list or array of labels.\n",
    "\n",
    "    Returns:\n",
    "        float: The entropy of the labels.\n",
    "    \"\"\"\n",
    "\n",
    "    label_counts = Counter(labels)\n",
    "    total_samples = len(labels)\n",
    "    entropy = 0.0\n",
    "\n",
    "    for count in label_counts.values():\n",
    "        probability = count / total_samples\n",
    "        entropy -= probability * math.log2(probability)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "def calculate_feature_variance(features):\n",
    "    \"\"\"\n",
    "    Calculates the variance of the features.\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): Array of feature vectors.\n",
    "\n",
    "    Returns:\n",
    "        float: The variance of the features.\n",
    "    \"\"\"\n",
    "    return np.var(features, axis=0).mean()  # Mean variance across all features\n",
    "\n",
    "def visualize_csv_with_adjusted_size(csv_filepath, output_filepath=\"heatmap_adjusted.png\"):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filepath, index_col=0)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Fehler: Datei '{csv_filepath}' nicht gefunden.\")\n",
    "        return\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "\n",
    "    if not numeric_cols.empty:\n",
    "        df_normalized = df[numeric_cols].apply(lambda x: (x - x.min()) / (x.max() - x.min()), axis=0)\n",
    "        cmap = LinearSegmentedColormap.from_list(\"mycmap\", [\"white\", \"lightblue\", \"darkblue\"])\n",
    "\n",
    "        # Erhöhe die Figurengröße, um die Kästchen größer zu machen\n",
    "        plt.figure(figsize=(len(numeric_cols) * 2, len(df) * 1))\n",
    "\n",
    "        sns.heatmap(df_normalized, annot=False, cmap=cmap, cbar=True, yticklabels=True)\n",
    "        plt.title(\"Farbliche Visualisierung der Datenspalten\", fontsize=12) # Kleinere Schriftgröße für den Titel\n",
    "        plt.xlabel(\"Numerische Spalten\", fontsize=10) # Kleinere Schriftgröße für die X-Achse\n",
    "        plt.ylabel(\"Datensätze\", fontsize=10) # Kleinere Schriftgröße für die Y-Achse\n",
    "        plt.xticks(rotation=45, ha=\"right\", fontsize=8) # Kleinere Schriftgröße für die X-Achsenbeschriftungen\n",
    "        plt.yticks(fontsize=8) # Kleinere Schriftgröße für die Y-Achsenbeschriftungen\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_filepath)\n",
    "        print(f\"Heatmap mit angepasster Größe und Schrift gespeichert als '{output_filepath}'.\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"Keine numerischen Spalten zum Visualisieren gefunden.\")\n",
    "\n",
    "\n",
    "def get_all_dataset_metrics(train_features, train_labels, dataset_name):\n",
    "    values = [None] * 6\n",
    "    values[0] = dataset_name\n",
    "    \n",
    "    label_entropy = calculate_entropy(train_labels)\n",
    "    print(f\"Label Entropy: {label_entropy:.4f}\")\n",
    "    values[1] = label_entropy\n",
    "    feature_entropy = calculate_entropy(train_features.flatten())\n",
    "    print(f\"Feature Entropy: {feature_entropy:.4f}\")\n",
    "    values[2] = feature_entropy\n",
    "\n",
    "    # 4. Calculate intra-class similarity on the training set\n",
    "    intra_class_similarities, overall_intra_similarity = calculate_intra_class_similarity(train_features, train_labels) # Use train_labels here\n",
    "    print(f\"Intra-Class Similarities per class: {intra_class_similarities}\")\n",
    "    print(f\"Overall Intra-Class Similarity: {overall_intra_similarity:.4f}\")\n",
    "    values[5] = overall_intra_similarity\n",
    "    \n",
    "    # 5. Calculate inter-class similarity on the training set\n",
    "    overall_inter_similarity = calculate_inter_class_similarity_vectorized(train_features, train_labels) # Use train_labels here\n",
    "    print(f\"Overall Inter-Class Similarity: {overall_inter_similarity:.4f}\")\n",
    "    values[4] = overall_inter_similarity\n",
    "\n",
    "    # 6. Calculate feature variance\n",
    "    feature_variance = calculate_feature_variance(train_features)\n",
    "    print(f\"Feature Variance: {feature_variance:.4f}\")\n",
    "    values[3] = feature_variance\n",
    "\n",
    "    return values\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # get dataset and augmentations\n",
    "    train_transform = make_train_transform_from_args(args)\n",
    "    test_transform = make_test_transform_from_args(args)\n",
    "    train_base_dataset, test_base_dataset = get_dataset(\n",
    "        args.dataset, path=args.data_root\n",
    "    )\n",
    "    update_transforms(test_base_dataset, transform=test_transform)\n",
    "\n",
    "\n",
    "    # get datamanager based on ds\n",
    "    data_manager = None\n",
    "    if DILDataManager.is_dil(str(train_base_dataset)):\n",
    "        print(\"DIL\")\n",
    "        data_manager = DILDataManager(\n",
    "            train_base_dataset,\n",
    "            test_base_dataset,\n",
    "        )\n",
    "    else:\n",
    "        print(\"CIL\")\n",
    "        data_manager = CILDataManager(\n",
    "            train_base_dataset,\n",
    "            test_base_dataset,\n",
    "            T=args.T,\n",
    "            num_first_task=None if args.dataset != \"cars\" else 16,\n",
    "            shuffle=True,\n",
    "            seed=args.seed,\n",
    "        )\n",
    "\n",
    "\n",
    "    feature_extractor = timm.create_model(args.backbone, pretrained=True).to(args.device)\n",
    "    feature_extractor.head = nn.Identity()\n",
    "    feature_extractor.eval()\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    bar = tqdm(enumerate(data_manager), desc=\"Extracting Features\", total=len(data_manager))\n",
    "    for i, (train_dataset, _) in bar:\n",
    "        train_dataset.transform = train_transform\n",
    "        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "        for images, labels in train_loader:  # Iterate through all batches in the loader\n",
    "            images = images.to(args.device)\n",
    "            features = feature_extractor(images)\n",
    "            train_features.append(features.cpu().detach().numpy())\n",
    "            train_labels.append(labels.cpu().detach().numpy())\n",
    "\n",
    "    del feature_extractor\n",
    "\n",
    "    train_features = np.concatenate(train_features, axis=0)\n",
    "    train_labels = np.concatenate(train_labels, axis=0)\n",
    "    print(\"Features shape:\", train_features.shape)\n",
    "\n",
    "    return train_features, train_labels   \n",
    "\n",
    "    #values = get_all_dataset_metrics(train_features, train_labels, args.dataset)\n",
    "    \n",
    "\"\"\"\n",
    "    # Saving values\n",
    "    save_path = \"local_data/dataset_metrics.csv\"\n",
    "    with open(save_path, 'a', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(values)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def main2_classes_per_task_and_seed(args):\n",
    "    # get dataset and augmentations\n",
    "    train_transform = make_train_transform_from_args(args)\n",
    "    test_transform = make_test_transform_from_args(args)\n",
    "    train_base_dataset, test_base_dataset = get_dataset(\n",
    "        args.dataset, path=args.data_root\n",
    "    )\n",
    "    update_transforms(test_base_dataset, transform=test_transform)\n",
    "\n",
    "\n",
    "    # get datamanager based on ds\n",
    "    data_manager = None\n",
    "    if DILDataManager.is_dil(str(train_base_dataset)):\n",
    "        print(\"DIL\")\n",
    "        data_manager = DILDataManager(\n",
    "            train_base_dataset,\n",
    "            test_base_dataset,\n",
    "        )\n",
    "    else:\n",
    "        print(\"CIL\")\n",
    "        data_manager = CILDataManager(\n",
    "            train_base_dataset,\n",
    "            test_base_dataset,\n",
    "            T=args.T,\n",
    "            num_first_task=None if args.dataset != \"cars\" else 16,\n",
    "            shuffle=True,\n",
    "            seed=args.seed,\n",
    "        )\n",
    "\n",
    "\n",
    "    csv_rows = []\n",
    "    bar = tqdm(enumerate(data_manager), desc=\"Iterate over tasks\", total=len(data_manager))\n",
    "    for t, (train_dataset, _) in bar:\n",
    "        task_labels_set = set()\n",
    "\n",
    "        train_dataset.transform = train_transform\n",
    "        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "        for _, labels in train_loader:  # Iterate through all batches in the loader\n",
    "            task_labels = labels.cpu().detach().numpy()\n",
    "            for batch_label in task_labels:\n",
    "                task_labels_set.add(batch_label.item())\n",
    "        for i in list(task_labels_set):\n",
    "            row = {\n",
    "                \"dataset\": args.dataset,\n",
    "                \"task\": t,\n",
    "                \"class\": i,\n",
    "                \"seed\": args.seed\n",
    "            }\n",
    "            csv_rows.append(row)\n",
    "\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    save_path = f\"local_data/classes_per_task/{args.dataset}_{args.seed}.csv\"\n",
    "\n",
    "    # Erstelle den Ordner, falls er nicht existiert\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    with open(save_path, 'w', newline='') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=[\"dataset\", \"task\", \"class\", \"seed\"])\n",
    "        writer.writeheader()  # Füge eine Kopfzeile zur CSV-Datei hinzu\n",
    "        writer.writerows(csv_rows)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66a1a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description=\"Your script description here\")\n",
    "\n",
    "    # Define your arguments as before\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.0, help='Weight decay')\n",
    "    parser.add_argument('--early_stopping', type=int, default=10, help='Patience for early stopping')\n",
    "    parser.add_argument('--dataset', type=str, default='cifar100',\n",
    "                        choices=['cifar100', 'imagenetr', 'imageneta', 'vtab', 'cars', 'cub',\n",
    "                                 'omnibenchmark', 'dil_imagenetr', 'cddb', 'limited_domainnet'],\n",
    "                        help='Dataset to use')\n",
    "    parser.add_argument('--T', type=int, default=10, help='Number of timesteps')\n",
    "    parser.add_argument('--backbone', type=str, default='vit_base_patch16_224',\n",
    "                        choices=['vit_base_patch16_224', 'vit_base_patch16_224_in21k'],\n",
    "                        help='Backbone architecture')\n",
    "    parser.add_argument('--finetune_method', type=str, default='none',\n",
    "                        choices=['none', 'adapter', 'ssf', 'vpt'],\n",
    "                        help='Finetuning method')\n",
    "    parser.add_argument('--finetune_epochs', type=int, default=20, help='Number of finetuning epochs')\n",
    "    parser.add_argument('--k', type=int, default=5, help='Number of nearest neighbors')\n",
    "    parser.add_argument('--device', type=str, default='cuda' if 'cuda' in sys.modules else 'cpu',\n",
    "                        help='Device to use (cuda or cpu)')\n",
    "    parser.add_argument('--seed', type=int, default=2001, help='Random seed')\n",
    "    parser.add_argument('--data_root', type=str, default='./local_data', help='Root directory for datasets')\n",
    "    parser.add_argument('--moe_max_experts', type=int, default=4, help='Maximum number of experts for MoE')\n",
    "    parser.add_argument('--reduce_dataset', type=float, default=1.0, help='Fraction of dataset to use')\n",
    "    parser.add_argument('--gmms', type=int, default=8, help='Number of GMM components')\n",
    "    parser.add_argument('--use_multivariate', action='store_true', help='Use multivariate Gaussian')\n",
    "    parser.add_argument('--selection_method', type=str, default='random',\n",
    "                        choices=['random', 'around', 'eucld_dist', 'inv_eucld_dist', 'kl_div',\n",
    "                                 'inv_kl_div', 'ws_div', 'inv_ws_div'],\n",
    "                        help='Selection method')\n",
    "    parser.add_argument('--kd', action='store_true', help='Use knowledge distillation')\n",
    "    parser.add_argument('--kd_alpha', type=float, default=0.5, help='Alpha for knowledge distillation loss')\n",
    "    parser.add_argument('--log_gpustat', action='store_true', help='Log GPU statistics')\n",
    "    parser.add_argument('--sweep_logging', action='store_true', help='Enable Weights & Biases sweep logging')\n",
    "    parser.add_argument('--exit_after_T', action='store_true', help='Exit after T timesteps')\n",
    "    parser.add_argument('--selection_criterion', type=int, default=0, choices=[0, 1, 2],\n",
    "                        help='Selection criterion')\n",
    "    parser.add_argument('--tau', type=float, default=0.1, help='Temperature parameter')\n",
    "    parser.add_argument('--exit_after_acc', type=float, default=0.0, help='Exit after reaching this accuracy')\n",
    "    parser.add_argument('--trash_var', type=float, default=0.0, help='Trash variable (for testing)')\n",
    "    parser.add_argument('--use_adamw', action='store_true', help='Use AdamW optimizer')\n",
    "    parser.add_argument('--use_cosine_annealing', action='store_true', help='Use cosine annealing scheduler')\n",
    "    parser.add_argument('--aug_resize_crop_min', type=float, default=0.8, help='Min scale for random resize crop')\n",
    "    parser.add_argument('--aug_resize_crop_max', type=float, default=1.0, help='Max scale for random resize crop')\n",
    "    parser.add_argument('--aug_random_rotation_degree', type=float, default=0.0, help='Degree for random rotation')\n",
    "    parser.add_argument('--aug_brightness_jitter', type=float, default=0.0, help='Brightness jitter')\n",
    "    parser.add_argument('--aug_contrast_jitter', type=float, default=0.0, help='Contrast jitter')\n",
    "    parser.add_argument('--aug_saturation_jitter', type=float, default=0.0, help='Saturation jitter')\n",
    "    parser.add_argument('--aug_hue_jitter', type=float, default=0.0, help='Hue jitter')\n",
    "    parser.add_argument('--aug_normalize', action='store_true', help='Normalize input images')\n",
    "    parser.add_argument('--wandb_project', type=str, default='your_project_name', help='WandB project name')\n",
    "    parser.add_argument('--wandb_entity', type=str, default='your_entity_name', help='WandB entity name')\n",
    "\n",
    "    if '__file__' in globals():  # Check if running as a script\n",
    "        args = parser.parse_args()\n",
    "    else:  # Running in a Jupyter Notebook\n",
    "        args = parser.parse_args(args=[]) # Pass an empty list to avoid errors\n",
    "        # You can set default values here or handle arguments differently in the notebook\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ecd4406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 1796it [00:00, 1226069.33it/s]\n",
      "Iterate over test dataset: 8619it [00:00, 1487438.54it/s]\n",
      "Extracting Features: 100%|██████████| 5/5 [30:13<00:00, 362.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (1796, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = CUDA_VISIBLE_DEVICES\n",
    "args = parse_arguments()\n",
    "args.dataset = DATASET\n",
    "args = update_args(args)\n",
    "set_seed(args.seed)\n",
    "\n",
    "\n",
    "features, labels = main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f38ff5c",
   "metadata": {},
   "source": [
    "## Saving all features into file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd9af4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten erfolgreich als './local_data/vtab_class_features.csv' gespeichert (korrigiert).\n"
     ]
    }
   ],
   "source": [
    "dataset_name = DATASET\n",
    "labels_list = labels\n",
    "features_list = features\n",
    "\n",
    "# Stelle sicher, dass features_list eine Python-Liste ist\n",
    "if not isinstance(features_list, list):\n",
    "    features_list = features_list.tolist()  # Konvertiere zu Liste, falls es ein NumPy Array ist\n",
    "\n",
    "# Annahme: Jede innere Liste hat die gleiche Länge und entspricht einer Zeile\n",
    "# und die Elemente der inneren Liste sollen separate Spalten werden.\n",
    "\n",
    "# Erstelle Spaltennamen für die Features\n",
    "num_features = len(features_list[0]) if features_list else 0\n",
    "feature_columns = [f'feature_{i}' for i in range(num_features)]\n",
    "\n",
    "# Erstelle ein Dictionary für den DataFrame\n",
    "data = {'dataset': [dataset_name] * len(labels_list),\n",
    "        'label': [item[0] if isinstance(item, list) else item for item in labels_list]} # Annahme: Label ist das erste Element der inneren Liste\n",
    "for i, col in enumerate(feature_columns):\n",
    "    data[col] = [item[i] if isinstance(item, list) and len(item) > i else None for item in features_list]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Speichern als CSV-Datei\n",
    "csv_filename = f\"./local_data/{dataset_name}_class_features.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"Daten erfolgreich als '{csv_filename}' gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85981a9b",
   "metadata": {},
   "source": [
    "## Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "524ea74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten erfolgreich aus CSV eingelesen:\n",
      "Geladene Labels (erste 5): [0, 0, 0, 0, 0]\n",
      "Geladene Features (erste 1): [[-1.751719355583191, 0.8563099503517151, -0.8320057988166809, 0.5462210774421692, 1.6325730085372925, 0.1137448772788047, -0.5725335478782654, -1.0483394861221311, -0.0714033916592598, -1.7993239164352417, 1.2114547491073608, 0.7065345644950867, -0.8003975749015808, -2.7898194789886475, -0.0857531875371933, -3.801489353179932, 0.8702642321586609, 0.0858083814382553, 1.3374313116073608, 0.2062888741493225, 0.2373815923929214, 0.6349830031394958, 2.385110855102539, -0.3392757475376129, -1.8584578037261963, -0.5441467761993408, 0.0477000139653682, -1.7922112941741943, -3.2841243743896484, -0.3403828740119934, 0.6959256529808044, -0.9947118759155272, 1.0811444520950315, -0.7592121958732605, -0.0447749905288219, 0.4561104774475097, -0.6492897868156433, 0.1021648347377777, 1.7643967866897583, -0.9342617988586426, -0.0910230129957199, -0.755124568939209, -0.4380847513675689, -0.7504517436027527, 2.219157695770264, 0.0688575580716133, -1.267135500907898, 0.4771862328052521, -0.3354485630989074, -0.6631701588630676, -0.2598463594913482, -1.163464069366455, -0.28297820687294, 0.8602548241615295, 1.1075732707977295, 0.2794869244098663, -0.6611941456794739, 0.6557817459106445, 0.8374765515327454, 1.2381738424301147, -1.6680150032043457, 1.3554415702819824, -0.4362684190273285, -0.6869558095932007, -0.7415676116943359, 0.0368158631026744, -0.0178110562264919, 0.2966625988483429, 1.240188479423523, -0.1039311960339546, -1.34735906124115, -0.1941645890474319, -3.209688186645508, -0.4081399142742157, 0.737677276134491, -1.5817582607269287, -0.4015733897686004, 0.1878430694341659, -0.6428401470184326, -1.1978543996810913, 2.4179532527923584, 0.3406212925910949, 0.56162029504776, 0.0378829352557659, -1.607581377029419, -1.0220434665679932, 1.987467885017395, 0.7337808609008789, -0.9788032174110411, 0.2301709055900573, -1.4312433004379272, -1.045209527015686, -0.080685019493103, -1.5866174697875977, 1.0106886625289917, -1.8678487539291384, 1.1416290998458862, 0.0404153689742088, 1.8390098810195925, -0.094655655324459, 1.671552658081055, 1.251028060913086, 0.3786019086837768, -0.3073670864105224, 0.46065354347229, -0.7553377151489258, -0.2388397455215454, 1.2032127380371094, 0.364278107881546, 0.5765628218650818, 0.9686629176139832, 0.1295620948076248, 1.3669321537017822, -0.0039843949489295, -0.9282213449478148, 1.4889990091323853, 1.7775659561157229, -0.4325611293315887, 1.3105651140213013, -0.4159647822380066, 0.3461777865886688, -0.0928863435983657, 0.7467164397239685, 0.4639289677143097, 0.6114761233329773, 1.030969738960266, -0.3821469247341156, 0.5499899387359619, 0.856337308883667, -1.3555686473846436, 2.3103315830230717, -0.0965969190001487, 1.1004092693328855, -0.0429354012012481, 1.8998510837554927, 2.19573712348938, 0.42315673828125, 1.4047212600708008, -0.92881840467453, -0.9288102984428406, -0.4674763679504394, 1.5867347717285156, 0.8383987545967102, -0.8461323380470276, 0.4141838848590851, -0.0487293936312198, -1.9479682445526123, -0.6978845000267029, -3.2097909450531006, 0.5595880746841431, 1.8660492897033687, 0.5736532807350159, 2.980037212371826, 0.6593300104141235, 1.205904841423035, -0.0628500282764434, 0.4805196523666382, 0.0610143654048442, -2.463020086288452, -2.891359806060791, 0.7083101868629456, 1.9341635704040527, -1.4104503393173218, 1.49194598197937, -0.8215408325195312, -2.7516419887542725, 0.4207001328468323, 0.2926988899707794, -0.2341471910476684, 1.6634658575057983, 0.1480410546064376, 2.360957145690918, 0.4341684579849243, -1.3974920511245728, -0.4612817466259002, 0.4992426633834839, 0.6108139157295227, 0.947043240070343, 0.3579176366329193, -0.4250131249427795, -0.4759533703327179, -0.405084490776062, -1.1207247972488403, 0.7646912932395935, 0.6197969913482666, 3.388504981994629, 1.2448832988739014, -1.200063943862915, 0.9621882438659668, 2.919445753097534, -1.904590606689453, 1.481453776359558, -0.3014667928218841, -1.2269190549850464, -1.5690709352493286, 0.7084301710128784, -0.5978431105613708, 0.720142126083374, 1.0827230215072632, 0.1193436905741691, 1.065380573272705, 0.569453239440918, 0.4730844497680664, -0.8426475524902344, -1.848560333251953, 0.9311040043830872, -1.3814698457717896, -0.6098400950431824, -1.0847679376602173, -1.9779613018035889, -1.3040366172790527, -2.2183570861816406, 0.3578426241874695, 0.7406476140022278, -1.6614569425582886, 0.869093656539917, -1.9749504327774048, 0.5137449502944946, -0.3001214265823364, 0.1051730290055275, -0.5568811297416687, 0.1773917973041534, 0.0762172192335128, 0.8636611700057983, -2.029379844665528, -1.5121196508407593, 2.165014266967773, -0.3051115572452545, 0.5416852831840515, -2.587421178817749, 0.9109999537467957, 0.9316421151161194, -0.8380434513092041, 0.4786881506443023, -2.3938004970550537, -1.6642976999282837, 0.5016006231307983, -0.2148726880550384, 0.9333513379096984, 0.087784856557846, -1.1696945428848269, -1.9962522983551023, -1.2834101915359497, -2.073851585388184, -0.7540135979652405, 0.4872232377529144, 0.3555976152420044, -0.3333829939365387, -0.6653870940208435, 2.256611585617065, -0.5348938703536987, 0.165466696023941, -0.665225625038147, -0.4381663799285888, -0.1416620314121246, -0.1278797090053558, 2.432016134262085, -1.5480401515960691, 0.1427655518054962, 1.161729335784912, 0.0886466577649116, 2.4410126209259038, -2.31929087638855, -0.2939886748790741, 0.5086477994918823, 0.5464744567871094, 1.7513469457626345, 2.383461236953736, -1.1059963703155518, 1.0666462182998655, -1.2162998914718628, 0.0114762419834733, -2.6180574893951416, -0.157154768705368, 0.1214396357536315, -0.5591023564338684, -1.5326911211013794, 0.2501576542854309, -0.7523602843284607, -0.503587007522583, 1.0510902404785156, -0.5360216498374939, -1.5544638633728027, 1.4937583208084106, 0.2303742170333862, 1.0579841136932373, 0.4420861899852752, 1.671523928642273, 2.3752965927124023, 0.5920788645744324, 0.1948872059583664, -0.9809327721595764, 0.3714599609375, -0.1486532539129257, -0.9035457968711852, 1.376560091972351, 0.1775351762771606, 0.7955865859985352, 0.2088566273450851, -0.2918725907802582, 1.0929402112960815, 0.3099199533462524, -0.2877426445484161, -2.751339912414551, -1.5976814031600952, 0.0247681606560945, 1.850783348083496, 0.9580249786376952, -2.2985315322875977, -1.2619320154190063, -0.3756504654884338, -0.8529621958732605, 2.319008111953736, -0.3787899613380432, -0.460166335105896, 0.2110197991132736, -2.5459015369415283, -2.1281423568725586, -0.7509423494338989, -0.3186363577842712, 0.3780831098556518, 0.0807265788316726, 0.0186571050435304, 0.1352483034133911, 0.8788223266601562, -2.1318907737731934, 0.1771515905857086, -2.413662433624268, -2.113333225250244, -0.4858892858028412, 0.0099298320710659, -0.0271175242960453, 1.444843053817749, 1.2974765300750732, -0.4158422648906708, 0.6957132816314697, 1.5062798261642456, 0.3026999831199646, 0.1577570587396621, -0.5811618566513062, 0.122373379766941, 0.3921828269958496, 1.5436441898345947, 0.642763078212738, 0.6904830932617188, -2.568180561065674, -2.2477595806121826, -0.963879942893982, -0.2142588794231414, 0.1810473948717117, 0.2520473599433899, -1.424604058265686, 2.061264038085937, 0.3498340249061584, -0.9281357526779176, 1.1468623876571655, -0.0619279481470584, 1.3730844259262085, -3.0612540245056152, -1.4288597106933594, -1.485706806182861, -1.4095436334609983, 1.0379966497421265, 0.0174590200185775, 2.44240403175354, 1.1588913202285769, 1.0324242115020752, -0.1619539558887481, 1.066223382949829, 0.5907047390937805, 0.3521861135959625, -0.2182344496250152, 0.5519893169403076, -0.781432569026947, -0.046569850295782, 1.0489387512207031, -0.676990807056427, 1.1938358545303345, 1.5342694520950315, 2.641543388366699, -0.5897008776664734, -1.3376470804214478, -0.2974726557731628, -0.0944623127579689, 0.4840031862258911, -1.090571641921997, 0.016119772568345, -0.4030844569206238, 1.3723084926605225, -0.7886525392532349, 0.3812120854854584, 0.3381766974925995, 0.3018659055233001, -1.9542149305343628, -0.975633442401886, 0.5341514348983765, -1.7693201303482056, 0.4522375166416168, -0.5333726406097412, 0.472346156835556, -1.2813804149627686, 0.8696518540382385, 1.510693907737732, 1.7505228519439695, 0.4578604996204376, 2.133070468902588, -0.6777377128601074, 1.3915669918060305, -0.6572554111480713, 0.6568357348442078, -0.5040230751037598, -0.2998421192169189, 1.2383098602294922, -1.5263118743896484, 0.3845714032649994, 0.2992264330387115, 0.8419219255447388, -1.5249035358428955, -0.2226199358701706, 0.2775804400444031, 0.4925369024276733, -0.5492908954620361, 0.4856342375278473, 0.0594833083450794, -0.4917803406715393, 0.4304376244544983, -0.0191346351057291, 0.7773747444152832, -1.265438437461853, 1.7428418397903442, -1.6780972480773926, -0.5635985732078552, -0.4215434193611145, -1.109843373298645, -0.2834105491638183, -0.7983717322349548, -0.3854564130306244, 1.7419854402542114, -0.8012678623199463, 1.6085213422775269, -0.928341507911682, 0.6619159579277039, -0.5700334310531616, 2.138761043548584, 0.6698527336120605, -1.5384413003921509, 0.1472925394773483, -0.2155626565217971, -0.1912541389465332, 2.466892719268799, 0.1473258882761001, -0.3655657172203064, -0.0350556522607803, -0.2090041190385818, 0.5576950311660767, 0.2032262980937957, 1.369264006614685, 0.213409885764122, 0.2313133031129837, -4.263792991638184, 1.2497204542160034, -0.3483769595623016, 0.2438791096210479, 0.1064961329102516, -1.1549715995788574, -1.2395259141921997, -0.4874531924724579, -1.6117233037948608, -0.596821665763855, -2.772088050842285, -1.52181077003479, -4.193596839904785, 0.9977396130561828, 0.6577321290969849, 0.5411312580108643, 1.097604274749756, -0.4817659854888916, -1.061978816986084, 0.3819775581359863, -0.3062678873538971, -0.7099953293800354, 1.1104027032852173, 0.9582725763320924, -0.0112507343292236, 0.6766219139099121, -0.3394055068492889, 0.4651437401771545, 0.1367281973361969, -1.7467283010482788, -0.7693485021591187, -2.8120152950286865, -0.963623046875, -0.858191967010498, -0.3717065155506134, -0.8644805550575256, -0.7009015679359436, -0.3816006481647491, -0.731985867023468, -1.9647186994552608, 2.696286916732788, 0.2892232537269592, 1.277297019958496, 0.1026667729020118, 0.429971694946289, 0.630267322063446, 0.3478775024414062, -1.0356051921844482, 0.1177175715565681, 0.585521399974823, -0.4711690545082092, 0.3651013374328613, 0.4539339542388916, 0.8048982620239258, -0.7224633097648621, 0.0825319290161132, 2.1475558280944824, 1.1620477437973022, 1.1905981302261353, 0.8959715366363525, 0.6281590461730957, 1.6007423400878906, -1.4630892276763916, 0.756385862827301, 0.0356852039694786, 2.3178727626800537, 1.0401595830917358, 1.992945432662964, -0.8407227396965027, -0.7931270599365234, 0.0172490049153566, 0.1321805715560913, -1.313941240310669, 0.2515008747577667, -1.9810906648635864, 0.2139631062746048, 0.2584750354290008, 1.4033159017562866, -0.393991470336914, 0.4191663563251495, -0.2934670150279999, -0.0914164185523986, -0.8494096994400024, -0.2697931826114654, -0.9826446771621704, -0.2370230704545974, -1.63444721698761, 0.7212595343589783, 0.4588010609149933, 0.7519078850746155, 0.4646693766117096, -0.3906987011432647, -0.7351610064506531, 0.3525691330432892, 2.120518684387207, 0.1822158098220825, 0.2736320793628692, -0.4751193523406982, 0.5323856472969055, 0.2423756122589111, 0.4241683781147003, -0.2494391053915023, -0.614709198474884, 1.5549471378326416, -0.1742409467697143, -0.8136683106422424, 0.6075944900512695, -0.262218713760376, 1.310417890548706, -0.3410853743553161, 0.9178643226623536, 0.4599844813346863, -1.638994812965393, -0.6028528809547424, -2.7650065422058105, 0.8946986198425293, 1.4167823791503906, 0.2922301590442657, 0.2115995287895202, -1.0315855741500854, 0.4076806306838989, -1.1062895059585571, -1.305774211883545, 0.4319663345813751, 0.7415214776992798, -0.0966371819376945, -1.1489893198013306, -0.024805225431919, 0.172983705997467, -0.7942830324172974, 0.0803748592734336, 0.2009368687868118, 1.6939655542373655, -0.5862147808074951, -1.97058367729187, 0.4956579804420471, 1.2451589107513428, -0.1044288650155067, -1.1908355951309204, -1.5572924613952637, 0.6435168981552124, -0.4002368748188019, -0.932839810848236, -1.6803056001663208, -0.6115517616271973, 1.1798701286315918, -0.5712175965309143, -1.6231768131256104, -0.1756744682788849, -3.8194563388824463, -0.662738561630249, -1.728887915611267, -3.1890134811401367, 2.004185914993286, -2.4539856910705566, 0.6194909811019897, -0.7806270122528076, -1.9046765565872192, -0.1270463019609451, -1.473031759262085, 0.1149976029992103, -0.763627290725708, 0.8808422088623047, 0.6245744228363037, -1.2013615369796753, 1.638745903968811, 0.6318548321723938, -0.5406637787818909, 0.1233567893505096, 1.2924593687057495, -0.4839100539684295, 0.5580573678016663, 1.0238533020019531, 0.527561366558075, -0.342305451631546, 3.0095276832580566, 0.8545624613761902, -0.5678433775901794, 1.11724853515625, 0.1057108864188194, 0.1876656413078308, 0.6429374814033508, -1.2350484132766724, 2.2377612590789795, 0.4316655993461609, -1.9832162857055664, 1.031989574432373, -0.4039466381072998, -0.9549030661582948, -0.044831920415163, 0.2595496773719787, -2.130555391311645, -0.3702075481414795, 1.1292215585708618, -1.5307244062423706, 0.0144836409017443, 1.7581779956817627, -1.059017539024353, -2.47363018989563, -2.329648494720459, -1.234566330909729, -2.3936734199523926, 0.7173207402229309, 0.8098627924919128, 2.1170217990875244, -3.4253060817718506, -1.929698348045349, 1.422194242477417, -0.4541375041007995, 1.4483531713485718, -0.5968967080116272, 0.1072780266404151, -0.8615095615386963, 0.4450051486492157, -3.2978432178497314, -2.6036508083343506, -1.0256186723709106, -3.790482521057129, -2.1279869079589844, 0.0591164454817771, -2.01678729057312, -1.3756123781204224, -0.3472175598144531, 1.0392944812774658, 1.4912662506103516, -0.5751866698265076, 0.4027765989303589, 1.3443591594696045, -0.7943858504295349, -0.8810811042785645, -2.001571655273437, -0.1573140025138855, -3.259233236312866, -1.2961713075637815, 0.4906704425811767, 1.8144294023513796, -3.308499336242676, 1.30592143535614, 0.134756326675415, 0.044849444180727, -0.1111035719513893, -0.8783352971076965, -1.199202537536621, -0.4473453462123871, -1.0768476724624634, 0.7010361552238464, 0.3071061074733734, -2.418158531188965, -0.375434398651123, 0.9625231623649596, -0.7843344807624817, -0.8579762578010559, -0.8300339579582214, -1.0448791980743408, 1.0217205286026, 1.8432034254074097, 0.9889528155326844, -0.9956023097038268, 0.3941071331501007, -0.8459734916687012, 1.0435144901275637, 0.9587594270706176, 1.1023461818695068, -0.5163640379905701, 0.354662150144577, 3.3859755992889404, 0.7959691882133484, 0.4291052520275116, -0.7101693749427795, 0.1305339336395263, -0.943549871444702, -0.5715420842170715, -2.8615622520446777, -0.3206324577331543, -0.0212271604686975, 1.1718872785568235, -0.0489786714315414, -0.0466795228421688, 0.6994212865829468, -1.0526596307754517, -0.7704136967658997, -0.5481773614883423, -1.0865193605422974, 0.7872155904769897, 1.3237098455429075, 1.3751981258392334, -2.9293112754821777, 1.0131027698516846, 1.3154375553131104, 2.222127676010132, 0.9524965882301332, 3.319255352020264, 0.7447304725646973, 0.7786521911621094, -0.5390214323997498, 1.6717243194580078, 1.37895929813385, 0.5603739023208618, -1.5508304834365845, -0.8777104616165161, -1.7213263511657717, 1.430957317352295, -2.5045182704925537, 1.1972932815551758, 2.037626028060913, -1.04143488407135, 2.8528735637664795, 0.3378118574619293, -0.7240778207778931]]\n"
     ]
    }
   ],
   "source": [
    "csv_filename = f\"./local_data/{DATASET}_class_features.csv\"\n",
    "\n",
    "# Lese die CSV-Datei ein\n",
    "df_loaded = pd.read_csv(csv_filename)\n",
    "\n",
    "# Extrahiere die Labels und Features\n",
    "loaded_labels = df_loaded['label'].tolist()\n",
    "\n",
    "# Wenn die Features als separate Spalten gespeichert wurden (Szenario 1):\n",
    "if 'feature_0' in df_loaded.columns:\n",
    "    loaded_features = []\n",
    "    i = 0\n",
    "    while f'feature_{i}' in df_loaded.columns:\n",
    "        loaded_features.append(df_loaded[f'feature_{i}'].tolist())\n",
    "        i += 1\n",
    "    # Transponiere die Liste von Listen, um die ursprüngliche Struktur wiederherzustellen\n",
    "    loaded_features = list(zip(*loaded_features))\n",
    "    loaded_features = [list(item) for item in loaded_features]\n",
    "\n",
    "# Wenn die Features als Listen in einer Zelle gespeichert wurden (Szenario 2):\n",
    "elif 'feature' in df_loaded.columns:\n",
    "    loaded_features = [ast.literal_eval(item) for item in df_loaded['feature'].tolist()]\n",
    "\n",
    "print(\"Daten erfolgreich aus CSV eingelesen:\")\n",
    "print(\"Geladene Labels (erste 5):\", loaded_labels[:5])\n",
    "print(\"Geladene Features (erste 1):\", loaded_features[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde39fe",
   "metadata": {},
   "source": [
    "## What are the classes per task?\n",
    "Ich brauche eine übersetzung.  \n",
    "Zusätzlich: Was ist mit den Seeds? Classenreihenfolge ist seed abhängig! Seed der runs berücksichtigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42e62d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: imagenetr, seed: 2000\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 24000it [00:00, 1155455.65it/s]\n",
      "Iterate over test dataset: 6000it [00:00, 1620987.05it/s]\n",
      "Iterate over tasks: 100%|██████████| 10/10 [02:10<00:00, 13.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: imagenetr, seed: 2001\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 24000it [00:00, 1149964.54it/s]\n",
      "Iterate over test dataset: 6000it [00:00, 1147866.45it/s]\n",
      "Iterate over tasks: 100%|██████████| 10/10 [02:09<00:00, 12.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: imagenetr, seed: 2002\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 24000it [00:00, 1155177.20it/s]\n",
      "Iterate over test dataset: 6000it [00:00, 1594691.34it/s]\n",
      "Iterate over tasks:  30%|███       | 3/10 [00:36<01:25, 12.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m args \u001b[38;5;241m=\u001b[39m update_args(args)\n\u001b[1;32m     23\u001b[0m set_seed(args\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mmain2_classes_per_task_and_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mmain2_classes_per_task_and_seed\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    389\u001b[0m train_dataset\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m train_transform\n\u001b[1;32m    390\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:  \u001b[38;5;66;03m# Iterate through all batches in the loader\u001b[39;00m\n\u001b[1;32m    392\u001b[0m     task_labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_label \u001b[38;5;129;01min\u001b[39;00m task_labels:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1283\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1283\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1285\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "args = parse_arguments()\n",
    "\n",
    "\n",
    "seeds = [2000, 2001, 2002, 2003, 2004]\n",
    "datasets = [\"imagenetr\", \"cifar100\", \"cub\", \"dil_imagenetr\", \"imageneta\", \"cars\", \"omnibenchmark\", \"limited_domainnet\"]\n",
    "\n",
    "#set_seed(args.seed)\n",
    "\n",
    "\n",
    "#main2_classes_per_task_and_seed(args)\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    DATASET = dataset\n",
    "    for seed in seeds:\n",
    "        print(f\"Dataset: {dataset}, seed: {seed}\")\n",
    "        args.seed = seed\n",
    "        args.dataset = dataset\n",
    "        args.device = \"cpu\"\n",
    "        args = update_args(args)\n",
    "\n",
    "        set_seed(args.seed)\n",
    "\n",
    "        main2_classes_per_task_and_seed(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8006742",
   "metadata": {},
   "source": [
    "## Which expert learned which class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71861470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /export/home/0schindl/.netrc\n"
     ]
    }
   ],
   "source": [
    "wandb.login(key=\"8a88a8c49d1c2d31b8677fe0b8eb7d3e3a031f83\")\n",
    "api = wandb.Api()\n",
    "\n",
    "\n",
    "def get_expert_distribution(run):\n",
    "    if run.state != \"finished\":\n",
    "        return None\n",
    "\n",
    "    history = run.history()\n",
    "    \n",
    "    expert_distributions = dict()\n",
    "    ft_tasks = [None] * 1000\n",
    "    ft_buffer = run.config.get(\"moe_max_experts\")\n",
    "    for line in run.history().columns:\n",
    "        if line.startswith(\"Expert\") and line.endswith(\"learned task\"):\n",
    "            \n",
    "            line_splited = line.split(\" \")\n",
    "            expert = int(line_splited[1])\n",
    "            tasks = history[line].dropna().tolist()\n",
    "            tasks = [int(task) for task in tasks]\n",
    "            \n",
    "            if expert not in expert_distributions:\n",
    "                expert_distributions[expert] = list()\n",
    "            expert_distributions[expert].extend(tasks) \n",
    "\n",
    "            \n",
    "            for i in tasks:\n",
    "                if i >= ft_buffer:\n",
    "                    ft_tasks[i - ft_buffer] = expert    \n",
    "\n",
    "    # cleaning ft_tasks\n",
    "    ft_tasks = [i for i in ft_tasks if i is not None]\n",
    "\n",
    "    return ft_tasks, expert_distributions\n",
    "\n",
    "def get_sweep_data(runs, attributes_config=[], attributes_summary=[], include_run_id=True, class_similarity=False):\n",
    "    sweep_data = []\n",
    "    for run in runs:\n",
    "        config = run.config\n",
    "        summary = run.summary\n",
    "\n",
    "        if summary.get(\"task_mean/acc\") is not None and run.state == \"finished\":\n",
    "            run_data = dict()\n",
    "            if include_run_id:\n",
    "                run_data[\"run_id\"] = run.id\n",
    "            # Add the config attributes to the run_data dictionary\n",
    "            for attr in attributes_config:\n",
    "                run_data[attr] = config.get(attr)\n",
    "\n",
    "            # Add the summary attributes to the run_data dictionary\n",
    "            for attr in attributes_summary:\n",
    "                run_data[attr] = summary.get(attr)\n",
    "            \n",
    "            # average class similarity of learned classes per expert\n",
    "            if class_similarity and run_data[\"dataset\"] == DATASET:\n",
    "                _, task_distribution = get_expert_distribution(run)\n",
    "\n",
    "                # Calculate inter-class similarity for each expert\n",
    "                expert_similaritys = list()\n",
    "\n",
    "                # Tasks are not classes!!\n",
    "\n",
    "                for expert_id, learned_classes in task_distribution.items():\n",
    "                    if len(learned_classes) > 1:\n",
    "                        label_mask = np.isin(loaded_labels, learned_classes)\n",
    "\n",
    "                        # Verwende die Maske, um die entsprechenden Features und Labels auszuwählen\n",
    "                        expert_features = loaded_features[label_mask]\n",
    "                        expert_labels = loaded_labels[label_mask]\n",
    "                        print(learned_classes)\n",
    "                        print(expert_features.shape)\n",
    "                        similarity = calculate_inter_class_similarity_vectorized(expert_features, expert_labels)\n",
    "                        print(f\"Expert {expert_id} - Inter-Class Similarity: {similarity:.4f}\")\n",
    "                        expert_similaritys.append(similarity)\n",
    "\n",
    "                average_similarity = np.mean(expert_similaritys) if expert_similaritys else 0.0       \n",
    "                run_data[\"average_expert_similarity\"] = average_similarity\n",
    "                print(f\"Average Expert Similarity: {average_similarity:.4f}\")\n",
    "                print(task_distribution)\n",
    "\n",
    "\n",
    "                # inter-class similarity to all classes except the one where their expert only learned this one class\n",
    "                lonly_learned_classes = []\n",
    "                for expert_id, learned_classes in task_distribution.items():\n",
    "                    if len(learned_classes) == 1:\n",
    "                        lonly_learned_classes.append(learned_classes[0])\n",
    "                print(f\"Classes where expert only learned this class: {lonly_learned_classes}\")\n",
    "\n",
    "                # Calculate inter-class similarity to all other classes\n",
    "                other_classes = [label for label in loaded_labels if label != lonly_learned_classes]\n",
    "                label_mask = np.isin(loaded_labels, other_classes)\n",
    "\n",
    "                expert_features = loaded_features[label_mask]\n",
    "                expert_labels = loaded_labels[label_mask]\n",
    "                similarity = calculate_inter_class_similarity_vectorized(expert_features, expert_labels)\n",
    "                run_data[\"filtered_dataset_similarity\"] = similarity\n",
    "                print(f\"Dataset similarity: {similarity:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            sweep_data.append(run_data)\n",
    "\n",
    "    return sweep_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a625922",
   "metadata": {},
   "source": [
    "## Average similarity per expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1dd1b3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Expert Similarity: 0.0000\n",
      "{2: [2], 4: [4], 0: [0], 3: [3], 1: [1]}\n",
      "Average Expert Similarity: 0.0000\n",
      "{3: [3], 0: [0], 2: [2], 4: [4], 1: [1]}\n",
      "Average Expert Similarity: 0.0000\n",
      "{1: [1], 3: [3], 0: [0], 4: [4], 2: [2]}\n",
      "Average Expert Similarity: 0.0000\n",
      "{3: [3], 4: [4], 0: [0], 1: [1], 2: [2]}\n",
      "Average Expert Similarity: 0.0000\n",
      "{1: [1], 2: [2], 3: [3], 4: [4], 0: [0]}\n"
     ]
    }
   ],
   "source": [
    "_42_adapter_performance = [\n",
    "    \"belaschindler-university-hamburg/0schindl-LayUp_sweeps_question1_results/6kim8tiu\", # DIL\n",
    "    \"belaschindler-university-hamburg/0schindl-LayUp_sweeps_question1_results/jdpa9z1x\", # Cars\n",
    "    \"belaschindler-university-hamburg/0schindl-LayUp_sweeps_question1_results/cjddpel4\", # Imagenet-a\n",
    "    \"belaschindler-university-hamburg/0schindl-LayUp_sweeps_question1_results/hxigp6ck\", # Imagenet-r\n",
    "    \"belaschindler-university-hamburg/0schindl-LayUp_sweeps_question1_results/p7zmthx9\", # CIL\n",
    "    ]\n",
    "datsets_in_CIL = [\"cifar100\", \"cub\", \"vtab\", \"omnibenchmark\"]\n",
    "\n",
    "loaded_labels = np.array(loaded_labels)\n",
    "loaded_features = np.array(loaded_features)\n",
    "\n",
    "table_421 = []\n",
    "for i, s in enumerate(_42_adapter_performance):\n",
    "    sweep = api.sweep(s)\n",
    "    runs = sweep.runs\n",
    "\n",
    "    attributes_config = [\"dataset\", \"selection_method\", \"seed\"]\n",
    "    attributes_summary = [\"task_mean/acc\"]\n",
    "\n",
    "    data = get_sweep_data(runs, attributes_config, attributes_summary, class_similarity=True)\n",
    "    for e in data:\n",
    "        if e[\"dataset\"] == DATASET:\n",
    "            if i == 4:\n",
    "                # CIL\n",
    "                if e[\"dataset\"] in datsets_in_CIL:\n",
    "                    table_421.append(e)\n",
    "            else:\n",
    "                table_421.append(e)\n",
    "    \n",
    "\n",
    "\n",
    "df_sweep = pd.DataFrame(table_421)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "001e63bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     run_id dataset selection_method  seed  task_mean/acc  \\\n",
      "0  u620gh0q    vtab       inv_ws_div  2001       0.783650   \n",
      "1  roof165o    vtab       inv_ws_div  2004       0.779962   \n",
      "2  ypg2gd39    vtab       inv_ws_div  2003       0.778159   \n",
      "3  12gkkkie    vtab       inv_ws_div  2002       0.793482   \n",
      "4  3dyl6fjx    vtab       inv_ws_div  2000       0.776197   \n",
      "\n",
      "   average_expert_similarity  \n",
      "0                        0.0  \n",
      "1                        0.0  \n",
      "2                        0.0  \n",
      "3                        0.0  \n",
      "4                        0.0  \n"
     ]
    }
   ],
   "source": [
    "data = df_sweep\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da0abdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
