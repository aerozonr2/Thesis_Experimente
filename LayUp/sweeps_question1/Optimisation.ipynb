{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation\n",
    "[\"cifar100\", \"imagenetr\", \"cub\", \"dil_imagenetr\", \"imageneta\", \"vtab\", \"cars\", \"omnibenchmark\", \"limited_domainnet\", \"cddb\"]  \n",
    "Runtimes: https://wandb.ai/belaschindler-university-hamburg/Thesis_Experimente-LayUp_sweeps_question1/sweeps/13nxga69/table\n",
    "## To Do's:\n",
    "1. Runtime optimisation\n",
    "2. Vergleich von datasets, kann man überall die gleiche lr und ft-epochs benutzen. Wie ist das in den Vergleichspapern?\n",
    "3. Optimieren: Alle gleichzeitig, oder jeden einzelnen, siehe 2. \n",
    "4. Ist es richtig, dass Vtab's Train dataset viel kleiner ist, als das Test dataset? Nein muss angepasst werden.\n",
    "\n",
    "|               | cifar100 | imagenetr | cub | dil_imagenetr | imageneta | vtab | cars | omnibenchmark | limited_domainnet | cddb |\n",
    "|---------------|----------|-----------|-----|--------------|-----------|------|------|--------------|-------------------|------|\n",
    "| T0 Size | Train: 5000, Test: 1000 | Train: 2804, Test: 689 | Train: 946, Test: 247 | Train: 3704, Test: 930 | Train: 705, Test: 175 | Train: 91, Test: 286 | Train: 685, Test: 672 | Train: 8947, Test: 600 | dauert Ewigkeiten | cddb |\n",
    "5. Interpretability\n",
    "6. Dataset caracteristics for selection process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\n",
    "Runtimes: https://wandb.ai/belaschindler-university-hamburg/Thesis_Experimente-LayUp_sweeps_question1/sweeps/13nxga69/table  \n",
    "Ich nehme vtab, weil es kurz und schon etwas optimiert ist.  \n",
    "1. Einmal mit cprofile laufen lassen: \n",
    "    - vtab3.prof ist mit sellection=kl_div. Forward ist langsam.\n",
    "    - lineprofiler wird benutzt um forward weiter zu untersuchen: main.py.lprof\n",
    "    - Was sind die Bottlenecks??\n",
    "2. Evaluation der Tasks dauert immer länger und länger, obwohl das eigentlich immer gleich lange laufen sollte. Logs in run: oo62n9iz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. \n",
    "\n",
    "LayUp:  \n",
    "|               | cifar100 | imagenetr | cub | dil_imagenetr | imageneta | vtab | cars | omnibenchmark | limited_domainnet | cddb |\n",
    "|---------------|----------|-----------|-----|--------------|-----------|------|------|--------------|-------------------|------|\n",
    "| T | 10 | 10 | 10 | 15 | 10 | 5 | 10 | 10 | 15 | 5 |\n",
    "\n",
    "#### Was ist eine gute lr und ft-epochs für alle?\n",
    "- vtab \n",
    "- imagenet a\n",
    "- cub \n",
    "- größerer Datensatz! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\n",
    "- PCA bei create distr.: Dimensionality Reduction (Strongly Recommended): Apply PCA or another dimensionality reduction technique to class_features before fitting the GMM. Reducing d from 768 to a much smaller number (e.g., 64, 128, or even 32) will make the GMM fitting much more statistically stable and robust, given the small number of samples n. This is a very common technique when using generative models on high-dimensional features. You would fit the PCA on the features of all classes for a given task/expert, then transform each class's features before GMM fitting.\n",
    "### Prameter:\n",
    "Bayes:\n",
    "- 1.: Lr\n",
    "- x.: kd_alpha (0.0==False)\n",
    "- x.: tau\n",
    "\n",
    "Grid:\n",
    "- x.: finetune_method\n",
    "- 2.: selection_method/criterium \n",
    "- 3.: Adding flipped features in create distr.  \n",
    "- 3.: optimizer and lr-sceduler\n",
    "- 3.: Backbone\n",
    "- x.: use_multivariate (manche datensäte sind anfälliger für Epsilonänderungen + könnte mit flipped features zusammenhängen)\n",
    "\n",
    "\n",
    "|               | cifar100 | imagenetr | cub | dil_imagenetr | imageneta | vtab | cars | omnibenchmark | limited_domainnet | cddb |\n",
    "|---------------|----------|-----------|-----|---------------|-----------|------|------|---------------|-------------------|------|\n",
    "|     Lr-vpt    | cifar100 | imagenetr | cub | dil_imagenetr | imageneta | vtab | cars | omnibenchmark | limited_domainnet | cddb |\n",
    "|     Lr-ssf    | cifar100 | imagenetr | cub | dil_imagenetr | imageneta | vtab | cars | omnibenchmark | limited_domainnet | cddb |\n",
    "|     Lr-ada    | cifar100 | imagenetr | cub | dil_imagenetr | imageneta | vtab | cars | omnibenchmark | limited_domainnet | cddb |\n",
    "|       x       | cifar100 | imagenetr | cub | dil_imagenetr | imageneta | vtab | cars | omnibenchmark | limited_domainnet | cddb |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cifar100\n",
    "\n",
    "1. ssf: jin7cy7a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
