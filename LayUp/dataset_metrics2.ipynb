{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00bd3040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "import os\n",
    "import cProfile\n",
    "import copy\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import subprocess\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import timm\n",
    "import pandas\n",
    "import seaborn\n",
    "import matplotlib.pyplot\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import math\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import numpy as np\n",
    "import csv\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import wandb\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.backbone import get_backbone\n",
    "from src.modules import CosineLinear\n",
    "from src.moe_seed import MoE_SEED\n",
    "from src.data import (\n",
    "    CILDataManager,\n",
    "    DILDataManager,\n",
    "    get_dataset,\n",
    "    DATASET_MAP,\n",
    "    make_test_transform_from_args,\n",
    "    make_train_transform_from_args,\n",
    "    update_transforms,\n",
    ")\n",
    "from src.logging import Logger, WandbLogger, ConsoleLogger, TQDMLogger\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from src.support_functions import check_gpu_memory, shrink_dataset, display_profile, log_gpustat, optimize_args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "762ebe77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    # Saving values\\n    save_path = \"local_data/dataset_metrics.csv\"\\n    with open(save_path, \\'a\\', newline=\\'\\') as outfile:\\n        writer = csv.writer(outfile)\\n        writer.writerow(values)\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def setup_logger(args):\n",
    "    Logger.instance().add_backend(ConsoleLogger())\n",
    "    if args.wandb_project is not None:\n",
    "        Logger.instance().add_backend(\n",
    "            WandbLogger(args.wandb_project, args.wandb_entity, args)\n",
    "        )\n",
    "\n",
    "\n",
    "def update_args(args):\n",
    "    assert args.k >= 1 and args.k <= 12\n",
    "    args.intralayers = [f\"blocks.{11 - i}\" for i in range(args.k)]\n",
    "\n",
    "    args.aug_normalize = bool(args.aug_normalize)\n",
    "\n",
    "    args.target_size = 224\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "def calculate_similarity(feature1, feature2, metric='cosine'):\n",
    "    \"\"\"\n",
    "    Calculates the similarity between two feature vectors.\n",
    "\n",
    "    Args:\n",
    "        feature1 (np.ndarray): First feature vector.\n",
    "        feature2 (np.ndarray): Second feature vector.\n",
    "        metric (str): The similarity metric to use ('cosine' or 'euclidean').\n",
    "\n",
    "    Returns:\n",
    "        float: The similarity score.\n",
    "    \"\"\"\n",
    "    if metric == 'cosine':\n",
    "        return cosine_similarity(feature1.reshape(1, -1), feature2.reshape(1, -1))[0][0]\n",
    "    elif metric == 'euclidean':\n",
    "        return -np.linalg.norm(feature1 - feature2) # Negative for consistency (higher value = more similar)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported similarity metric: {metric}\")\n",
    "\n",
    "def calculate_intra_class_similarity(features, labels, similarity_metric='cosine'):\n",
    "    \"\"\"\n",
    "    Calculates the average intra-class similarity for a dataset.\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): Array of feature vectors.\n",
    "        labels (np.ndarray): Array of corresponding labels.\n",
    "        similarity_metric (str): The similarity metric to use ('cosine' or 'euclidean').\n",
    "\n",
    "    Returns:\n",
    "        tuple: (dict, float) - A dictionary of per-class intra-class similarities\n",
    "               and the overall average intra-class similarity.\n",
    "    \"\"\"\n",
    "    intra_class_similarities = {}\n",
    "    unique_labels = np.unique(labels)\n",
    "    bar = tqdm(enumerate(unique_labels), desc=\"Calculating Intra-Class Similarity\", total=len(unique_labels))\n",
    "    for i, label in bar:\n",
    "        class_features = features[labels == label]\n",
    "        similarities = []\n",
    "        for i in range(len(class_features)):\n",
    "            for j in range(i + 1, len(class_features)):\n",
    "                similarity = calculate_similarity(class_features[i], class_features[j], similarity_metric)\n",
    "                similarities.append(similarity)\n",
    "        if similarities:\n",
    "            intra_class_similarities[label] = np.mean(similarities)\n",
    "        else:\n",
    "            intra_class_similarities[label] = 0.0\n",
    "\n",
    "    overall_intra_class_similarity = np.mean(list(intra_class_similarities.values())) if intra_class_similarities else 0.0\n",
    "    return intra_class_similarities, overall_intra_class_similarity\n",
    "\n",
    "def calculate_inter_class_similarity(features, labels, similarity_metric='cosine'):\n",
    "    \"\"\"\n",
    "    Calculates the average inter-class similarity for a dataset.\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): Array of feature vectors.\n",
    "        labels (np.ndarray): Array of corresponding labels.\n",
    "        similarity_metric (str): The similarity metric to use ('cosine' or 'euclidean').\n",
    "\n",
    "    Returns:\n",
    "        float: The overall average inter-class similarity.\n",
    "    \"\"\"\n",
    "    inter_class_similarities = []\n",
    "    unique_labels = np.unique(labels)\n",
    "    bar = tqdm(enumerate(unique_labels), desc=\"Calculating Inter-Class Similarity\", total=len(unique_labels))\n",
    "    # Iterate through all pairs of classes\n",
    "    for i, _ in bar:\n",
    "        for j in range(i + 1, len(unique_labels)):\n",
    "            label1 = unique_labels[i]\n",
    "            label2 = unique_labels[j]\n",
    "            features_class1 = features[labels == label1]\n",
    "            features_class2 = features[labels == label2]\n",
    "            for feat1 in features_class1:\n",
    "                for feat2 in features_class2:\n",
    "                    similarity = calculate_similarity(feat1, feat2, similarity_metric)\n",
    "                    inter_class_similarities.append(similarity)\n",
    "\n",
    "    overall_inter_class_similarity = np.mean(inter_class_similarities) if inter_class_similarities else 0.0\n",
    "    return overall_inter_class_similarity\n",
    "\n",
    "def calculate_inter_class_similarity_vectorized(features, labels, similarity_metric='cosine'):\n",
    "    \"\"\"\n",
    "    Calculates the average inter-class similarity for a dataset using vectorization.\n",
    "    \"\"\"\n",
    "    inter_class_similarities = []\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_classes = len(unique_labels)\n",
    "    bar = tqdm(range(n_classes), desc=\"Calculating Inter-Class Similarity\", total=n_classes)\n",
    "\n",
    "    for i in bar:\n",
    "        for j in range(i + 1, n_classes):\n",
    "            label1 = unique_labels[i]\n",
    "            label2 = unique_labels[j]\n",
    "            features_class1 = features[labels == label1]\n",
    "            features_class2 = features[labels == label2]\n",
    "\n",
    "            if similarity_metric == 'cosine':\n",
    "                similarity_matrix = cosine_similarity(features_class1, features_class2)\n",
    "                inter_class_similarities.extend(similarity_matrix.flatten())\n",
    "            elif similarity_metric == 'euclidean':\n",
    "                # Calculate pairwise Euclidean distances and negate for consistency\n",
    "                distances = np.linalg.norm(features_class1[:, np.newaxis, :] - features_class2[np.newaxis, :, :], axis=2)\n",
    "                inter_class_similarities.extend((-distances).flatten())\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported similarity metric: {similarity_metric}\")\n",
    "\n",
    "    overall_inter_class_similarity = np.mean(inter_class_similarities) if inter_class_similarities else 0.0\n",
    "    return overall_inter_class_similarity\n",
    "\n",
    "# Expert learned a set of classes and we want to calculate the similarity to all other classes\n",
    "def calculate_selective_inter_class_similarity(features, labels, target_classes, similarity_metric='cosine'):\n",
    "    \"\"\"\n",
    "    Calculates the average inter-class similarity between a target list of classes\n",
    "    and all other classes not in the target list, using vectorization.\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): Array of feature vectors.\n",
    "        labels (np.ndarray): Array of corresponding labels.\n",
    "        target_classes (list): List of class labels for which to compute similarity\n",
    "                               to all other classes (excluding those in this list).\n",
    "        similarity_metric (str, optional): Metric to use for similarity calculation.\n",
    "                                           'cosine' or 'euclidean' are supported.\n",
    "                                           Defaults to 'cosine'.\n",
    "\n",
    "    Returns:\n",
    "        float: The average inter-class similarity between the target classes\n",
    "               and the other classes. Returns 0.0 if no such pairs exist.\n",
    "    \"\"\"\n",
    "    inter_class_similarities = []\n",
    "    unique_labels = np.unique(labels)\n",
    "    target_classes = set(target_classes)  # Convert to set for faster lookups\n",
    "    other_classes = [label for label in unique_labels if label not in target_classes]\n",
    "\n",
    "    bar = tqdm(target_classes, desc=\"Calculating Selective Inter-Class Similarity\", total=len(target_classes))\n",
    "\n",
    "    for label1 in bar:\n",
    "        features_class1 = features[labels == label1]\n",
    "        for label2 in other_classes:\n",
    "            features_class2 = features[labels == label2]\n",
    "\n",
    "            if similarity_metric == 'cosine':\n",
    "                similarity_matrix = cosine_similarity(features_class1, features_class2)\n",
    "                inter_class_similarities.extend(similarity_matrix.flatten())\n",
    "            elif similarity_metric == 'euclidean':\n",
    "                # Calculate pairwise Euclidean distances and negate for consistency\n",
    "                distances = np.linalg.norm(features_class1[:, np.newaxis, :] - features_class2[np.newaxis, :, :], axis=2)\n",
    "                inter_class_similarities.extend((-distances).flatten())\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported similarity metric: {similarity_metric}\")\n",
    "\n",
    "    overall_inter_class_similarity = np.mean(inter_class_similarities) if inter_class_similarities else 0.0\n",
    "    return overall_inter_class_similarity\n",
    "\n",
    "\n",
    "def calculate_entropy(labels):\n",
    "    \"\"\"\n",
    "    Calculates the entropy of a list or NumPy array of labels.\n",
    "\n",
    "    Args:\n",
    "        labels (list or np.ndarray): A list or array of labels.\n",
    "\n",
    "    Returns:\n",
    "        float: The entropy of the labels.\n",
    "    \"\"\"\n",
    "\n",
    "    label_counts = Counter(labels)\n",
    "    total_samples = len(labels)\n",
    "    entropy = 0.0\n",
    "\n",
    "    for count in label_counts.values():\n",
    "        probability = count / total_samples\n",
    "        entropy -= probability * math.log2(probability)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "def calculate_feature_variance(features):\n",
    "    \"\"\"\n",
    "    Calculates the variance of the features.\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): Array of feature vectors.\n",
    "\n",
    "    Returns:\n",
    "        float: The variance of the features.\n",
    "    \"\"\"\n",
    "    return np.var(features, axis=0).mean()  # Mean variance across all features\n",
    "\n",
    "def visualize_csv_with_adjusted_size(csv_filepath, output_filepath=\"heatmap_adjusted.png\"):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filepath, index_col=0)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Fehler: Datei '{csv_filepath}' nicht gefunden.\")\n",
    "        return\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "\n",
    "    if not numeric_cols.empty:\n",
    "        df_normalized = df[numeric_cols].apply(lambda x: (x - x.min()) / (x.max() - x.min()), axis=0)\n",
    "        cmap = LinearSegmentedColormap.from_list(\"mycmap\", [\"white\", \"lightblue\", \"darkblue\"])\n",
    "\n",
    "        # Erhöhe die Figurengröße, um die Kästchen größer zu machen\n",
    "        plt.figure(figsize=(len(numeric_cols) * 2, len(df) * 1))\n",
    "\n",
    "        sns.heatmap(df_normalized, annot=False, cmap=cmap, cbar=True, yticklabels=True)\n",
    "        plt.title(\"Farbliche Visualisierung der Datenspalten\", fontsize=12) # Kleinere Schriftgröße für den Titel\n",
    "        plt.xlabel(\"Numerische Spalten\", fontsize=10) # Kleinere Schriftgröße für die X-Achse\n",
    "        plt.ylabel(\"Datensätze\", fontsize=10) # Kleinere Schriftgröße für die Y-Achse\n",
    "        plt.xticks(rotation=45, ha=\"right\", fontsize=8) # Kleinere Schriftgröße für die X-Achsenbeschriftungen\n",
    "        plt.yticks(fontsize=8) # Kleinere Schriftgröße für die Y-Achsenbeschriftungen\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_filepath)\n",
    "        print(f\"Heatmap mit angepasster Größe und Schrift gespeichert als '{output_filepath}'.\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"Keine numerischen Spalten zum Visualisieren gefunden.\")\n",
    "\n",
    "\n",
    "def get_all_dataset_metrics(train_features, train_labels, dataset_name):\n",
    "    values = [None] * 6\n",
    "    values[0] = dataset_name\n",
    "    \n",
    "    label_entropy = calculate_entropy(train_labels)\n",
    "    print(f\"Label Entropy: {label_entropy:.4f}\")\n",
    "    values[1] = label_entropy\n",
    "    feature_entropy = calculate_entropy(train_features.flatten())\n",
    "    print(f\"Feature Entropy: {feature_entropy:.4f}\")\n",
    "    values[2] = feature_entropy\n",
    "\n",
    "    # 4. Calculate intra-class similarity on the training set\n",
    "    intra_class_similarities, overall_intra_similarity = calculate_intra_class_similarity(train_features, train_labels) # Use train_labels here\n",
    "    print(f\"Intra-Class Similarities per class: {intra_class_similarities}\")\n",
    "    print(f\"Overall Intra-Class Similarity: {overall_intra_similarity:.4f}\")\n",
    "    values[5] = overall_intra_similarity\n",
    "    \n",
    "    # 5. Calculate inter-class similarity on the training set\n",
    "    overall_inter_similarity = calculate_inter_class_similarity_vectorized(train_features, train_labels) # Use train_labels here\n",
    "    print(f\"Overall Inter-Class Similarity: {overall_inter_similarity:.4f}\")\n",
    "    values[4] = overall_inter_similarity\n",
    "\n",
    "    # 6. Calculate feature variance\n",
    "    feature_variance = calculate_feature_variance(train_features)\n",
    "    print(f\"Feature Variance: {feature_variance:.4f}\")\n",
    "    values[3] = feature_variance\n",
    "\n",
    "    return values\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # get dataset and augmentations\n",
    "    train_transform = make_train_transform_from_args(args)\n",
    "    test_transform = make_test_transform_from_args(args)\n",
    "    train_base_dataset, test_base_dataset = get_dataset(\n",
    "        args.dataset, path=args.data_root\n",
    "    )\n",
    "    update_transforms(test_base_dataset, transform=test_transform)\n",
    "\n",
    "\n",
    "    # get datamanager based on ds\n",
    "    data_manager = None\n",
    "    if DILDataManager.is_dil(str(train_base_dataset)):\n",
    "        print(\"DIL\")\n",
    "        data_manager = DILDataManager(\n",
    "            train_base_dataset,\n",
    "            test_base_dataset,\n",
    "        )\n",
    "    else:\n",
    "        print(\"CIL\")\n",
    "        data_manager = CILDataManager(\n",
    "            train_base_dataset,\n",
    "            test_base_dataset,\n",
    "            T=args.T,\n",
    "            num_first_task=None if args.dataset != \"cars\" else 16,\n",
    "            shuffle=True,\n",
    "            seed=args.seed,\n",
    "        )\n",
    "\n",
    "\n",
    "    feature_extractor = timm.create_model(args.backbone, pretrained=True).to(args.device)\n",
    "    feature_extractor.head = nn.Identity()\n",
    "    feature_extractor.eval()\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    bar = tqdm(enumerate(data_manager), desc=\"Extracting Features\", total=len(data_manager))\n",
    "    for i, (train_dataset, _) in bar:\n",
    "        train_dataset.transform = train_transform\n",
    "        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "        for images, labels in train_loader:  # Iterate through all batches in the loader\n",
    "            images = images.to(args.device)\n",
    "            features = feature_extractor(images)\n",
    "            train_features.append(features.cpu().detach().numpy())\n",
    "            train_labels.append(labels.cpu().detach().numpy())\n",
    "\n",
    "    del feature_extractor\n",
    "\n",
    "    train_features = np.concatenate(train_features, axis=0)\n",
    "    train_labels = np.concatenate(train_labels, axis=0)\n",
    "    print(\"Features shape:\", train_features.shape)\n",
    "\n",
    "    return train_features, train_labels   \n",
    "\n",
    "    #values = get_all_dataset_metrics(train_features, train_labels, args.dataset)\n",
    "    \n",
    "\"\"\"\n",
    "    # Saving values\n",
    "    save_path = \"local_data/dataset_metrics.csv\"\n",
    "    with open(save_path, 'a', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(values)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66a1a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description=\"Your script description here\")\n",
    "\n",
    "    # Define your arguments as before\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.0, help='Weight decay')\n",
    "    parser.add_argument('--early_stopping', type=int, default=10, help='Patience for early stopping')\n",
    "    parser.add_argument('--dataset', type=str, default='cifar100',\n",
    "                        choices=['cifar100', 'imagenetr', 'imageneta', 'vtab', 'cars', 'cub',\n",
    "                                 'omnibenchmark', 'dil_imagenetr', 'cddb', 'limited_domainnet'],\n",
    "                        help='Dataset to use')\n",
    "    parser.add_argument('--T', type=int, default=5, help='Number of timesteps')\n",
    "    parser.add_argument('--backbone', type=str, default='vit_base_patch16_224',\n",
    "                        choices=['vit_base_patch16_224', 'vit_base_patch16_224_in21k'],\n",
    "                        help='Backbone architecture')\n",
    "    parser.add_argument('--finetune_method', type=str, default='none',\n",
    "                        choices=['none', 'adapter', 'ssf', 'vpt'],\n",
    "                        help='Finetuning method')\n",
    "    parser.add_argument('--finetune_epochs', type=int, default=20, help='Number of finetuning epochs')\n",
    "    parser.add_argument('--k', type=int, default=5, help='Number of nearest neighbors')\n",
    "    parser.add_argument('--device', type=str, default='cuda' if 'cuda' in sys.modules else 'cpu',\n",
    "                        help='Device to use (cuda or cpu)')\n",
    "    parser.add_argument('--seed', type=int, default=42, help='Random seed')\n",
    "    parser.add_argument('--data_root', type=str, default='./local_data', help='Root directory for datasets')\n",
    "    parser.add_argument('--moe_max_experts', type=int, default=4, help='Maximum number of experts for MoE')\n",
    "    parser.add_argument('--reduce_dataset', type=float, default=1.0, help='Fraction of dataset to use')\n",
    "    parser.add_argument('--gmms', type=int, default=8, help='Number of GMM components')\n",
    "    parser.add_argument('--use_multivariate', action='store_true', help='Use multivariate Gaussian')\n",
    "    parser.add_argument('--selection_method', type=str, default='random',\n",
    "                        choices=['random', 'around', 'eucld_dist', 'inv_eucld_dist', 'kl_div',\n",
    "                                 'inv_kl_div', 'ws_div', 'inv_ws_div'],\n",
    "                        help='Selection method')\n",
    "    parser.add_argument('--kd', action='store_true', help='Use knowledge distillation')\n",
    "    parser.add_argument('--kd_alpha', type=float, default=0.5, help='Alpha for knowledge distillation loss')\n",
    "    parser.add_argument('--log_gpustat', action='store_true', help='Log GPU statistics')\n",
    "    parser.add_argument('--sweep_logging', action='store_true', help='Enable Weights & Biases sweep logging')\n",
    "    parser.add_argument('--exit_after_T', action='store_true', help='Exit after T timesteps')\n",
    "    parser.add_argument('--selection_criterion', type=int, default=0, choices=[0, 1, 2],\n",
    "                        help='Selection criterion')\n",
    "    parser.add_argument('--tau', type=float, default=0.1, help='Temperature parameter')\n",
    "    parser.add_argument('--exit_after_acc', type=float, default=0.0, help='Exit after reaching this accuracy')\n",
    "    parser.add_argument('--trash_var', type=float, default=0.0, help='Trash variable (for testing)')\n",
    "    parser.add_argument('--use_adamw', action='store_true', help='Use AdamW optimizer')\n",
    "    parser.add_argument('--use_cosine_annealing', action='store_true', help='Use cosine annealing scheduler')\n",
    "    parser.add_argument('--aug_resize_crop_min', type=float, default=0.8, help='Min scale for random resize crop')\n",
    "    parser.add_argument('--aug_resize_crop_max', type=float, default=1.0, help='Max scale for random resize crop')\n",
    "    parser.add_argument('--aug_random_rotation_degree', type=float, default=0.0, help='Degree for random rotation')\n",
    "    parser.add_argument('--aug_brightness_jitter', type=float, default=0.0, help='Brightness jitter')\n",
    "    parser.add_argument('--aug_contrast_jitter', type=float, default=0.0, help='Contrast jitter')\n",
    "    parser.add_argument('--aug_saturation_jitter', type=float, default=0.0, help='Saturation jitter')\n",
    "    parser.add_argument('--aug_hue_jitter', type=float, default=0.0, help='Hue jitter')\n",
    "    parser.add_argument('--aug_normalize', action='store_true', help='Normalize input images')\n",
    "    parser.add_argument('--wandb_project', type=str, default='your_project_name', help='WandB project name')\n",
    "    parser.add_argument('--wandb_entity', type=str, default='your_entity_name', help='WandB entity name')\n",
    "\n",
    "    if '__file__' in globals():  # Check if running as a script\n",
    "        args = parser.parse_args()\n",
    "    else:  # Running in a Jupyter Notebook\n",
    "        args = parser.parse_args(args=[]) # Pass an empty list to avoid errors\n",
    "        # You can set default values here or handle arguments differently in the notebook\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ecd4406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 1796it [00:00, 1657419.14it/s]\n",
      "Iterate over test dataset: 8619it [00:00, 1777146.11it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m args \u001b[38;5;241m=\u001b[39m update_args(args)\n\u001b[1;32m      8\u001b[0m set_seed(args\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m---> 11\u001b[0m features, labels \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCIL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    298\u001b[0m     data_manager \u001b[38;5;241m=\u001b[39m CILDataManager(\n\u001b[1;32m    299\u001b[0m         train_base_dataset,\n\u001b[1;32m    300\u001b[0m         test_base_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m         seed\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mseed,\n\u001b[1;32m    305\u001b[0m     )\n\u001b[0;32m--> 308\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m \u001b[43mtimm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    309\u001b[0m feature_extractor\u001b[38;5;241m.\u001b[39mhead \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mIdentity()\n\u001b[1;32m    310\u001b[0m feature_extractor\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/timm/models/_factory.py:117\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m create_fn \u001b[38;5;241m=\u001b[39m model_entrypoint(model_name)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_layer_config(scriptable\u001b[38;5;241m=\u001b[39mscriptable, exportable\u001b[38;5;241m=\u001b[39mexportable, no_jit\u001b[38;5;241m=\u001b[39mno_jit):\n\u001b[0;32m--> 117\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_path:\n\u001b[1;32m    125\u001b[0m     load_checkpoint(model, checkpoint_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/timm/models/vision_transformer.py:2232\u001b[0m, in \u001b[0;36mvit_base_patch16_224\u001b[0;34m(pretrained, **kwargs)\u001b[0m\n\u001b[1;32m   2228\u001b[0m \u001b[38;5;124;03m\"\"\" ViT-Base (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m \u001b[38;5;124;03mImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m model_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m-> 2232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_create_vision_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvit_base_patch16_224\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/timm/models/vision_transformer.py:2132\u001b[0m, in \u001b[0;36m_create_vision_transformer\u001b[0;34m(variant, pretrained, **kwargs)\u001b[0m\n\u001b[1;32m   2129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msiglip\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m variant \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_pool\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   2130\u001b[0m     strict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_model_with_cfg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mVisionTransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_filter_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_filter_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_strict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgetter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/timm/models/_builder.py:415\u001b[0m, in \u001b[0;36mbuild_model_with_cfg\u001b[0;34m(model_cls, variant, pretrained, pretrained_cfg, pretrained_cfg_overlay, model_cfg, feature_cfg, pretrained_strict, pretrained_filter_fn, kwargs_filter, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# Instantiate the model\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_cls(cfg\u001b[38;5;241m=\u001b[39mmodel_cfg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/timm/models/vision_transformer.py:543\u001b[0m, in \u001b[0;36mVisionTransformer.__init__\u001b[0;34m(self, img_size, patch_size, in_chans, num_classes, global_pool, embed_dim, depth, num_heads, mlp_ratio, qkv_bias, qk_norm, init_values, class_token, pos_embed, no_embed_class, reg_tokens, pre_norm, final_norm, fc_norm, dynamic_img_size, dynamic_img_pad, drop_rate, pos_drop_rate, patch_drop_rate, proj_drop_rate, attn_drop_rate, drop_path_rate, weight_init, fix_init, embed_layer, norm_layer, act_layer, block_fn, mlp_layer)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_pre \u001b[38;5;241m=\u001b[39m norm_layer(embed_dim) \u001b[38;5;28;01mif\u001b[39;00m pre_norm \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mIdentity()\n\u001b[1;32m    542\u001b[0m dpr \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, drop_path_rate, depth)]  \u001b[38;5;66;03m# stochastic depth decay rule\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m    544\u001b[0m     block_fn(\n\u001b[1;32m    545\u001b[0m         dim\u001b[38;5;241m=\u001b[39membed_dim,\n\u001b[1;32m    546\u001b[0m         num_heads\u001b[38;5;241m=\u001b[39mnum_heads,\n\u001b[1;32m    547\u001b[0m         mlp_ratio\u001b[38;5;241m=\u001b[39mmlp_ratio,\n\u001b[1;32m    548\u001b[0m         qkv_bias\u001b[38;5;241m=\u001b[39mqkv_bias,\n\u001b[1;32m    549\u001b[0m         qk_norm\u001b[38;5;241m=\u001b[39mqk_norm,\n\u001b[1;32m    550\u001b[0m         init_values\u001b[38;5;241m=\u001b[39minit_values,\n\u001b[1;32m    551\u001b[0m         proj_drop\u001b[38;5;241m=\u001b[39mproj_drop_rate,\n\u001b[1;32m    552\u001b[0m         attn_drop\u001b[38;5;241m=\u001b[39mattn_drop_rate,\n\u001b[1;32m    553\u001b[0m         drop_path\u001b[38;5;241m=\u001b[39mdpr[i],\n\u001b[1;32m    554\u001b[0m         norm_layer\u001b[38;5;241m=\u001b[39mnorm_layer,\n\u001b[1;32m    555\u001b[0m         act_layer\u001b[38;5;241m=\u001b[39mact_layer,\n\u001b[1;32m    556\u001b[0m         mlp_layer\u001b[38;5;241m=\u001b[39mmlp_layer,\n\u001b[1;32m    557\u001b[0m     )\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth)])\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_info \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28mdict\u001b[39m(module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, num_chs\u001b[38;5;241m=\u001b[39membed_dim, reduction\u001b[38;5;241m=\u001b[39mreduction) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth)]\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m norm_layer(embed_dim) \u001b[38;5;28;01mif\u001b[39;00m final_norm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_fc_norm \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mIdentity()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/timm/models/vision_transformer.py:544\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_pre \u001b[38;5;241m=\u001b[39m norm_layer(embed_dim) \u001b[38;5;28;01mif\u001b[39;00m pre_norm \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mIdentity()\n\u001b[1;32m    542\u001b[0m dpr \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, drop_path_rate, depth)]  \u001b[38;5;66;03m# stochastic depth decay rule\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39m[\n\u001b[0;32m--> 544\u001b[0m     \u001b[43mblock_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqkv_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqk_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqk_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproj_drop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproj_drop_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_drop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_drop_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdpr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mact_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmlp_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth)])\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_info \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28mdict\u001b[39m(module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, num_chs\u001b[38;5;241m=\u001b[39membed_dim, reduction\u001b[38;5;241m=\u001b[39mreduction) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth)]\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m norm_layer(embed_dim) \u001b[38;5;28;01mif\u001b[39;00m final_norm \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_fc_norm \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mIdentity()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/timm/models/vision_transformer.py:156\u001b[0m, in \u001b[0;36mBlock.__init__\u001b[0;34m(self, dim, num_heads, mlp_ratio, qkv_bias, qk_norm, proj_drop, attn_drop, init_values, drop_path, act_layer, norm_layer, mlp_layer)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1 \u001b[38;5;241m=\u001b[39m DropPath(drop_path) \u001b[38;5;28;01mif\u001b[39;00m drop_path \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mIdentity()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2 \u001b[38;5;241m=\u001b[39m norm_layer(dim)\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43mmlp_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mact_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproj_drop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls2 \u001b[38;5;241m=\u001b[39m LayerScale(dim, init_values\u001b[38;5;241m=\u001b[39minit_values) \u001b[38;5;28;01mif\u001b[39;00m init_values \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mIdentity()\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2 \u001b[38;5;241m=\u001b[39m DropPath(drop_path) \u001b[38;5;28;01mif\u001b[39;00m drop_path \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mIdentity()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/timm/layers/mlp.py:38\u001b[0m, in \u001b[0;36mMlp.__init__\u001b[0;34m(self, in_features, hidden_features, out_features, act_layer, norm_layer, bias, drop, use_conv)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(drop_probs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m norm_layer(hidden_features) \u001b[38;5;28;01mif\u001b[39;00m norm_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mIdentity()\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2 \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(drop_probs[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:104\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:110\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/init.py:460\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    458\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DATASET = \"vtab\"\n",
    "CUDA_VISIBLE_DEVICES = \"4\"\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = CUDA_VISIBLE_DEVICES\n",
    "args = parse_arguments()\n",
    "args.dataset = DATASET\n",
    "args = update_args(args)\n",
    "set_seed(args.seed)\n",
    "\n",
    "\n",
    "features, labels = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5799f872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({0: 0.404879, 1: 0.71943855, 2: 0.44615406, 3: 0.8199341, 4: 0.33074144, 5: 0.78092754, 6: 0.7489024, 7: 0.60773534, 8: 0.527426, 9: 0.26716688, 10: 0.23423919, 11: 0.46803683, 12: 0.46809265, 13: 0.5118063, 14: 0.5899071, 15: 0.4957721, 16: 0.88559484, 17: 0.5422535, 18: 0.90276074, 19: 0.3853124, 20: 0.45831147, 21: 0.41843164, 22: 0.4866485, 23: 0.55178463, 24: 0.7465788, 25: 0.5648913, 26: 0.714228, 27: 0.6028725, 28: 0.4497971, 29: 0.556562, 30: 0.5575757, 31: 0.22412403, 32: 0.45714498, 33: 0.6376291, 34: 0.60207206, 35: 0.6695693, 36: 0.793339, 37: 0.5768212, 38: 0.6843294, 39: 0.7417639, 40: 0.3188536, 41: 0.5054952, 42: 0.22406928, 43: 0.8706063, 44: 0.55285555, 45: 0.6206727, 46: 0.80375963, 47: 0.13300587, 48: 0.5665677, 49: 0.42743763}, 0.55309755)\n"
     ]
    }
   ],
   "source": [
    "test = calculate_selective_inter_class_similarity(features, labels, target_classes=[i for i in range(10)], similarity_metric='cosine')\n",
    "print(\"Selective Inter-Class Similarity:\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71861470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
