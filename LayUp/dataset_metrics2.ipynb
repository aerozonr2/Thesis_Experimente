{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00bd3040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "import os\n",
    "import cProfile\n",
    "import copy\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import subprocess\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import timm\n",
    "import pandas\n",
    "import seaborn\n",
    "import matplotlib.pyplot\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import math\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import numpy as np\n",
    "import csv\n",
    "import argparse\n",
    "import sys\n",
    "import ast\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "\n",
    "\n",
    "import wandb\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.backbone import get_backbone\n",
    "from src.modules import CosineLinear\n",
    "from src.moe_seed import MoE_SEED\n",
    "from src.data import (\n",
    "    CILDataManager,\n",
    "    DILDataManager,\n",
    "    get_dataset,\n",
    "    DATASET_MAP,\n",
    "    make_test_transform_from_args,\n",
    "    make_train_transform_from_args,\n",
    "    update_transforms,\n",
    ")\n",
    "from src.logging import Logger, WandbLogger, ConsoleLogger, TQDMLogger\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from src.support_functions import check_gpu_memory, shrink_dataset, display_profile, log_gpustat, optimize_args\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6a5e9",
   "metadata": {},
   "source": [
    "# Important! Only one dataset at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "364f60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"vtab\"\n",
    "CUDA_VISIBLE_DEVICES = \"4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4df6d7",
   "metadata": {},
   "source": [
    "## Get dataset or class features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "762ebe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def setup_logger(args):\n",
    "    Logger.instance().add_backend(ConsoleLogger())\n",
    "    if args.wandb_project is not None:\n",
    "        Logger.instance().add_backend(\n",
    "            WandbLogger(args.wandb_project, args.wandb_entity, args)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def update_args(args):\n",
    "    assert args.k >= 1 and args.k <= 12\n",
    "    args.intralayers = [f\"blocks.{11 - i}\" for i in range(args.k)]\n",
    "\n",
    "    args.aug_normalize = bool(args.aug_normalize)\n",
    "\n",
    "    args.target_size = 224\n",
    "    \n",
    "    dataset_T_map = {\n",
    "        \"dil_imagenetr\": {\"T\": 15, \"moe_max_experts\": 2},\n",
    "        \"limited_domainnet\": {\"T\": 6, \"moe_max_experts\": 3},\n",
    "        \"vtab\": {\"T\": 5, \"moe_max_experts\": 3},\n",
    "        \"cddb\": {\"T\": 5, \"moe_max_experts\": 3},\n",
    "    }\n",
    "\n",
    "    if args.dataset in dataset_T_map.keys():\n",
    "        args.T = dataset_T_map[args.dataset][\"T\"]\n",
    "        #args.moe_max_experts = dataset_T_map[args.dataset][\"moe_max_experts\"] #immer 5!\n",
    "        print(f\"Dataset {args.dataset} has T={args.T} and moe_max_experts={args.moe_max_experts}\")\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_similarity(feature1, feature2, metric='cosine'):\n",
    "    \"\"\"\n",
    "    Calculates the similarity between two feature vectors.\n",
    "\n",
    "    Args:\n",
    "        feature1 (np.ndarray): First feature vector.\n",
    "        feature2 (np.ndarray): Second feature vector.\n",
    "        metric (str): The similarity metric to use ('cosine' or 'euclidean').\n",
    "\n",
    "    Returns:\n",
    "        float: The similarity score.\n",
    "    \"\"\"\n",
    "    if metric == 'cosine':\n",
    "        return cosine_similarity(feature1.reshape(1, -1), feature2.reshape(1, -1))[0][0]\n",
    "    elif metric == 'euclidean':\n",
    "        return -np.linalg.norm(feature1 - feature2) # Negative for consistency (higher value = more similar)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported similarity metric: {metric}\")\n",
    "\n",
    "def calculate_intra_class_similarity(features, labels, similarity_metric='cosine'):\n",
    "    \"\"\"\n",
    "    Calculates the average intra-class similarity for a dataset.\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): Array of feature vectors.\n",
    "        labels (np.ndarray): Array of corresponding labels.\n",
    "        similarity_metric (str): The similarity metric to use ('cosine' or 'euclidean').\n",
    "\n",
    "    Returns:\n",
    "        tuple: (dict, float) - A dictionary of per-class intra-class similarities\n",
    "               and the overall average intra-class similarity.\n",
    "    \"\"\"\n",
    "    intra_class_similarities = {}\n",
    "    unique_labels = np.unique(labels)\n",
    "    bar = tqdm(enumerate(unique_labels), desc=\"Calculating Intra-Class Similarity\", total=len(unique_labels))\n",
    "    for i, label in bar:\n",
    "        class_features = features[labels == label]\n",
    "        similarities = []\n",
    "        for i in range(len(class_features)):\n",
    "            for j in range(i + 1, len(class_features)):\n",
    "                similarity = calculate_similarity(class_features[i], class_features[j], similarity_metric)\n",
    "                similarities.append(similarity)\n",
    "        if similarities:\n",
    "            intra_class_similarities[label] = np.mean(similarities)\n",
    "        else:\n",
    "            intra_class_similarities[label] = 0.0\n",
    "\n",
    "    overall_intra_class_similarity = np.mean(list(intra_class_similarities.values())) if intra_class_similarities else 0.0\n",
    "    return intra_class_similarities, overall_intra_class_similarity\n",
    "\n",
    "# new for intra... last chapter\n",
    "def calculate_similarity(feature1: np.ndarray, feature2: np.ndarray, metric: str = 'cosine') -> float:\n",
    "    \"\"\"\n",
    "    Calculates the similarity between two feature vectors.\n",
    "\n",
    "    Args:\n",
    "        feature1 (np.ndarray): First feature vector.\n",
    "        feature2 (np.ndarray): Second feature vector.\n",
    "        metric (str): The similarity metric to use ('cosine' or 'euclidean').\n",
    "\n",
    "    Returns:\n",
    "        float: The similarity score.\n",
    "    \"\"\"\n",
    "    if metric == 'cosine':\n",
    "        # Ensure inputs are 2D for scikit-learn's cosine_similarity\n",
    "        return cosine_similarity(feature1.reshape(1, -1), feature2.reshape(1, -1))[0][0]\n",
    "    elif metric == 'euclidean':\n",
    "        # euclidean_distances returns distances, so we negate for similarity.\n",
    "        # It's more efficient to calculate norm directly for two vectors.\n",
    "        return -np.linalg.norm(feature1 - feature2)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported similarity metric: {metric}\")\n",
    "\n",
    "def calculate_intra_class_similarity_optimized(features: np.ndarray, labels: np.ndarray, similarity_metric: str = 'cosine') -> Tuple[dict, float]:\n",
    "    \"\"\"\n",
    "    Calculates the average intra-class similarity for a dataset efficiently.\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): Array of feature vectors (n_samples, n_features).\n",
    "        labels (np.ndarray): Array of corresponding labels (n_samples,).\n",
    "        similarity_metric (str): The similarity metric to use ('cosine' or 'euclidean').\n",
    "\n",
    "    Returns:\n",
    "        tuple: (dict, float) - A dictionary of per-class intra-class similarities\n",
    "               and the overall average intra-class similarity.\n",
    "    \"\"\"\n",
    "    intra_class_similarities = {}\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    # Choose the appropriate pairwise function based on the metric\n",
    "    if similarity_metric == 'cosine':\n",
    "        pairwise_func = cosine_similarity\n",
    "    elif similarity_metric == 'euclidean':\n",
    "        # For Euclidean distance, we'll use euclidean_distances and then negate it\n",
    "        pairwise_func = euclidean_distances\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported similarity metric: {similarity_metric}\")\n",
    "\n",
    "    bar = tqdm(unique_labels, desc=\"Calculating Intra-Class Similarity\", unit=\"class\")\n",
    "    for label in bar:\n",
    "        class_features = features[labels == label]\n",
    "        n_samples_in_class = class_features.shape[0]\n",
    "\n",
    "        if n_samples_in_class < 2:\n",
    "            # Cannot calculate pairwise similarity for 0 or 1 sample\n",
    "            intra_class_similarities[label] = 0.0\n",
    "            continue\n",
    "\n",
    "        # Calculate all-to-all pairwise similarities within the class\n",
    "        # This is where the main efficiency gain comes from\n",
    "        pairwise_sims_or_dists = pairwise_func(class_features)\n",
    "\n",
    "        # Extract only the upper triangle (excluding diagonal) to avoid duplicates and self-similarity\n",
    "        # np.triu_indices_from gets indices for upper triangle\n",
    "        upper_tri_indices = np.triu_indices_from(pairwise_sims_or_dists, k=1)\n",
    "        \n",
    "        similarities = pairwise_sims_or_dists[upper_tri_indices]\n",
    "\n",
    "        # If Euclidean, negate the distances to represent similarity\n",
    "        if similarity_metric == 'euclidean':\n",
    "            similarities = -similarities\n",
    "        \n",
    "        if similarities.size > 0:\n",
    "            intra_class_similarities[label] = np.mean(similarities)\n",
    "        else:\n",
    "            intra_class_similarities[label] = 0.0 # Should not happen if n_samples_in_class >= 2\n",
    "\n",
    "    overall_intra_class_similarity = np.mean(list(intra_class_similarities.values())) if intra_class_similarities else 0.0\n",
    "    \n",
    "    return intra_class_similarities, overall_intra_class_similarity\n",
    "\n",
    "\n",
    "def calculate_inter_class_similarity(features, labels, similarity_metric='cosine'):\n",
    "    \"\"\"\n",
    "    Calculates the average inter-class similarity for a dataset.\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): Array of feature vectors.\n",
    "        labels (np.ndarray): Array of corresponding labels.\n",
    "        similarity_metric (str): The similarity metric to use ('cosine' or 'euclidean').\n",
    "\n",
    "    Returns:\n",
    "        float: The overall average inter-class similarity.\n",
    "    \"\"\"\n",
    "    inter_class_similarities = []\n",
    "    unique_labels = np.unique(labels)\n",
    "    bar = tqdm(enumerate(unique_labels), desc=\"Calculating Inter-Class Similarity\", total=len(unique_labels))\n",
    "    # Iterate through all pairs of classes\n",
    "    for i, _ in bar:\n",
    "        for j in range(i + 1, len(unique_labels)):\n",
    "            label1 = unique_labels[i]\n",
    "            label2 = unique_labels[j]\n",
    "            features_class1 = features[labels == label1]\n",
    "            features_class2 = features[labels == label2]\n",
    "            for feat1 in features_class1:\n",
    "                for feat2 in features_class2:\n",
    "                    similarity = calculate_similarity(feat1, feat2, similarity_metric)\n",
    "                    inter_class_similarities.append(similarity)\n",
    "\n",
    "    overall_inter_class_similarity = np.mean(inter_class_similarities) if inter_class_similarities else 0.0\n",
    "    return overall_inter_class_similarity\n",
    "\n",
    "def calculate_inter_class_similarity_vectorized(features, labels, similarity_metric='cosine'):\n",
    "    \"\"\"\n",
    "    Calculates the average inter-class similarity for a dataset using vectorization.\n",
    "    \"\"\"\n",
    "    inter_class_similarities = []\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_classes = len(unique_labels)\n",
    "    bar = tqdm(range(n_classes), desc=\"Calculating Inter-Class Similarity\", total=n_classes)\n",
    "\n",
    "    for i in bar:\n",
    "        for j in range(i + 1, n_classes):\n",
    "            label1 = unique_labels[i]\n",
    "            label2 = unique_labels[j]\n",
    "            features_class1 = features[labels == label1]\n",
    "            features_class2 = features[labels == label2]\n",
    "\n",
    "            if similarity_metric == 'cosine':\n",
    "                similarity_matrix = cosine_similarity(features_class1, features_class2)\n",
    "                inter_class_similarities.extend(similarity_matrix.flatten())\n",
    "            elif similarity_metric == 'euclidean':\n",
    "                # Calculate pairwise Euclidean distances and negate for consistency\n",
    "                distances = np.linalg.norm(features_class1[:, np.newaxis, :] - features_class2[np.newaxis, :, :], axis=2)\n",
    "                inter_class_similarities.extend((-distances).flatten())\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported similarity metric: {similarity_metric}\")\n",
    "\n",
    "    overall_inter_class_similarity = np.mean(inter_class_similarities) if inter_class_similarities else 0.0\n",
    "    return overall_inter_class_similarity\n",
    "\n",
    "# Expert learned a set of classes and we want to calculate the similarity to all other classes\n",
    "def calculate_selective_inter_class_similarity(features, labels, target_classes, similarity_metric='cosine'):\n",
    "    \"\"\"\n",
    "    Calculates the average inter-class similarity between a target list of classes\n",
    "    and all other classes not in the target list, using vectorization.\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): Array of feature vectors.\n",
    "        labels (np.ndarray): Array of corresponding labels.\n",
    "        target_classes (list): List of class labels for which to compute similarity\n",
    "                               to all other classes (excluding those in this list).\n",
    "        similarity_metric (str, optional): Metric to use for similarity calculation.\n",
    "                                           'cosine' or 'euclidean' are supported.\n",
    "                                           Defaults to 'cosine'.\n",
    "\n",
    "    Returns:\n",
    "        float: The average inter-class similarity between the target classes\n",
    "               and the other classes. Returns 0.0 if no such pairs exist.\n",
    "    \"\"\"\n",
    "    inter_class_similarities = []\n",
    "    unique_labels = np.unique(labels)\n",
    "    target_classes = set(target_classes)  # Convert to set for faster lookups\n",
    "    other_classes = [label for label in unique_labels if label not in target_classes]\n",
    "\n",
    "    bar = tqdm(target_classes, desc=\"Calculating Selective Inter-Class Similarity\", total=len(target_classes))\n",
    "\n",
    "    for label1 in bar:\n",
    "        features_class1 = features[labels == label1]\n",
    "        for label2 in other_classes:\n",
    "            features_class2 = features[labels == label2]\n",
    "\n",
    "            if similarity_metric == 'cosine':\n",
    "                similarity_matrix = cosine_similarity(features_class1, features_class2)\n",
    "                inter_class_similarities.extend(similarity_matrix.flatten())\n",
    "            elif similarity_metric == 'euclidean':\n",
    "                # Calculate pairwise Euclidean distances and negate for consistency\n",
    "                distances = np.linalg.norm(features_class1[:, np.newaxis, :] - features_class2[np.newaxis, :, :], axis=2)\n",
    "                inter_class_similarities.extend((-distances).flatten())\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported similarity metric: {similarity_metric}\")\n",
    "\n",
    "    overall_inter_class_similarity = np.mean(inter_class_similarities) if inter_class_similarities else 0.0\n",
    "    return overall_inter_class_similarity\n",
    "\n",
    "\n",
    "def calculate_entropy(labels):\n",
    "    \"\"\"\n",
    "    Calculates the entropy of a list or NumPy array of labels.\n",
    "\n",
    "    Args:\n",
    "        labels (list or np.ndarray): A list or array of labels.\n",
    "\n",
    "    Returns:\n",
    "        float: The entropy of the labels.\n",
    "    \"\"\"\n",
    "\n",
    "    label_counts = Counter(labels)\n",
    "    total_samples = len(labels)\n",
    "    entropy = 0.0\n",
    "\n",
    "    for count in label_counts.values():\n",
    "        probability = count / total_samples\n",
    "        entropy -= probability * math.log2(probability)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "def calculate_feature_variance(features):\n",
    "    \"\"\"\n",
    "    Calculates the variance of the features.\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): Array of feature vectors.\n",
    "\n",
    "    Returns:\n",
    "        float: The variance of the features.\n",
    "    \"\"\"\n",
    "    return np.var(features, axis=0).mean()  # Mean variance across all features\n",
    "\n",
    "def visualize_csv_with_adjusted_size(csv_filepath, output_filepath=\"heatmap_adjusted.png\"):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filepath, index_col=0)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Fehler: Datei '{csv_filepath}' nicht gefunden.\")\n",
    "        return\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "\n",
    "    if not numeric_cols.empty:\n",
    "        df_normalized = df[numeric_cols].apply(lambda x: (x - x.min()) / (x.max() - x.min()), axis=0)\n",
    "        cmap = LinearSegmentedColormap.from_list(\"mycmap\", [\"white\", \"lightblue\", \"darkblue\"])\n",
    "\n",
    "        # Erhöhe die Figurengröße, um die Kästchen größer zu machen\n",
    "        plt.figure(figsize=(len(numeric_cols) * 2, len(df) * 1))\n",
    "\n",
    "        sns.heatmap(df_normalized, annot=False, cmap=cmap, cbar=True, yticklabels=True)\n",
    "        plt.title(\"Farbliche Visualisierung der Datenspalten\", fontsize=12) # Kleinere Schriftgröße für den Titel\n",
    "        plt.xlabel(\"Numerische Spalten\", fontsize=10) # Kleinere Schriftgröße für die X-Achse\n",
    "        plt.ylabel(\"Datensätze\", fontsize=10) # Kleinere Schriftgröße für die Y-Achse\n",
    "        plt.xticks(rotation=45, ha=\"right\", fontsize=8) # Kleinere Schriftgröße für die X-Achsenbeschriftungen\n",
    "        plt.yticks(fontsize=8) # Kleinere Schriftgröße für die Y-Achsenbeschriftungen\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_filepath)\n",
    "        print(f\"Heatmap mit angepasster Größe und Schrift gespeichert als '{output_filepath}'.\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"Keine numerischen Spalten zum Visualisieren gefunden.\")\n",
    "\n",
    "\n",
    "def get_all_dataset_metrics(train_features, train_labels, dataset_name):\n",
    "    values = [None] * 6\n",
    "    values[0] = dataset_name\n",
    "    \n",
    "    label_entropy = calculate_entropy(train_labels)\n",
    "    print(f\"Label Entropy: {label_entropy:.4f}\")\n",
    "    values[1] = label_entropy\n",
    "    feature_entropy = calculate_entropy(train_features.flatten())\n",
    "    print(f\"Feature Entropy: {feature_entropy:.4f}\")\n",
    "    values[2] = feature_entropy\n",
    "\n",
    "    # 4. Calculate intra-class similarity on the training set\n",
    "    intra_class_similarities, overall_intra_similarity = calculate_intra_class_similarity(train_features, train_labels) # Use train_labels here\n",
    "    print(f\"Intra-Class Similarities per class: {intra_class_similarities}\")\n",
    "    print(f\"Overall Intra-Class Similarity: {overall_intra_similarity:.4f}\")\n",
    "    values[5] = overall_intra_similarity\n",
    "    \n",
    "    # 5. Calculate inter-class similarity on the training set\n",
    "    overall_inter_similarity = calculate_inter_class_similarity_vectorized(train_features, train_labels) # Use train_labels here\n",
    "    print(f\"Overall Inter-Class Similarity: {overall_inter_similarity:.4f}\")\n",
    "    values[4] = overall_inter_similarity\n",
    "\n",
    "    # 6. Calculate feature variance\n",
    "    feature_variance = calculate_feature_variance(train_features)\n",
    "    print(f\"Feature Variance: {feature_variance:.4f}\")\n",
    "    values[3] = feature_variance\n",
    "\n",
    "    return values\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # get dataset and augmentations\n",
    "    train_transform = make_train_transform_from_args(args)\n",
    "    test_transform = make_test_transform_from_args(args)\n",
    "    train_base_dataset, test_base_dataset = get_dataset(\n",
    "        args.dataset, path=args.data_root\n",
    "    )\n",
    "    update_transforms(test_base_dataset, transform=test_transform)\n",
    "\n",
    "\n",
    "    # get datamanager based on ds\n",
    "    data_manager = None\n",
    "    if DILDataManager.is_dil(str(train_base_dataset)):\n",
    "        print(\"DIL\")\n",
    "        data_manager = DILDataManager(\n",
    "            train_base_dataset,\n",
    "            test_base_dataset,\n",
    "        )\n",
    "    else:\n",
    "        print(\"CIL\")\n",
    "        data_manager = CILDataManager(\n",
    "            train_base_dataset,\n",
    "            test_base_dataset,\n",
    "            T=args.T,\n",
    "            num_first_task=None if args.dataset != \"cars\" else 16,\n",
    "            shuffle=True,\n",
    "            seed=args.seed,\n",
    "        )\n",
    "\n",
    "\n",
    "    feature_extractor = timm.create_model(args.backbone, pretrained=True).to(args.device)\n",
    "    feature_extractor.head = nn.Identity()\n",
    "    feature_extractor.eval()\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    bar = tqdm(enumerate(data_manager), desc=\"Extracting Features\", total=len(data_manager))\n",
    "    for i, (train_dataset, _) in bar:\n",
    "        train_dataset.transform = train_transform\n",
    "        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "        for images, labels in train_loader:  # Iterate through all batches in the loader\n",
    "            images = images.to(args.device)\n",
    "            features = feature_extractor(images)\n",
    "            train_features.append(features.cpu().detach().numpy())\n",
    "            train_labels.append(labels.cpu().detach().numpy())\n",
    "\n",
    "    del feature_extractor\n",
    "\n",
    "    train_features = np.concatenate(train_features, axis=0)\n",
    "    train_labels = np.concatenate(train_labels, axis=0)\n",
    "    print(\"Features shape:\", train_features.shape)\n",
    "\n",
    "    return train_features, train_labels   \n",
    "\n",
    "    #values = get_all_dataset_metrics(train_features, train_labels, args.dataset)\n",
    "    \n",
    "\"\"\"\n",
    "    # Saving values\n",
    "    save_path = \"local_data/dataset_metrics.csv\"\n",
    "    with open(save_path, 'a', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(values)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def main2_classes_per_task_and_seed(args):\n",
    "    # get dataset and augmentations\n",
    "    train_transform = make_train_transform_from_args(args)\n",
    "    test_transform = make_test_transform_from_args(args)\n",
    "    train_base_dataset, test_base_dataset = get_dataset(\n",
    "        args.dataset, path=args.data_root\n",
    "    )\n",
    "    update_transforms(test_base_dataset, transform=test_transform)\n",
    "\n",
    "\n",
    "    # get datamanager based on ds\n",
    "    data_manager = None\n",
    "    if DILDataManager.is_dil(str(train_base_dataset)):\n",
    "        print(\"DIL\")\n",
    "        data_manager = DILDataManager(\n",
    "            train_base_dataset,\n",
    "            test_base_dataset,\n",
    "        )\n",
    "    else:\n",
    "        print(\"CIL\")\n",
    "        data_manager = CILDataManager(\n",
    "            train_base_dataset,\n",
    "            test_base_dataset,\n",
    "            T=args.T,\n",
    "            num_first_task=None if args.dataset != \"cars\" else 16,\n",
    "            shuffle=True,\n",
    "            seed=args.seed,\n",
    "        )\n",
    "        class_order = data_manager.class_order\n",
    "\n",
    "    csv_rows = []\n",
    "    bar = tqdm(enumerate(data_manager), desc=\"Accumulating classes per task\", total=len(data_manager))\n",
    "    for t, (train_dataset, _) in bar:\n",
    "        task_labels_set = np.array([], dtype=np.int_)\n",
    "\n",
    "        train_dataset.transform = train_transform\n",
    "        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "        for _, labels in train_loader:  # Iterate through all batches in the loader\n",
    "            batch_labels = labels.cpu().detach().numpy()\n",
    "            task_labels_set = np.concatenate((task_labels_set, batch_labels))\n",
    "        # Get unique labels\n",
    "        _, idx = np.unique(task_labels_set, return_index=True)\n",
    "        task_labels_set = task_labels_set[np.sort(idx)]\n",
    "        # label mapping\n",
    "        for i, e in enumerate(task_labels_set):\n",
    "            task_labels_set[i] = class_order[e]\n",
    "\n",
    "        for i in task_labels_set:\n",
    "            row = {\n",
    "                \"dataset\": args.dataset,\n",
    "                \"task\": t,\n",
    "                \"class\": i,\n",
    "                \"seed\": args.seed\n",
    "            }\n",
    "            csv_rows.append(row)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    save_path = f\"local_data/classes_per_task/CpT_{args.dataset}_{args.seed}.csv\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    with open(save_path, 'w', newline='') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=[\"dataset\", \"task\", \"class\", \"seed\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_rows)\n",
    "    print(f\"Classes per task saved in {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "016566f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 24000it [00:00, 663822.00it/s]\n",
      "Iterate over test dataset: 6000it [00:00, 1166272.31it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [02:04<00:00, 12.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_imagenetr_2000.csv\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 50000it [00:02, 19682.41it/s]\n",
      "Iterate over test dataset: 10000it [00:14, 691.54it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [01:27<00:00,  8.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_cifar100_2000.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 9430it [00:00, 1283040.41it/s]\n",
      "Iterate over test dataset: 2358it [00:00, 1365668.16it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [00:44<00:00,  4.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_cub_2000.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 5981it [00:00, 1215177.88it/s]\n",
      "Iterate over test dataset: 1519it [00:00, 1339039.04it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [00:30<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_imageneta_2000.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 8144it [00:00, 1299688.45it/s]\n",
      "Iterate over test dataset: 8041it [00:00, 1578507.84it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [00:53<00:00,  5.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_cars_2000.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 89697it [00:00, 1184874.10it/s]\n",
      "Iterate over test dataset: 5985it [00:00, 1477945.80it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [10:11<00:00, 61.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_omnibenchmark_2000.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 24000it [00:00, 1165691.58it/s]\n",
      "Iterate over test dataset: 6000it [00:00, 1512641.94it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [02:06<00:00, 12.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_imagenetr_2001.csv\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 50000it [00:03, 16512.66it/s]\n",
      "Iterate over test dataset: 10000it [00:14, 707.31it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [01:28<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_cifar100_2001.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 9430it [00:00, 444392.74it/s]\n",
      "Iterate over test dataset: 2358it [00:00, 1123244.61it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [00:50<00:00,  5.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_cub_2001.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 5981it [00:00, 1170062.14it/s]\n",
      "Iterate over test dataset: 1519it [00:00, 1314723.02it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [00:30<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_imageneta_2001.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 8144it [00:00, 666232.60it/s]\n",
      "Iterate over test dataset: 8041it [00:00, 729298.27it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [00:55<00:00,  5.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_cars_2001.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 89697it [00:00, 1280244.49it/s]\n",
      "Iterate over test dataset: 5985it [00:00, 1547365.43it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [10:39<00:00, 63.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_omnibenchmark_2001.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 24000it [00:00, 683199.49it/s]\n",
      "Iterate over test dataset: 6000it [00:00, 1235496.29it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [02:09<00:00, 12.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_imagenetr_2002.csv\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 50000it [00:03, 14157.52it/s]\n",
      "Iterate over test dataset: 10000it [00:17, 575.21it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [01:28<00:00,  8.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_cifar100_2002.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 9430it [00:00, 1214489.72it/s]\n",
      "Iterate over test dataset: 2358it [00:00, 1379189.63it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [00:44<00:00,  4.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_cub_2002.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 5981it [00:00, 1221092.88it/s]\n",
      "Iterate over test dataset: 1519it [00:00, 1580145.78it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [00:26<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_imageneta_2002.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 8144it [00:00, 1678463.55it/s]\n",
      "Iterate over test dataset: 8041it [00:00, 1429084.68it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [00:45<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_cars_2002.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 89697it [00:00, 1397363.94it/s]\n",
      "Iterate over test dataset: 5985it [00:00, 1667413.45it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [09:10<00:00, 55.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_omnibenchmark_2002.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 24000it [00:00, 1325266.87it/s]\n",
      "Iterate over test dataset: 6000it [00:00, 1877766.30it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [01:45<00:00, 10.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_imagenetr_2003.csv\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 50000it [00:02, 22062.05it/s]\n",
      "Iterate over test dataset: 10000it [00:12, 798.67it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [01:20<00:00,  8.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_cifar100_2003.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 9430it [00:00, 1486145.89it/s]\n",
      "Iterate over test dataset: 2358it [00:00, 1757315.00it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [00:42<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_cub_2003.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 5981it [00:00, 820371.24it/s]\n",
      "Iterate over test dataset: 1519it [00:00, 596605.28it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [00:26<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_imageneta_2003.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 8144it [00:00, 1386411.71it/s]\n",
      "Iterate over test dataset: 8041it [00:00, 1708098.17it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [00:47<00:00,  4.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_cars_2003.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 89697it [00:00, 1387080.61it/s]\n",
      "Iterate over test dataset: 5985it [00:00, 1015038.19it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [08:19<00:00, 49.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_omnibenchmark_2003.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 24000it [00:00, 1209895.38it/s]\n",
      "Iterate over test dataset: 6000it [00:00, 1847168.53it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [01:50<00:00, 11.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_imagenetr_2004.csv\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 50000it [00:02, 17587.85it/s]\n",
      "Iterate over test dataset: 10000it [00:15, 665.26it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [01:28<00:00,  8.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_cifar100_2004.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 9430it [00:00, 875050.59it/s]\n",
      "Iterate over test dataset: 2358it [00:00, 1012092.59it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [00:47<00:00,  4.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_cub_2004.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 5981it [00:00, 1262957.87it/s]\n",
      "Iterate over test dataset: 1519it [00:00, 1050651.02it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [00:30<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_imageneta_2004.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 8144it [00:00, 1320693.31it/s]\n",
      "Iterate over test dataset: 8041it [00:00, 1280522.38it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [00:56<00:00,  5.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_cars_2004.csv\n",
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 89697it [00:00, 943193.95it/s]\n",
      "Iterate over test dataset: 5985it [00:00, 1110994.00it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [09:02<00:00, 54.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_omnibenchmark_2004.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "args = parse_arguments()\n",
    "\n",
    "\n",
    "seeds = [2000, 2001, 2002, 2003, 2004]\n",
    "datasets = [\"imagenetr\", \"cifar100\", \"cub\", \"imageneta\", \"cars\", \"omnibenchmark\"]\n",
    "\n",
    "\n",
    "for seed in seeds:\n",
    "    for dataset in datasets:\n",
    "        args.seed = seed\n",
    "        args.dataset = dataset\n",
    "        args.device = \"cpu\"\n",
    "        args = update_args(args)\n",
    "\n",
    "        set_seed(args.seed)\n",
    "\n",
    "        main2_classes_per_task_and_seed(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a1a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description=\"Your script description here\")\n",
    "\n",
    "    # Define your arguments as before\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.0, help='Weight decay')\n",
    "    parser.add_argument('--early_stopping', type=int, default=10, help='Patience for early stopping')\n",
    "    parser.add_argument('--dataset', type=str, default='cifar100',\n",
    "                        choices=['cifar100', 'imagenetr', 'imageneta', 'vtab', 'cars', 'cub',\n",
    "                                 'omnibenchmark', 'dil_imagenetr', 'cddb', 'limited_domainnet'],\n",
    "                        help='Dataset to use')\n",
    "    parser.add_argument('--T', type=int, default=10, help='Number of timesteps')\n",
    "    parser.add_argument('--backbone', type=str, default='vit_base_patch16_224',\n",
    "                        choices=['vit_base_patch16_224', 'vit_base_patch16_224_in21k'],\n",
    "                        help='Backbone architecture')\n",
    "    parser.add_argument('--finetune_method', type=str, default='none',\n",
    "                        choices=['none', 'adapter', 'ssf', 'vpt'],\n",
    "                        help='Finetuning method')\n",
    "    parser.add_argument('--finetune_epochs', type=int, default=20, help='Number of finetuning epochs')\n",
    "    parser.add_argument('--k', type=int, default=5, help='Number of nearest neighbors')\n",
    "    parser.add_argument('--device', type=str, default='cuda' if 'cuda' in sys.modules else 'cpu',\n",
    "                        help='Device to use (cuda or cpu)')\n",
    "    parser.add_argument('--seed', type=int, default=2001, help='Random seed')\n",
    "    parser.add_argument('--data_root', type=str, default='./local_data', help='Root directory for datasets')\n",
    "    parser.add_argument('--moe_max_experts', type=int, default=4, help='Maximum number of experts for MoE')\n",
    "    parser.add_argument('--reduce_dataset', type=float, default=1.0, help='Fraction of dataset to use')\n",
    "    parser.add_argument('--gmms', type=int, default=8, help='Number of GMM components')\n",
    "    parser.add_argument('--use_multivariate', action='store_true', help='Use multivariate Gaussian')\n",
    "    parser.add_argument('--selection_method', type=str, default='random',\n",
    "                        choices=['random', 'around', 'eucld_dist', 'inv_eucld_dist', 'kl_div',\n",
    "                                 'inv_kl_div', 'ws_div', 'inv_ws_div'],\n",
    "                        help='Selection method')\n",
    "    parser.add_argument('--kd', action='store_true', help='Use knowledge distillation')\n",
    "    parser.add_argument('--kd_alpha', type=float, default=0.5, help='Alpha for knowledge distillation loss')\n",
    "    parser.add_argument('--log_gpustat', action='store_true', help='Log GPU statistics')\n",
    "    parser.add_argument('--sweep_logging', action='store_true', help='Enable Weights & Biases sweep logging')\n",
    "    parser.add_argument('--exit_after_T', action='store_true', help='Exit after T timesteps')\n",
    "    parser.add_argument('--selection_criterion', type=int, default=0, choices=[0, 1, 2],\n",
    "                        help='Selection criterion')\n",
    "    parser.add_argument('--tau', type=float, default=0.1, help='Temperature parameter')\n",
    "    parser.add_argument('--exit_after_acc', type=float, default=0.0, help='Exit after reaching this accuracy')\n",
    "    parser.add_argument('--trash_var', type=float, default=0.0, help='Trash variable (for testing)')\n",
    "    parser.add_argument('--use_adamw', action='store_true', help='Use AdamW optimizer')\n",
    "    parser.add_argument('--use_cosine_annealing', action='store_true', help='Use cosine annealing scheduler')\n",
    "    parser.add_argument('--aug_resize_crop_min', type=float, default=0.8, help='Min scale for random resize crop')\n",
    "    parser.add_argument('--aug_resize_crop_max', type=float, default=1.0, help='Max scale for random resize crop')\n",
    "    parser.add_argument('--aug_random_rotation_degree', type=float, default=0.0, help='Degree for random rotation')\n",
    "    parser.add_argument('--aug_brightness_jitter', type=float, default=0.0, help='Brightness jitter')\n",
    "    parser.add_argument('--aug_contrast_jitter', type=float, default=0.0, help='Contrast jitter')\n",
    "    parser.add_argument('--aug_saturation_jitter', type=float, default=0.0, help='Saturation jitter')\n",
    "    parser.add_argument('--aug_hue_jitter', type=float, default=0.0, help='Hue jitter')\n",
    "    parser.add_argument('--aug_normalize', action='store_true', help='Normalize input images')\n",
    "    parser.add_argument('--wandb_project', type=str, default='your_project_name', help='WandB project name')\n",
    "    parser.add_argument('--wandb_entity', type=str, default='your_entity_name', help='WandB entity name')\n",
    "\n",
    "    if '__file__' in globals():  # Check if running as a script\n",
    "        args = parser.parse_args()\n",
    "    else:  # Running in a Jupyter Notebook\n",
    "        args = parser.parse_args(args=[]) # Pass an empty list to avoid errors\n",
    "        # You can set default values here or handle arguments differently in the notebook\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ecd4406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 1796it [00:00, 1226069.33it/s]\n",
      "Iterate over test dataset: 8619it [00:00, 1487438.54it/s]\n",
      "Extracting Features: 100%|██████████| 5/5 [30:13<00:00, 362.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (1796, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = CUDA_VISIBLE_DEVICES\n",
    "args = parse_arguments()\n",
    "args.dataset = DATASET\n",
    "args = update_args(args)\n",
    "set_seed(args.seed)\n",
    "\n",
    "\n",
    "features, labels = main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f38ff5c",
   "metadata": {},
   "source": [
    "## Saving all features into file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd9af4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten erfolgreich als './local_data/vtab_class_features.csv' gespeichert (korrigiert).\n"
     ]
    }
   ],
   "source": [
    "dataset_name = DATASET\n",
    "labels_list = labels\n",
    "features_list = features\n",
    "\n",
    "# Stelle sicher, dass features_list eine Python-Liste ist\n",
    "if not isinstance(features_list, list):\n",
    "    features_list = features_list.tolist()  # Konvertiere zu Liste, falls es ein NumPy Array ist\n",
    "\n",
    "# Annahme: Jede innere Liste hat die gleiche Länge und entspricht einer Zeile\n",
    "# und die Elemente der inneren Liste sollen separate Spalten werden.\n",
    "\n",
    "# Erstelle Spaltennamen für die Features\n",
    "num_features = len(features_list[0]) if features_list else 0\n",
    "feature_columns = [f'feature_{i}' for i in range(num_features)]\n",
    "\n",
    "# Erstelle ein Dictionary für den DataFrame\n",
    "data = {'dataset': [dataset_name] * len(labels_list),\n",
    "        'label': [item[0] if isinstance(item, list) else item for item in labels_list]} # Annahme: Label ist das erste Element der inneren Liste\n",
    "for i, col in enumerate(feature_columns):\n",
    "    data[col] = [item[i] if isinstance(item, list) and len(item) > i else None for item in features_list]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Speichern als CSV-Datei\n",
    "csv_filename = f\"./local_data/{dataset_name}_class_features.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"Daten erfolgreich als '{csv_filename}' gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107c7137",
   "metadata": {},
   "source": [
    "## Rename class labels\n",
    "- Die Labels werden ranomised nach seed, werden im Modell aber immer normal durchnummeriert. Das führt aber zu einem missmatch mit den erstellten Features, weil die eine ganz bestimmte random Nummerierung haben, aber 0-99 gelabelt werden.  \n",
    "- Die Labelreihenfolge wurde bei DIL nicht gelogged. Ich beschränke mich auf CIL!\n",
    "- Features werden neu erstellt. Mit shuffle=False. Mit Logs (class order =>) überprüfen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f98cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefundene Zeile: class_order => [164, 106, 3, 122, 90, 148, 76, 193, 179, 177, 142, 192, 6, 43, 131, 30, 104, 20, 160, 115, 80, 69, 119, 55, 99, 78, 172, 185, 145, 8, 85, 47, 154, 13, 88, 195, 19, 178, 132, 51, 81, 70, 170, 87, 184, 113, 14, 7, 125, 117, 139, 79, 50, 107, 114, 97, 92, 174, 77, 41, 54, 150, 108, 35, 156, 149, 56, 130, 110, 40, 135, 196, 17, 91, 10, 173, 37, 22, 32, 100, 189, 12, 116, 66, 199, 71, 23, 161, 136, 94, 36, 158, 52, 67, 27, 187, 153, 63, 98, 33, 45, 74, 18, 93, 140, 59, 190, 165, 83, 15, 191, 5, 157, 123, 57, 138, 144, 53, 95, 166, 65, 49, 38, 25, 58, 11, 163, 75, 152, 89, 96, 159, 64, 34, 167, 168, 2, 176, 103, 102, 124, 29, 28, 73, 24, 180, 162, 84, 31, 197, 137, 126, 182, 133, 111, 61, 46, 155, 181, 188, 127, 21, 82, 169, 9, 143, 48, 109, 120, 183, 194, 146, 62, 112, 118, 129, 151, 175, 141, 86, 105, 72, 147, 39, 198, 121, 26, 44, 60, 171, 68, 42, 128, 1, 101, 186, 16, 134, 4, 0]\n",
      "Die Zeile mit 'class order => ...' ist: class_order => [164, 106, 3, 122, 90, 148, 76, 193, 179, 177, 142, 192, 6, 43, 131, 30, 104, 20, 160, 115, 80, 69, 119, 55, 99, 78, 172, 185, 145, 8, 85, 47, 154, 13, 88, 195, 19, 178, 132, 51, 81, 70, 170, 87, 184, 113, 14, 7, 125, 117, 139, 79, 50, 107, 114, 97, 92, 174, 77, 41, 54, 150, 108, 35, 156, 149, 56, 130, 110, 40, 135, 196, 17, 91, 10, 173, 37, 22, 32, 100, 189, 12, 116, 66, 199, 71, 23, 161, 136, 94, 36, 158, 52, 67, 27, 187, 153, 63, 98, 33, 45, 74, 18, 93, 140, 59, 190, 165, 83, 15, 191, 5, 157, 123, 57, 138, 144, 53, 95, 166, 65, 49, 38, 25, 58, 11, 163, 75, 152, 89, 96, 159, 64, 34, 167, 168, 2, 176, 103, 102, 124, 29, 28, 73, 24, 180, 162, 84, 31, 197, 137, 126, 182, 133, 111, 61, 46, 155, 181, 188, 127, 21, 82, 169, 9, 143, 48, 109, 120, 183, 194, 146, 62, 112, 118, 129, 151, 175, 141, 86, 105, 72, 147, 39, 198, 121, 26, 44, 60, 171, 68, 42, 128, 1, 101, 186, 16, 134, 4, 0]\n",
      "[164, 106, 3, 122, 90, 148, 76, 193, 179, 177, 142, 192, 6, 43, 131, 30, 104, 20, 160, 115, 80, 69, 119, 55, 99, 78, 172, 185, 145, 8, 85, 47, 154, 13, 88, 195, 19, 178, 132, 51, 81, 70, 170, 87, 184, 113, 14, 7, 125, 117, 139, 79, 50, 107, 114, 97, 92, 174, 77, 41, 54, 150, 108, 35, 156, 149, 56, 130, 110, 40, 135, 196, 17, 91, 10, 173, 37, 22, 32, 100, 189, 12, 116, 66, 199, 71, 23, 161, 136, 94, 36, 158, 52, 67, 27, 187, 153, 63, 98, 33, 45, 74, 18, 93, 140, 59, 190, 165, 83, 15, 191, 5, 157, 123, 57, 138, 144, 53, 95, 166, 65, 49, 38, 25, 58, 11, 163, 75, 152, 89, 96, 159, 64, 34, 167, 168, 2, 176, 103, 102, 124, 29, 28, 73, 24, 180, 162, 84, 31, 197, 137, 126, 182, 133, 111, 61, 46, 155, 181, 188, 127, 21, 82, 169, 9, 143, 48, 109, 120, 183, 194, 146, 62, 112, 118, 129, 151, 175, 141, 86, 105, 72, 147, 39, 198, 121, 26, 44, 60, 171, 68, 42, 128, 1, 101, 186, 16, 134, 4, 0]\n"
     ]
    }
   ],
   "source": [
    "def get_class_order_from_logs(run_id):\n",
    "    \"\"\"Lädt die Log-Datei eines WandB-Runs herunter und gibt die Zeile mit \"class order => ...\" zurück.\n",
    "\n",
    "    Args:\n",
    "        run_id (str): Die ID des WandB-Runs.\n",
    "\n",
    "    Returns:\n",
    "        str: Die Zeile, die mit \"class order => ...\" beginnt, oder None, falls nicht gefunden.\n",
    "    \"\"\"\n",
    "    api = wandb.Api()\n",
    "    try:\n",
    "        run = api.run(run_id)  # Ersetze mit deiner Entity und deinem Projekt\n",
    "        if run.files:\n",
    "            for f in run.files():\n",
    "                if f.name == 'output.log':  # Der typische Name für die stdout/stderr Logs\n",
    "                    log_content = f.download(root='.', replace=True, exist_ok=True)\n",
    "                    if log_content:\n",
    "                        with open(log_content.name, 'r') as log_file:\n",
    "                            for line in log_file:\n",
    "                                if line.startswith(\"class_order =>\"):\n",
    "                                    print(f\"Gefundene Zeile: {line.strip()}\")\n",
    "                                    os.remove(log_content.name) # Optional: Lösche die temporäre Log-Datei\n",
    "                                    return line.strip()                                \n",
    "                        os.remove(log_content.name) # Optional: Lösche die temporäre Log-Datei\n",
    "                        print(f\"Keine Zeile mit 'class order => ...' in der Log-Datei für Run {run_id} gefunden.\")\n",
    "                        return None\n",
    "        print(f\"Keine 'output.log'-Datei für Run {run_id} gefunden.\")\n",
    "        return None\n",
    "    except wandb.CommError as e:\n",
    "        print(f\"Fehler beim Abrufen des Runs {run_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Beispielhafte Verwendung\n",
    "run_path = \"belaschindler-university-hamburg/0schindl-LayUp_sweeps_question1/runs/crnqagtv\"  # Ersetze mit der tatsächlichen Run-ID\n",
    "class_order_line = get_class_order_from_logs(run_path)\n",
    "\n",
    "if class_order_line:\n",
    "    print(f\"Die Zeile mit 'class order => ...' ist: {class_order_line}\")\n",
    "else:\n",
    "    print(\"Die Zeile mit 'class order => ...' wurde nicht gefunden.\")\n",
    "\n",
    "class_order_line = np.array(class_order_line.lstrip(\"class_order =>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85981a9b",
   "metadata": {},
   "source": [
    "## Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "524ea74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten erfolgreich aus CSV eingelesen:\n",
      "Geladene Labels (erste 5): [0, 1, 8, 8, 9]\n",
      "Geladene Features (erste 1): [[1.1573697328567505, 0.3032321631908417, 0.4962291121482849, 0.1067860126495361, -1.625678539276123, 0.4844562709331512, 0.6799285411834717, 0.4303563237190246, -1.5096973180770874, -1.3390100002288818, -0.6796276569366455, 0.913780689239502, 0.3822452127933502, 1.5332729816436768, 0.0255593322217464, -3.4735898971557617, 0.6965407133102417, 0.8458954691886902, 0.5373067855834961, -0.2713418304920196, 1.312721848487854, 4.076414585113525, -0.0822739154100418, 0.0968436002731323, -1.4931050539016724, -2.24968695640564, -0.820317804813385, -0.6016552448272705, 4.658712387084961, 0.5360066294670105, 0.9967613816261292, -0.3212272822856903, 1.1852117776870728, -1.3273863792419434, -2.471297264099121, -1.567915439605713, -0.9555495381355286, -1.0097880363464355, -0.481315404176712, 0.1386799365282058, 1.704442024230957, -1.3899831771850586, -1.121779203414917, 0.5226563811302185, -1.858442783355713, -0.6145659685134888, 0.1786250621080398, -1.8990418910980225, -0.5900105834007263, -0.3777697682380676, 1.9702908992767332, -0.3790469169616699, -0.7254249453544617, 1.0229922533035278, 0.9347850680351256, 0.8521354794502258, -1.9892358779907229, 0.180534690618515, -0.6818726658821106, -0.6415033340454102, 3.4057188034057617, 1.9597833156585691, 2.545409679412842, -1.5826263427734375, 0.7595124244689941, -0.1670891493558883, 0.7933147549629211, 1.8728646039962769, 2.766567707061768, -2.756880044937134, -0.1095297411084175, 0.8669648170471191, 1.3038687705993652, 0.1632276028394699, 0.5288355350494385, 0.0956613719463348, -2.8777077198028564, -1.942079782485962, 0.2543229162693023, -1.5287625789642334, 0.370932787656784, -1.6248396635055542, 0.2660467028617859, 1.5553609132766724, 0.7200220227241516, 0.3872510194778442, -1.4725414514541626, 0.3974376022815704, 0.4719256460666656, -0.7479323744773865, 0.3530233800411224, -0.3388862907886505, -0.6031090617179871, 0.9655562043190002, 1.8243043422698972, -0.2754165828227997, -0.4594477117061615, -0.7902719378471375, 0.6803377270698547, 2.6872353553771973, -2.196385383605957, -0.6147477030754089, -1.955437183380127, -2.1979875564575195, -1.0861032009124756, 1.276048183441162, 1.472487211227417, -0.3634343147277832, -1.7783311605453491, -2.092790126800537, 0.274759441614151, 0.6624730825424194, -0.0416120886802673, 0.1924226582050323, 1.4950965642929075, -1.061061978340149, 1.798759937286377, -0.4079323709011078, -1.9084357023239136, 0.5004746317863464, -1.066125750541687, 0.5108792781829834, -0.6107761263847351, -1.2560343742370603, 1.7027912139892578, -0.9377214312553406, 1.073446273803711, 0.4651133418083191, 2.4597878456115723, -1.5959131717681885, 1.683177471160889, 1.400631070137024, -1.039159059524536, -1.2427232265472412, -2.093613147735596, -0.6478729844093323, 1.4663351774215698, 0.271146148443222, -0.7944361567497253, 1.329359531402588, 0.6037150025367737, 0.3171103596687317, 1.134770393371582, 0.1128562465310096, 1.9550906419754028, -1.1210442781448364, 0.8356449007987976, 1.9639954566955569, -0.1420966684818267, 2.66721773147583, 0.2322400659322738, -3.430408477783203, -0.7136045694351196, 0.6890458464622498, -0.9003425240516663, 0.3894060850143432, -2.833578824996948, -0.9213310480117798, -5.39215612411499, -6.951631546020508, 2.98551082611084, -0.8781229853630066, 0.9296172857284546, 0.7642894983291626, 1.89718759059906, -1.4849867820739746, -1.1971343755722046, 1.105269193649292, -0.2713381052017212, -2.358671188354492, -2.83137321472168, 2.0504825115203857, -0.8165058493614197, 0.2785830497741699, -1.8693994283676147, 0.2661425471305847, 1.1777753829956057, 0.3020936846733093, -0.397838145494461, -1.8003811836242676, -1.528811812400818, -2.361241817474365, -0.2370924800634384, 0.5176478624343872, -0.8931934237480164, 0.8023045659065247, 0.883167564868927, 0.1680191755294799, 0.3275890648365021, -0.8021509051322937, -1.486558437347412, -2.106263875961304, -0.4520125687122345, 1.769070744514465, -0.7201457619667053, 3.9346134662628174, -0.0688377320766449, 0.967870593070984, -1.353914499282837, 0.1132745668292045, -1.729216456413269, -1.322550892829895, -0.5361184477806091, -1.597991108894348, 1.2150518894195557, -0.3063098490238189, 0.9152274131774902, -0.3566848337650299, 1.4186252355575562, 0.3789978325366974, -0.2891305685043335, -0.6310831904411316, -0.8868058919906616, -0.9720649123191832, -1.3539563417434692, -0.6125133037567139, -0.8977087736129761, -0.0455839745700359, -1.608520746231079, -1.41051185131073, 3.3386855125427246, 1.398816466331482, 0.6526697874069214, 0.978437602519989, 1.6484596729278564, -0.8100585341453552, -0.8847041726112366, -1.2430790662765503, -0.9484313130378724, 0.0929872915148735, -0.5399129390716553, -0.7631791830062866, 1.1498557329177856, 1.1857422590255735, -0.827273428440094, 0.4501462876796722, 0.7276001572608948, 0.4694570004940033, 1.3312097787857056, 0.2464348375797271, 0.5426682829856873, 2.282599449157715, -1.029851317405701, 1.7334154844284058, -0.7396888732910156, 1.804209589958191, 0.9849130511283876, 1.0372494459152222, -0.5225798487663269, -1.173934817314148, -3.065009355545044, 0.5457509160041809, 0.1840508431196212, 1.6910979747772217, -0.569203794002533, -1.6751322746276855, -0.0917291045188903, 11.213448524475098, -2.374918937683105, -1.5099951028823853, -0.0241661239415407, -1.2830731868743896, 1.0706559419631958, 0.4110347032546997, -2.3268964290618896, 0.1236420720815658, 1.4602466821670532, 2.0249240398406982, 0.9700084924697876, -0.8797915577888489, -2.9466257095336914, 0.870021641254425, -0.6677054762840271, -0.7900902032852173, 0.0678280889987945, 0.9688130617141724, 3.586266040802002, -2.263399839401245, 1.256395936012268, 1.973777651786804, -1.3019567728042605, -1.1610732078552246, 0.9938591122627258, 0.3339430391788482, -0.2122329771518707, -2.5426087379455566, 0.5914576649665833, 0.7363156676292419, -2.234084367752075, -1.04690420627594, -0.2814258635044098, 0.4972040355205536, -0.4127663671970367, 1.1585612297058103, 3.5559933185577397, -2.33845329284668, 3.228243589401245, -1.3798667192459106, 0.746592104434967, -0.2253297865390777, 0.7763383388519287, -0.853142499923706, 2.1479103565216064, -1.7068328857421875, -2.302097797393799, -0.3094692826271057, 1.8354874849319456, 2.5649869441986084, 1.124286651611328, -0.1140548437833786, 0.1368694454431533, 0.5345194339752197, 1.2311311960220337, -0.0566571019589901, -2.6834332942962646, -1.5700082778930664, -2.8303444385528564, -0.6493939757347107, 0.9941467046737672, 0.4911313951015472, 3.005856513977051, -2.442066431045532, 0.4899255037307739, -1.54942786693573, 0.8973602056503296, 0.1200231835246086, 0.3755146265029907, -0.6132969260215759, 2.504662036895752, 0.3230520784854889, 0.8007217049598694, 0.6731104254722595, 1.788701415061951, -0.4457523226737976, 0.3544619679450989, 0.9256198406219482, 0.2849946916103363, -1.178937315940857, 2.780799150466919, 1.3166804313659668, 1.974375605583191, 0.0237291790544986, 0.1161534637212753, -0.5671632289886475, -1.1286287307739258, 0.6571779251098633, -0.9224593043327332, -1.2536948919296265, 0.711965024471283, 1.4330322742462158, 1.4010213613510132, 0.0255186855792999, -0.0905370563268661, 0.5197294354438782, -1.0995978116989136, -0.6974313855171204, 1.5310359001159668, -0.6372406482696533, 3.934299945831299, -3.261046886444092, -1.1877413988113403, 2.215431928634644, 1.8026715517044067, -0.4106042683124542, -1.188099980354309, -0.2475895881652832, -0.3902066946029663, 0.6128835678100586, 0.1858554780483245, 1.1098426580429075, -1.2767820358276367, -0.9015039205551147, 2.744403600692749, -0.8597361445426941, -0.0348104201257228, -1.934170484542847, 1.5686171054840088, 0.122564285993576, -0.8490167856216431, 0.3275328576564789, -0.2463103085756302, -1.5269837379455566, -0.2643092274665832, -0.015890184789896, -0.3493101894855499, 0.4410282373428345, -0.5135743021965027, 0.2801426351070404, -0.7065088152885437, -0.637010931968689, -0.8386673927307129, 1.953520655632019, -1.923553466796875, -2.0093801021575928, -0.7832038998603821, 3.0702013969421387, -0.5816184878349304, 0.5146214365959167, -0.6911645531654358, -3.628182411193848, 0.976502001285553, 0.6374965310096741, -1.5398950576782229, -0.676297664642334, -0.2658583521842956, 1.3918980360031128, 1.0578031539916992, 0.4804104566574096, 0.0317168720066547, 1.415732502937317, -0.0816196426749229, -1.605307698249817, -2.350804328918457, 0.2419445365667343, -0.1806666105985641, 0.2265045344829559, 0.037560760974884, -0.5782549977302551, -0.7249630093574524, -0.7860044240951538, -0.7079320549964905, -0.8950897455215454, 1.7592570781707764, -0.61967533826828, -0.2628182470798492, 3.529757499694824, -0.739395797252655, 1.915133357048035, 0.6628563404083252, -2.653858423233032, 0.7909558415412903, -2.1061623096466064, -1.0943069458007812, -0.7428879737854004, 0.7441423535346985, -0.949028253555298, -2.1002063751220703, 1.1374238729476929, -0.657874345779419, 0.918766438961029, -1.6043652296066284, 2.4121105670928955, 1.6831538677215576, -1.1808671951293943, -0.160061776638031, -1.5381402969360352, 0.3177764415740967, 2.9060075283050537, -1.968795657157898, -0.6845800876617432, -3.002744197845459, 0.4728595018386841, -0.2442863583564758, 1.9929074048995967, -0.2783063352108001, -1.5242269039154053, -0.773276150226593, -0.7168353796005249, -1.052525520324707, -4.201510906219482, -0.1449865847826004, 0.7556872367858887, 0.7233242392539978, -2.4607787132263184, 0.5542234182357788, -1.820585250854492, -0.676568865776062, 0.1014227047562599, 1.2403100728988647, 0.6066350340843201, 0.6112269163131714, -4.493571758270264, -0.1019641906023025, -3.281891345977783, 0.1446079164743423, 0.5069479942321777, 0.8085293769836426, -1.5194858312606812, -0.1682416796684265, -2.327400684356689, 0.2155934125185012, 0.233976125717163, 1.3839704990386963, -0.7597256898880005, 1.4058915376663208, -0.425040066242218, -0.2758098542690277, -1.753386616706848, -0.7445430755615234, -1.146231770515442, -2.05078649520874, -0.3276377916336059, -0.4424166977405548, -1.0682172775268557, -0.721051037311554, 0.1555096805095672, 1.0624170303344729, -0.8579257130622864, 1.1169110536575315, -2.3698160648345947, 0.0740807056427002, 0.7453285455703735, -2.9193320274353027, -0.5593802332878113, -0.9790890216827391, -1.7424392700195312, 2.3177237510681152, -0.5412900447845459, 2.129610300064087, 4.700562000274658, -0.3543812036514282, 0.8689340949058533, -0.0668418258428573, -1.2888890504837036, -2.09055495262146, -0.7915573120117188, -2.462353229522705, -2.5796608924865723, -0.4636655747890472, 0.7353008389472961, -3.75476598739624, -1.2601360082626345, 0.2233097702264785, 0.9052436947822572, 1.4539803266525269, -1.2468756437301636, -1.2704545259475708, 0.9833208918571472, -0.7863901853561401, -1.5402051210403442, 5.44406795501709, 1.4577878713607788, 2.3302865028381348, 0.3131720125675201, 0.341974139213562, 1.4015696048736572, 1.90561842918396, -1.9969708919525144, -0.5425966382026672, 3.037266969680786, 2.1303114891052246, -0.285036563873291, -2.119082450866699, 0.1719946861267089, -1.345405101776123, 1.967861294746399, 0.3169526755809784, 2.077280044555664, 0.2781782746315002, 0.4049171507358551, 1.9492874145507808, -1.0525933504104614, 1.887031078338623, -1.335652232170105, 0.6948937177658081, 1.1997219324111938, 1.0774630308151243, -2.05814528465271, 0.1005667895078659, 0.2378283143043518, -0.4148436188697815, -1.426469087600708, -0.4632815718650818, 0.4977401494979858, -1.7344281673431396, 0.9831309914588928, -1.2456992864608765, 0.723651111125946, 0.790474534034729, -0.0344994142651557, 0.5452526807785034, 1.4678990840911863, 0.6208139657974243, -1.53096604347229, -0.1793901175260543, 1.7663819789886477, -1.163877010345459, 0.9401105046272278, 1.28790283203125, 0.2049903720617294, -0.9365630745887756, -2.497594118118286, -0.7726141214370728, 1.0003055334091189, 0.9013044238090516, 1.8504769802093504, 0.9116519093513488, -0.8891882300376892, -0.6624175906181335, 4.947312831878662, -0.8282722234725952, -0.6071962714195251, 0.5907065272331238, -1.8904718160629272, -0.3314484357833862, -1.733993411064148, 1.6419620513916016, 0.3620076775550842, 1.859225273132324, -1.2752503156661987, -0.0338999256491661, 1.8407657146453855, 1.7944220304489136, -0.179179236292839, 0.7632281184196472, -1.1355663537979126, 0.510021984577179, 0.5099465250968933, 0.9987428784370422, 1.535051345825195, 1.3197519779205322, 0.8033618330955505, 2.756071090698242, 2.155235767364502, -3.570488691329956, -0.6333770751953125, -0.5960471034049988, -0.2070908993482589, 2.3209614753723145, -0.366549015045166, -0.7822290658950806, -0.438870906829834, -0.6464326977729797, 0.2686330676078796, 0.9447911977767944, -0.4902149438858032, 1.6732240915298462, -1.1201528310775757, -2.622171401977539, -2.514112710952759, -0.2917584776878357, -2.187974214553833, -2.061380386352539, -1.3469852209091189, 0.8091508746147156, 0.1809094697237014, -0.0410486757755279, -0.4849386811256408, -0.2693567872047424, -0.2065831273794174, 0.6818772554397583, 3.799898386001587, -1.0480692386627195, 0.8485561013221741, -2.9014780521392822, -0.7718744277954102, -2.619636058807373, -1.7586992979049685, 1.356658935546875, 3.2163684368133545, 1.0803840160369873, -1.4466924667358398, 0.6186626553535461, 0.7475140690803528, -1.3012510538101196, -1.8935285806655884, -1.4639205932617188, 1.1325603723526, -1.1041966676712036, 2.5491628646850586, -1.0173909664154053, 1.237464189529419, 1.4539220333099363, 2.02322006225586, 2.1106033325195312, 0.4629108607769012, 1.759903073310852, -2.343165874481201, -4.266570568084717, -0.1439028829336166, -1.4661169052124023, -0.6077178120613098, 0.7404848337173462, -4.648966312408447, 0.1637157499790191, 0.7254418730735779, 0.7898926138877869, 1.570784330368042, -1.1450988054275513, -0.8110930323600769, 0.4008024632930755, 0.0317988954484462, -1.6360557079315186, -1.2624356746673584, -0.4551742970943451, -1.0010288953781128, 1.0883253812789917, 1.381442666053772, 0.1241587698459625, 0.8332368731498718, -0.2929487526416778, 0.9846590161323548, -1.0528786182403564, 1.193160891532898, 0.4478768408298492, 0.0160429310053586, -1.3547301292419434, 0.3615188002586365, -0.6878611445426941, 0.3502135276794433, 0.4349992871284485, -1.4584306478500366, -0.0388935469090938, -0.0901323184370994, -1.4424418210983276, -0.4473301768302917, 0.6803356409072876, 0.6701406836509705, -1.1840252876281738, 1.951759934425354, 0.3299573063850403, 1.1317576169967651, 2.2868552207946777, 1.4646239280700684, -2.673621892929077, 1.5126254558563232, -1.003459334373474, -2.547570943832397, 0.1481288075447082, -0.3203576505184173, 1.9254181385040283, 0.1129922643303871, -0.6629936695098877, -0.3124794960021972, 0.6961076259613037, 1.2216774225234983, -0.8225799202919006, 1.5867440700531006, -2.9267756938934326, -0.3036612570285797, -1.094735622406006, -1.1627284288406372, 0.7452458739280701, 1.8924633264541624, -1.2724848985671997, 0.2347618043422699, 0.8181158304214478, 0.7267674207687378, -0.2820219099521637, -1.5773645639419556, 1.987008810043335, 0.2250147610902786, -2.714522123336792, -1.1242879629135132, -1.4550774097442627, 1.6344375610351562, -1.4177054166793823, -1.5127153396606443, -3.959244966506958, 0.9798767566680908, 1.0097405910491943, 1.131766676902771, 2.896096706390381, 0.2887055873870849, 0.5288363099098206, 0.5088128447532654, 1.361661672592163, 1.5378657579421997, 0.5535393953323364, -2.0298328399658203, -1.6447323560714722, 0.045562770217657, 1.259479522705078, -0.4447833895683288, -2.82480525970459, 2.408638000488281, -0.4976060390472412, 0.271858662366867]]\n"
     ]
    }
   ],
   "source": [
    "csv_filename = f\"./local_data/class_features/class_features_{DATASET}.csv\"\n",
    "\n",
    "# Lese die CSV-Datei ein\n",
    "df_loaded = pd.read_csv(csv_filename)\n",
    "\n",
    "# Extrahiere die Labels und Features\n",
    "loaded_labels = df_loaded['label'].tolist()\n",
    "\n",
    "# Wenn die Features als separate Spalten gespeichert wurden (Szenario 1):\n",
    "if 'feature_0' in df_loaded.columns:\n",
    "    loaded_features = []\n",
    "    i = 0\n",
    "    while f'feature_{i}' in df_loaded.columns:\n",
    "        loaded_features.append(df_loaded[f'feature_{i}'].tolist())\n",
    "        i += 1\n",
    "    # Transponiere die Liste von Listen, um die ursprüngliche Struktur wiederherzustellen\n",
    "    loaded_features = list(zip(*loaded_features))\n",
    "    loaded_features = [list(item) for item in loaded_features]\n",
    "\n",
    "# Wenn die Features als Listen in einer Zelle gespeichert wurden (Szenario 2):\n",
    "elif 'feature' in df_loaded.columns:\n",
    "    loaded_features = [ast.literal_eval(item) for item in df_loaded['feature'].tolist()]\n",
    "\n",
    "print(\"Daten erfolgreich aus CSV eingelesen:\")\n",
    "print(\"Geladene Labels (erste 5):\", loaded_labels[:5])\n",
    "print(\"Geladene Features (erste 1):\", loaded_features[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde39fe",
   "metadata": {},
   "source": [
    "## What are the classes per task?\n",
    "Ich brauche eine übersetzung.  \n",
    "Zusätzlich: Was ist mit den Seeds? Classenreihenfolge ist seed abhängig! Seed der runs berücksichtigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42e62d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterate over train dataset: 24000it [00:00, 977436.92it/s]\n",
      "Iterate over test dataset: 6000it [00:00, 1569920.40it/s]\n",
      "Accumulating classes per task: 100%|██████████| 10/10 [02:14<00:00, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per task saved in local_data/classes_per_task/CpT_imagenetr_2000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "args = parse_arguments()\n",
    "\n",
    "\n",
    "seeds = [2000, 2001, 2002, 2003, 2004]\n",
    "datasets = [\"imagenetr\", \"cifar100\", \"cub\", \"dil_imagenetr\", \"imageneta\", \"cars\", \"omnibenchmark\", \"limited_domainnet\"]\n",
    "\n",
    "\n",
    "for seed in seeds:\n",
    "    for dataset in datasets:\n",
    "        args.seed = seed\n",
    "        args.dataset = dataset\n",
    "        args.device = \"cpu\"\n",
    "        args = update_args(args)\n",
    "\n",
    "        set_seed(args.seed)\n",
    "\n",
    "        main2_classes_per_task_and_seed(args)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8006742",
   "metadata": {},
   "source": [
    "## Which expert learned which class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "71861470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /export/home/0schindl/.netrc\n"
     ]
    }
   ],
   "source": [
    "wandb.login(key=\"8a88a8c49d1c2d31b8677fe0b8eb7d3e3a031f83\")\n",
    "api = wandb.Api()\n",
    "\n",
    "\n",
    "def get_expert_distribution(run):\n",
    "    if run.state != \"finished\":\n",
    "        return None\n",
    "\n",
    "    history = run.history()\n",
    "    \n",
    "    expert_distributions = dict()\n",
    "    ft_tasks = [None] * 1000\n",
    "    ft_buffer = run.config.get(\"moe_max_experts\")\n",
    "    for line in run.history().columns:\n",
    "        if line.startswith(\"Expert\") and line.endswith(\"learned task\"):\n",
    "            \n",
    "            line_splited = line.split(\" \")\n",
    "            expert = int(line_splited[1])\n",
    "            tasks = history[line].dropna().tolist()\n",
    "            tasks = [int(task) for task in tasks]\n",
    "            \n",
    "            if expert not in expert_distributions:\n",
    "                expert_distributions[expert] = list()\n",
    "            expert_distributions[expert].extend(tasks) \n",
    "\n",
    "            \n",
    "            for i in tasks:\n",
    "                if i >= ft_buffer:\n",
    "                    ft_tasks[i - ft_buffer] = expert    \n",
    "\n",
    "    # cleaning ft_tasks\n",
    "    ft_tasks = [i for i in ft_tasks if i is not None]\n",
    "\n",
    "    return ft_tasks, expert_distributions\n",
    "\n",
    "def get_sweep_data(runs, attributes_config=[], attributes_summary=[], include_run_id=True, class_similarity=False):\n",
    "    sweep_data = []\n",
    "    for run in runs:\n",
    "        config = run.config\n",
    "        summary = run.summary\n",
    "\n",
    "        if summary.get(\"task_mean/acc\") is not None and run.state == \"finished\":\n",
    "            run_data = dict()\n",
    "            if include_run_id:\n",
    "                run_data[\"run_id\"] = run.id\n",
    "            # Add the config attributes to the run_data dictionary\n",
    "            for attr in attributes_config:\n",
    "                run_data[attr] = config.get(attr)\n",
    "\n",
    "            # Add the summary attributes to the run_data dictionary\n",
    "            for attr in attributes_summary:\n",
    "                run_data[attr] = summary.get(attr)\n",
    "            \n",
    "            # average class similarity of learned classes per expert\n",
    "            if class_similarity and run_data[\"dataset\"] == DATASET:\n",
    "                _, task_distribution = get_expert_distribution(run)\n",
    "                print(f\"Run {run.id} - Task Distribution: {task_distribution}\")\n",
    "\n",
    "                # Map classes per task\n",
    "                classes_per_task = get_classes_per_task(run_data[\"dataset\"], run_data[\"seed\"])\n",
    "                for expert_id, learned_tasks in task_distribution.items():\n",
    "                    classes= []\n",
    "                    for task in learned_tasks:\n",
    "                        classes.extend(classes_per_task[task])\n",
    "                    task_distribution[expert_id] = classes\n",
    "            \n",
    "\n",
    "                # Calculate inter-class similarity for each expert\n",
    "                expert_similaritys = list()\n",
    "                for expert_id, learned_classes in task_distribution.items():\n",
    "                    if len(learned_classes) > 1:\n",
    "                        label_mask = np.isin(loaded_labels, learned_classes)\n",
    "\n",
    "                        # Verwende die Maske, um die entsprechenden Features und Labels auszuwählen\n",
    "                        expert_features = loaded_features[label_mask]\n",
    "                        expert_labels = loaded_labels[label_mask]\n",
    "                        print(expert_labels)\n",
    "                        print(f\"Expert {expert_id} learned classes: {learned_classes}\")\n",
    "                        print(f\"Shape: {expert_features.shape}\")\n",
    "                        similarity = calculate_intra_class_similarity_optimized(expert_features, expert_labels, similarity_metric='cosine')[1]\n",
    "                        print(f\"Expert {expert_id} - Intra-Class Similarity: {similarity:.4f}\")\n",
    "                        expert_similaritys.append(similarity)\n",
    "\n",
    "                run_data[\"expert_similaritys\"] = expert_similaritys\n",
    "                average_similarity = np.mean(expert_similaritys) if expert_similaritys else 0.0       \n",
    "                run_data[\"average_expert_similarity\"] = average_similarity\n",
    "                print(f\"Average Expert Similarity: {average_similarity:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            sweep_data.append(run_data)\n",
    "\n",
    "    return sweep_data\n",
    "\n",
    "\n",
    "def get_classes_per_task(dataset, seed):\n",
    "    \"\"\"\n",
    "    Get the classes per task for a given dataset and seed.\n",
    "    \"\"\"\n",
    "    csv_path = f\"local_data/classes_per_task/CpT_{dataset}_{seed}.csv\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"CSV file {csv_path} does not exist.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    classes_per_task = {}\n",
    "    for _, row in df.iterrows():\n",
    "        task = row['task']\n",
    "        class_label = row['class']\n",
    "        if task not in classes_per_task:\n",
    "            classes_per_task[task] = []\n",
    "        classes_per_task[task].append(class_label)\n",
    "\n",
    "    return classes_per_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a625922",
   "metadata": {},
   "source": [
    "## Average similarity per expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d8671ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"omnibenchmark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5f97396e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten erfolgreich aus CSV eingelesen:\n",
      "Geladene Labels (erste 5): [0, 0, 0, 0, 0]\n",
      "Geladene Features (erste 1): [[1.9689552783966064, 0.0870765522122383, -0.0113297663629055, 0.6839339137077332, 0.1754369139671325, 0.9810996055603028, -0.4749040901660919, 1.8897793292999268, -0.4923095107078552, -0.4473477303981781, -1.486798882484436, 0.8747390508651733, -0.0753733217716217, -0.6563200950622559, 0.718726634979248, -0.4465139806270599, 1.195293664932251, 0.6493381857872009, 0.6927939653396606, -0.5141491293907166, 0.8577017188072205, -0.3272809982299804, -1.0333484411239624, 0.4337775409221649, -1.115162372589111, -0.0877170041203498, 1.3080565929412842, 0.7482463121414185, -0.8268628716468811, -0.1584079265594482, -0.5005767941474915, -1.4997069835662842, 2.248469591140747, -1.0296906232833862, -0.1423937827348709, -0.4433310627937317, -0.1288578808307647, 0.3147761523723602, -0.3346352875232696, -0.6630178689956665, -0.0858169347047805, -0.3058180212974548, -0.4738557934761047, -0.5707864165306091, 0.1124154329299926, 0.3615204691886902, 0.2311349958181381, 1.3265273571014404, -0.3158140480518341, 0.6687760949134827, -1.4003180265426636, 1.3144055604934692, -0.6250783801078796, 0.0503050722181797, 0.9442912340164183, 0.7843559980392456, -0.3371094763278961, -1.4575541019439695, 0.4054116904735565, 0.900941789150238, 0.6718043088912964, -0.838424801826477, -0.5250180959701538, -0.04365886002779, -0.2649962306022644, 0.0801498144865036, -0.0823972597718238, -0.9682557582855223, -0.3257511556148529, -1.528015375137329, -0.6843653321266174, -0.0470639690756797, -0.9211956858634948, 0.1493757218122482, 0.3160102665424347, -0.6079080700874329, 0.17124605178833, 0.6866927146911621, -0.0375284366309642, 2.0011980533599854, 1.0218274593353271, 2.114463806152344, 0.3921245336532593, 0.351257860660553, -1.2415977716445925, 0.4142159819602966, -0.3008414804935455, -0.9714871644973756, 0.8583556413650513, 0.3560821413993835, -0.4794554114341736, -0.2030420303344726, 1.0438686609268188, -0.2999566495418548, -0.5600224137306213, -0.5703715682029724, 0.3364681303501129, 0.2229752540588379, -0.5097177624702454, -0.924290657043457, -0.3382710218429565, -0.8112733960151672, -0.7246159315109253, -0.6018049120903015, -0.9170442223548888, 0.624413788318634, 0.8702399730682373, -0.3759670555591583, -0.2358667850494384, -0.1769316494464874, -1.0672794580459597, -0.978075921535492, -1.3394759893417358, -1.8361555337905884, -1.197524905204773, 0.5263850092887878, 0.6293108463287354, 0.3249465823173523, 0.8350753784179688, -0.0215189158916473, 0.4382273256778717, -0.1823172122240066, -0.0436647161841392, -0.5266821980476379, 0.4585236310958862, 0.4162185192108154, -0.4495692551136017, -0.2695445418357849, -0.1687269508838653, 0.5573973059654236, 2.0994348526000977, 1.6399930715560913, -1.348718285560608, -0.0197410732507705, 0.8777657747268677, 0.3267890810966491, 0.2517587840557098, 1.2427027225494385, -0.5926951169967651, -2.052478790283203, 0.0221452433615922, -0.4519194662570953, -0.2554469704627991, -1.7485032081604004, -0.7468812465667725, 1.0476102828979492, -0.029381887987256, 0.9455466270446776, 1.7345077991485596, -1.482868194580078, 0.4788756668567657, -1.5266422033309937, -0.1505209803581237, -0.8362574577331543, -1.1640771627426147, 0.4017622768878937, 0.6965913772583008, 0.2832098305225372, 0.34397092461586, -3.724332332611084, 0.7911558151245117, 0.6073941588401794, 0.6375910639762878, -0.2817418277263641, -1.4493296146392822, 1.6187450885772705, -0.1646745651960373, 0.8777540326118469, -1.2332819700241089, -0.4535413086414337, -0.2708575129508972, -0.934327244758606, -0.1846679598093032, -1.798369288444519, -0.8783950805664062, 1.4746443033218384, 0.0481357164680957, -0.9114179611206056, 2.403722047805786, -0.5884090065956116, 1.593903422355652, -0.1343821585178375, 0.0733548104763031, -0.9463520646095276, 1.0835368633270264, 0.5758939981460571, -0.4456530809402466, -0.031411875039339, 0.091895081102848, 0.2571477890014648, -1.5536211729049685, 0.6843160390853882, -2.414607048034668, 0.0690121874213218, -0.576460599899292, 0.8812568187713623, -0.2166927009820938, 0.5836583375930786, -1.1174514293670654, -1.422649621963501, -0.0737050026655197, -0.9192752242088318, 0.347643107175827, -0.6785198450088501, -0.8707731366157532, 0.9270633459091188, -1.388634443283081, -0.8063857555389404, 1.2451791763305664, 1.0815773010253906, 1.6331802606582642, -0.4015505015850067, 1.0122313499450684, -0.6857444643974304, -1.069170594215393, 1.0569320917129517, -1.5054415464401243, -1.2640838623046875, -0.3324554562568664, -1.7258087396621704, 0.4861282706260681, 1.3725045919418335, -1.411655306816101, 0.5470597147941589, 0.8565680980682373, 0.1059586033225059, 0.7204264998435974, 0.11281618475914, -1.286937236785889, -0.6109617948532104, 0.5919374227523804, -0.458566814661026, 0.0877837613224983, -0.3327260315418243, -0.5801783204078674, -1.438846230506897, 0.0342796593904495, -0.5123595595359802, 0.1969611644744873, 0.109683983027935, 1.0493128299713137, -2.767911672592163, 0.7719711065292358, -1.3535112142562866, 0.4423124194145202, 1.087363600730896, 1.831966757774353, 0.9258410334587096, -0.3295230865478515, 0.7305727601051331, -0.4704266786575317, -1.265802025794983, 0.4263213872909546, -1.0407551527023315, -0.9686982035636902, 0.1061637103557586, -0.498120903968811, -1.5679467916488647, -1.1435929536819458, 0.9611793160438538, -0.2604821920394897, 0.4722326397895813, -0.998485028743744, -0.2160274088382721, -1.0148342847824097, -1.3989063501358032, -0.9941293597221376, -0.4519583880901336, 1.2737644910812378, 0.0004700189456343, 0.0306346081197261, -0.0603100545704364, 1.3648682832717896, 0.8792780041694641, 0.7940391302108765, -0.8115297555923462, -2.926309823989868, -1.0077944993972778, 1.3002030849456787, -0.8209447860717773, 0.3053966760635376, 0.9663523435592652, -0.9278683662414552, -0.3411401808261871, 0.0988837331533432, -0.1762384176254272, -0.6241993308067322, 0.139944776892662, -0.9628466367721558, 1.371546506881714, 0.4218983650207519, -0.5675315856933594, -0.3839106261730194, -0.4538677334785461, 0.9363070726394652, -0.1757768392562866, -0.3764173984527588, -3.1261620279110502e-06, 0.5797271728515625, 0.7876668572425842, 0.3470506966114044, -1.0414592027664185, 0.8585985898971558, -1.5902788639068604, 0.3417757749557495, -1.1603984832763672, 0.1668338179588318, -1.700002908706665, -0.1780354380607605, -0.1711951345205307, -0.8697648644447327, -1.3808976411819458, 0.5998808741569519, -0.8245865702629089, 0.1063021272420883, 0.7671781182289124, 0.6302599906921387, -0.0630506351590156, -0.0275604203343391, 0.6601308584213257, 0.5908382534980774, 0.1844844222068786, -0.5136731863021851, -0.923399031162262, -0.9849084615707396, 0.6191586256027222, -0.131004512310028, 0.60540372133255, 0.2146104723215103, 0.2593089342117309, -0.6819142699241638, -0.7795776724815369, 0.790753960609436, -1.0378386974334717, -1.7305009365081787, -0.7212411761283875, 0.6849874258041382, 1.627040982246399, -0.5176625847816467, 0.1703133434057235, 0.0175877232104539, 1.2397398948669434, 0.788981020450592, -1.2453269958496094, 0.6108480095863342, 0.4439892172813415, -0.0462058074772357, 0.1855886876583099, -0.7953251600265503, 0.5294578671455383, 0.7172064781188965, 0.4045091271400451, -0.6044144034385681, -0.962772250175476, -0.8562923073768616, 1.4061760902404783, 0.5265116095542908, -0.3434552550315857, -1.3434464931488037, 0.2249708771705627, -0.8732317686080933, 0.3794094622135162, -1.370846390724182, 1.0844407081604004, 0.7877753376960754, -0.8813299536705017, -0.8382021188735962, 0.310340017080307, 0.4160161912441253, 0.269457995891571, -1.0917773246765137, 1.2549644708633425, -0.8108235001564026, -0.2587461173534393, 0.0914743840694427, 0.468217521905899, 0.502362072467804, 0.5476263761520386, 0.7009240984916687, -0.7938022613525391, 0.784479558467865, -0.162680834531784, -0.5703704357147217, -0.5764976739883423, 1.0978178977966309, -0.8902842998504639, -0.8106321692466736, -0.1418221443891525, 0.3762727677822113, -1.1287827491760254, -1.0014259815216064, -1.8644959926605225, -0.7733258008956909, 1.2764264345169067, 1.3709639310836792, 0.948666214942932, -0.3419846892356872, -0.1815898269414901, 0.661577582359314, 1.6982042789459229, 0.6124780178070068, 1.246635675430298, 0.1324605196714401, -1.2507344484329224, -1.469460368156433, -2.404652833938598, 0.7805041670799255, -0.8630419373512268, -0.4713888466358185, -1.6773340702056885, -0.1993376463651657, 0.1855036318302154, -0.5371244549751282, 0.1093795076012611, -1.475702881813049, -0.5632458329200745, 0.1412024050951004, -0.487106442451477, -0.4276685714721679, 0.9130916595458984, -0.2501359581947326, 0.8043559193611145, 0.6915495991706848, 0.6025705933570862, 1.2437137365341189, -1.0236685276031494, -0.3282265663146972, -1.0884857177734375, 1.094208836555481, -0.1033460795879364, 1.7310643196105957, 0.1624075025320053, -0.0735263079404831, -0.4978738129138946, 0.2019554078578949, 0.9617903232574464, -0.414988100528717, -0.1390879601240158, 0.6555832624435425, 0.923734486103058, 0.7520061731338501, -0.4366369545459747, -0.7217284440994263, -1.2399718761444092, 0.1046584621071815, -2.927581548690796, 1.307013988494873, 1.1697320938110352, 0.4454294443130493, 0.1002145931124687, 0.9193329811096193, -0.947654128074646, 0.3366950154304504, -1.6179206371307373, -0.9038116931915284, -0.980124056339264, -1.4252426624298096, -1.8086105585098269, -1.7159337997436523, -0.5325440168380737, -0.5184749364852905, -0.5630776882171631, -0.803447961807251, 0.1118193194270134, -1.471359133720398, -1.030017971992493, -0.8716141581535339, -0.2050879150629043, 1.0618163347244265, -0.5522522330284119, 1.1489579677581787, -3.060962677001953, -1.864097595214844, -0.6256697773933411, -0.2975647449493408, 0.328555554151535, -1.717778563499451, 1.0412020683288574, 0.3749531805515289, -0.614967942237854, -0.3766430020332336, 0.9147406220436096, -0.7631345987319946, 1.4673113822937012, 0.4640187919139862, -0.0868741273880004, 0.1439138948917389, 0.2346457093954086, -0.9884443283081056, -0.6117910146713257, -1.181307315826416, -0.2408533096313476, -0.0979222729802131, 0.4307217299938202, -0.5569734573364258, 1.1190431118011477, 0.3919963538646698, -0.9378581047058104, -0.5474919676780701, 1.2668538093566897, 0.4238526821136474, -0.418793648481369, -0.4735842943191528, -0.5964969396591187, 0.4253396987915039, 0.2057480961084365, 0.9370281100273132, -0.1954152882099151, 0.5975437164306641, -1.6819528341293335, 1.782920241355896, 0.2880727648735046, -0.0340004749596118, 0.1200162023305893, -0.5599491000175476, -1.2597519159317017, -0.983774483203888, -0.0783254727721214, 1.2127705812454224, -0.3301267623901367, -1.3712552785873413, 1.7137765884399414, 1.1988989114761353, 0.8349754810333252, 0.3565137982368469, -0.0598149523138999, -0.3281928896903991, -0.1209703758358955, -0.2839822173118591, -1.8650074005126955, 3.9661190509796143, -1.4993385076522827, -0.1553487330675125, 0.5477076768875122, -0.1948363631963729, -1.3700542449951172, -0.0588095970451831, 0.4509493112564087, 0.6876735091209412, 0.762881875038147, 0.7830736637115479, 0.0686917379498481, 0.4472704827785492, 0.0309970825910568, -1.0164024829864502, 0.3864448070526123, -1.6133183240890503, 1.4719711542129517, 1.4164841175079346, -0.5465328693389893, 0.8775874972343445, -0.1926915794610977, 0.1194290220737457, -0.3221461772918701, -0.3311983942985534, -0.5705974698066711, -0.5871062278747559, 1.5963054895401, -0.1914678514003753, 0.2158244103193283, -0.4534414112567901, -0.2437984347343444, -1.281408667564392, 1.5466890335083008, -1.043653964996338, 0.3700222969055176, 0.7540575265884399, 0.0366883836686611, 1.4391814470291138, 0.2486212998628616, -0.5823108553886414, 0.589770495891571, 0.3734836876392364, -0.4924716651439667, 0.219122365117073, 0.6665638089179993, 0.2899386584758758, -0.8249241709709167, 0.6899924874305725, 1.6570998430252075, -0.1160163134336471, -1.3076975345611572, 0.2241067737340927, -0.6114881634712219, 0.0997058227658271, 1.075042724609375, 0.6505340337753296, -0.5318902730941772, -0.7435432076454163, 2.873210906982422, -2.169210433959961, 0.1867130696773529, -0.3511930108070373, 0.713888943195343, -0.2067825943231582, 0.7971041202545166, 0.8673087358474731, 0.1448083519935608, -0.280807763338089, 1.328657865524292, -1.6169949769973757, -1.5370465517044067, -1.2151402235031128, -0.0388606823980808, -1.7192580699920654, 0.6847070455551147, -0.5306428670883179, 0.4597605466842651, -0.3927139639854431, 1.141195297241211, 1.106452703475952, 1.554071068763733, -1.3116655349731443, 0.3232114315032959, 0.9319978952407836, -0.304185688495636, 0.4697479009628296, -0.4112680554389953, 0.3410215973854065, -0.2391914129257202, 0.1008083969354629, 1.916747450828552, -0.1548202633857727, 0.2403917759656906, 0.8555892705917358, -0.7567880749702454, -0.2653051614761352, 0.2784293889999389, -0.6637390851974487, -0.97767972946167, 1.0520058870315552, -0.6385751366615295, -0.2939068377017975, 1.367349624633789, 0.3038011491298675, -0.5148887038230896, 0.6318219304084778, -1.2163163423538208, 0.4823535680770874, -0.7287096381187439, 0.0597502700984478, 0.6499109268188477, -0.9786677956581116, -0.2985012829303741, -0.3247742056846618, 0.4560454189777374, 0.0045392024330794, 0.1286899596452713, 1.5648093223571775, -0.2574817836284637, 0.946767508983612, 1.070425033569336, -1.505743384361267, -0.9403533935546876, 0.5427858829498291, 0.5143972039222717, -0.1309396624565124, 1.2857917547225952, 0.617129921913147, 0.1296725422143936, 0.4041259288787842, 0.1229711845517158, -0.0739745795726776, -0.7518020868301392, -0.6063719391822815, 0.4058533906936645, -0.002609689021483, 0.0048914835788309, -1.0295406579971311, 1.0969960689544678, -0.2990254759788513, 1.1385831832885742, 0.3404732346534729, 0.4923071563243866, 0.586788535118103, -1.050565481185913, -0.3397498726844787, -0.611656665802002, 0.4018059372901916, -0.6724894046783447, -0.9358675479888916, 0.4085202217102051, -0.100187063217163, -0.4141192138195038, -0.09134241938591, 0.0356746390461921, 0.7135429978370667, 0.32391557097435, 0.2930780053138733, -0.195089966058731, -0.6588378548622131, 0.8081645369529724, -1.064146637916565, -0.738598108291626, 0.2618471980094909, -1.4306142330169678, 0.1363427191972732, -0.6087706089019775, 1.1444966793060305, 0.2388997822999954, 1.4253950119018557, -0.7571117877960205, -1.2390213012695312, -0.7905393242835999, 0.3977026045322418, -1.858884572982788, -0.3444458544254303, -0.9909963011741638, 0.5313562154769897, -0.773621678352356, 0.2986519932746887, 2.170130014419556, -0.2039424777030944, -0.4983775019645691, -1.5649768114089966, -0.3875662982463836, 0.1566835045814514, 1.974769234657288, -0.7001925110816956, -0.473753273487091, 1.2641897201538086, 0.786517858505249, 0.1624618619680404, -0.0708547830581665, -0.0271193515509367, 0.7636047601699829, 0.3481412827968597, -0.040556002408266, -1.0245037078857422, 0.0719869881868362, -0.4964458048343658, -0.8376736640930176, -1.638900876045227, -1.6807883977890017, -0.8152866363525391, 1.403356671333313, 0.4989112317562103, 0.1326737403869629, 0.634868860244751, -0.5136812925338745, 1.7027751207351685, -0.2317236065864563, -0.4959739744663238, 0.6998757123947144, -1.3789767026901243, -0.4751293659210205, 0.565834641456604, 0.2869698703289032, -0.9781095385551452, 0.1159094721078872, -0.3634070456027984, 1.0277687311172483, -0.315434455871582, 0.5313920974731445, 0.5012248754501343, -0.6457148790359497, 0.2235594242811203, -1.2516605854034424, -0.2562712430953979, -0.444747120141983, -1.7778542041778564, 0.6639769673347473, 0.7007946372032166, -1.5955671072006226, 0.3554822504520416, 0.6321107149124146, 1.3237990140914917, 0.4999330341815948]]\n"
     ]
    }
   ],
   "source": [
    "## Load data from file\n",
    "csv_filename = f\"./local_data/class_features/class_features_{DATASET}.csv\"\n",
    "\n",
    "# Lese die CSV-Datei ein\n",
    "df_loaded = pd.read_csv(csv_filename)\n",
    "\n",
    "# Extrahiere die Labels und Features\n",
    "loaded_labels = df_loaded['label'].tolist()\n",
    "\n",
    "# Wenn die Features als separate Spalten gespeichert wurden (Szenario 1):\n",
    "if 'feature_0' in df_loaded.columns:\n",
    "    loaded_features = []\n",
    "    i = 0\n",
    "    while f'feature_{i}' in df_loaded.columns:\n",
    "        loaded_features.append(df_loaded[f'feature_{i}'].tolist())\n",
    "        i += 1\n",
    "    # Transponiere die Liste von Listen, um die ursprüngliche Struktur wiederherzustellen\n",
    "    loaded_features = list(zip(*loaded_features))\n",
    "    loaded_features = [list(item) for item in loaded_features]\n",
    "\n",
    "# Wenn die Features als Listen in einer Zelle gespeichert wurden (Szenario 2):\n",
    "elif 'feature' in df_loaded.columns:\n",
    "    loaded_features = [ast.literal_eval(item) for item in df_loaded['feature'].tolist()]\n",
    "\n",
    "print(\"Daten erfolgreich aus CSV eingelesen:\")\n",
    "print(\"Geladene Labels (erste 5):\", loaded_labels[:5])\n",
    "print(\"Geladene Features (erste 1):\", loaded_features[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1dd1b3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6dwyk5lm - Task Distribution: {4: [4, 5, 7], 0: [0, 6], 2: [2, 8, 9], 1: [1], 3: [3]}\n",
      "[  2   2   2 ... 299 299 299]\n",
      "Expert 4 learned classes: [2, 13, 49, 53, 61, 65, 66, 73, 75, 77, 81, 84, 94, 100, 102, 113, 128, 145, 154, 171, 199, 216, 224, 229, 243, 258, 277, 285, 288, 290, 6, 26, 29, 30, 39, 51, 103, 105, 110, 116, 117, 121, 142, 152, 172, 181, 182, 190, 198, 211, 215, 218, 221, 223, 235, 250, 257, 265, 279, 294, 10, 12, 22, 23, 55, 69, 76, 83, 93, 101, 140, 160, 161, 167, 168, 170, 176, 209, 233, 236, 244, 263, 269, 271, 280, 286, 289, 292, 293, 299]\n",
      "Shape: (26911, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 90/90 [00:00<00:00, 260.36class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 4 - Intra-Class Similarity: 0.5415\n",
      "[  5   5   5 ... 297 297 297]\n",
      "Expert 0 learned classes: [5, 17, 38, 50, 58, 67, 70, 74, 89, 99, 106, 114, 118, 119, 122, 130, 136, 137, 151, 166, 177, 197, 205, 210, 219, 220, 248, 252, 270, 297, 8, 16, 31, 34, 36, 42, 48, 60, 82, 86, 92, 104, 112, 120, 126, 138, 139, 143, 148, 158, 165, 185, 200, 201, 202, 228, 241, 275, 287, 296]\n",
      "Shape: (18264, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 60/60 [00:00<00:00, 243.82class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 0 - Intra-Class Similarity: 0.5497\n",
      "[  1   1   1 ... 298 298 298]\n",
      "Expert 2 learned classes: [32, 35, 57, 64, 68, 80, 95, 111, 115, 125, 131, 132, 146, 150, 155, 179, 187, 188, 193, 196, 212, 231, 232, 238, 240, 253, 262, 266, 267, 273, 7, 9, 21, 24, 37, 40, 44, 62, 78, 79, 85, 96, 97, 107, 134, 135, 141, 159, 174, 184, 208, 226, 242, 246, 256, 259, 260, 276, 278, 291, 1, 4, 14, 15, 28, 41, 43, 52, 54, 63, 71, 91, 109, 123, 124, 164, 178, 183, 192, 194, 206, 207, 214, 217, 247, 264, 268, 272, 284, 298]\n",
      "Shape: (26990, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 90/90 [00:00<00:00, 258.59class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 2 - Intra-Class Similarity: 0.5378\n",
      "[  3   3   3 ... 282 282 282]\n",
      "Expert 1 learned classes: [3, 19, 33, 45, 47, 59, 72, 90, 98, 127, 129, 156, 157, 169, 173, 175, 180, 195, 203, 222, 227, 230, 234, 237, 239, 249, 254, 274, 281, 282]\n",
      "Shape: (8709, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 30/30 [00:00<00:00, 272.36class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 1 - Intra-Class Similarity: 0.5009\n",
      "[  0   0   0 ... 295 295 295]\n",
      "Expert 3 learned classes: [0, 11, 18, 20, 25, 27, 46, 56, 87, 88, 108, 133, 144, 147, 149, 153, 162, 163, 186, 189, 191, 204, 213, 225, 245, 251, 255, 261, 283, 295]\n",
      "Shape: (8823, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 30/30 [00:00<00:00, 246.19class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 3 - Intra-Class Similarity: 0.5251\n",
      "Average Expert Similarity: 0.5310\n",
      "Run 7z247qq2 - Task Distribution: {0: [0], 1: [1, 8], 3: [3], 2: [2, 7], 4: [4, 5, 6, 9]}\n",
      "[  5   5   5 ... 298 298 298]\n",
      "Expert 0 learned classes: [5, 22, 24, 33, 43, 50, 52, 53, 73, 88, 99, 115, 121, 125, 130, 148, 161, 165, 170, 182, 223, 231, 234, 247, 252, 254, 268, 277, 281, 298]\n",
      "Shape: (8582, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 30/30 [00:00<00:00, 216.92class/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 0 - Intra-Class Similarity: 0.4892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3   3   3 ... 299 299 299]\n",
      "Expert 1 learned classes: [10, 13, 26, 34, 44, 64, 68, 71, 89, 90, 101, 107, 129, 137, 149, 155, 171, 173, 176, 190, 200, 218, 227, 242, 245, 249, 278, 282, 293, 297, 3, 23, 51, 54, 58, 60, 75, 78, 95, 106, 120, 136, 139, 156, 163, 166, 168, 183, 198, 202, 207, 215, 219, 222, 232, 251, 253, 258, 292, 299]\n",
      "Shape: (18139, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 60/60 [00:00<00:00, 262.10class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 1 - Intra-Class Similarity: 0.5505\n",
      "[ 11  11  11 ... 296 296 296]\n",
      "Expert 3 learned classes: [11, 21, 31, 39, 77, 81, 103, 109, 112, 122, 124, 141, 142, 147, 151, 153, 159, 174, 179, 196, 197, 208, 228, 244, 257, 273, 283, 288, 295, 296]\n",
      "Shape: (8916, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 30/30 [00:00<00:00, 265.16class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 3 - Intra-Class Similarity: 0.5361\n",
      "[  8   8   8 ... 290 290 290]\n",
      "Expert 2 learned classes: [8, 9, 12, 18, 36, 41, 57, 80, 82, 92, 94, 117, 123, 127, 140, 158, 172, 187, 188, 199, 210, 213, 217, 225, 233, 238, 243, 264, 276, 286, 17, 32, 35, 55, 59, 63, 74, 93, 100, 102, 105, 143, 144, 146, 157, 164, 177, 181, 195, 201, 203, 211, 216, 229, 230, 236, 255, 265, 289, 290]\n",
      "Shape: (18213, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 60/60 [00:00<00:00, 251.79class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 2 - Intra-Class Similarity: 0.5626\n",
      "[  0   0   0 ... 294 294 294]\n",
      "Expert 4 learned classes: [4, 27, 28, 30, 38, 45, 48, 49, 69, 72, 83, 87, 108, 116, 118, 133, 135, 138, 162, 180, 185, 206, 214, 248, 259, 261, 262, 263, 275, 287, 6, 15, 16, 19, 29, 56, 62, 67, 76, 79, 84, 97, 98, 104, 111, 119, 152, 175, 184, 191, 204, 205, 209, 221, 235, 237, 240, 246, 270, 291, 0, 1, 2, 37, 40, 46, 47, 61, 65, 66, 85, 91, 110, 114, 131, 132, 134, 150, 154, 167, 169, 189, 194, 212, 224, 241, 271, 279, 280, 284, 7, 14, 20, 25, 42, 70, 86, 96, 113, 126, 128, 145, 160, 178, 186, 192, 193, 220, 226, 239, 250, 256, 260, 266, 267, 269, 272, 274, 285, 294]\n",
      "Shape: (35847, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 120/120 [00:00<00:00, 258.18class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 4 - Intra-Class Similarity: 0.5279\n",
      "Average Expert Similarity: 0.5333\n",
      "Run 5b5h2uzl - Task Distribution: {3: [3], 0: [0, 6], 4: [4], 1: [1], 2: [2, 5, 7, 8, 9]}\n",
      "[  1   1   1 ... 285 285 285]\n",
      "Expert 3 learned classes: [1, 4, 14, 16, 27, 29, 45, 51, 61, 77, 97, 105, 131, 141, 143, 167, 169, 170, 177, 181, 198, 203, 211, 223, 239, 247, 256, 265, 282, 285]\n",
      "Shape: (8902, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 30/30 [00:00<00:00, 256.33class/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 3 - Intra-Class Similarity: 0.4967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 34  34  34 ... 299 299 299]\n",
      "Expert 0 learned classes: [34, 48, 54, 60, 64, 74, 79, 99, 100, 107, 128, 136, 144, 162, 165, 188, 189, 197, 213, 219, 224, 226, 237, 240, 246, 251, 259, 263, 294, 299, 53, 66, 75, 89, 90, 93, 96, 102, 110, 118, 120, 124, 148, 172, 174, 179, 195, 200, 201, 206, 207, 208, 216, 230, 235, 249, 250, 274, 283, 290]\n",
      "Shape: (18092, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 60/60 [00:00<00:00, 259.28class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 0 - Intra-Class Similarity: 0.5529\n",
      "[  2   2   2 ... 293 293 293]\n",
      "Expert 4 learned classes: [2, 8, 24, 35, 36, 40, 41, 44, 57, 59, 70, 76, 83, 112, 130, 133, 142, 146, 147, 156, 158, 168, 194, 205, 221, 225, 253, 257, 273, 293]\n",
      "Shape: (9181, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 30/30 [00:00<00:00, 253.15class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 4 - Intra-Class Similarity: 0.5571\n",
      "[  9   9   9 ... 297 297 297]\n",
      "Expert 1 learned classes: [9, 17, 19, 25, 28, 31, 32, 38, 39, 56, 62, 63, 68, 72, 98, 116, 125, 161, 182, 190, 193, 204, 220, 242, 243, 248, 270, 272, 280, 297]\n",
      "Shape: (8873, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 30/30 [00:00<00:00, 265.90class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 1 - Intra-Class Similarity: 0.5236\n",
      "[  0   0   0 ... 298 298 298]\n",
      "Expert 2 learned classes: [13, 20, 23, 30, 55, 65, 71, 81, 86, 87, 91, 122, 138, 149, 150, 153, 164, 166, 180, 187, 191, 214, 217, 227, 234, 236, 241, 255, 289, 295, 18, 37, 46, 50, 52, 58, 69, 84, 92, 108, 113, 121, 123, 135, 145, 163, 171, 178, 186, 196, 202, 209, 228, 233, 252, 254, 261, 266, 281, 296, 0, 3, 15, 22, 67, 82, 88, 95, 101, 114, 119, 126, 140, 154, 155, 160, 173, 175, 218, 232, 244, 245, 260, 268, 271, 275, 284, 286, 292, 298, 10, 11, 12, 21, 42, 43, 47, 49, 73, 78, 80, 94, 103, 104, 115, 127, 132, 134, 137, 151, 159, 176, 183, 184, 185, 199, 264, 267, 269, 279, 5, 6, 7, 26, 33, 85, 106, 109, 111, 117, 129, 139, 152, 157, 192, 210, 212, 215, 222, 229, 231, 238, 258, 262, 276, 277, 278, 287, 288, 291]\n",
      "Shape: (44649, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 150/150 [00:00<00:00, 258.07class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 2 - Intra-Class Similarity: 0.5360\n",
      "Average Expert Similarity: 0.5333\n",
      "Run fs3coo40 - Task Distribution: {0: [0, 5, 8, 9], 3: [3, 6, 7], 4: [4], 2: [2], 1: [1]}\n",
      "[  0   0   0 ... 299 299 299]\n",
      "Expert 0 learned classes: [10, 14, 23, 30, 50, 76, 87, 90, 92, 99, 111, 128, 138, 147, 166, 175, 176, 179, 187, 194, 202, 216, 224, 228, 232, 258, 262, 263, 295, 299, 3, 21, 25, 26, 32, 38, 44, 45, 54, 60, 66, 75, 83, 86, 112, 115, 142, 157, 168, 196, 206, 219, 252, 259, 271, 272, 288, 293, 296, 298, 0, 20, 28, 29, 41, 47, 58, 62, 71, 82, 88, 93, 103, 107, 126, 133, 139, 140, 143, 156, 199, 211, 223, 233, 235, 239, 241, 246, 249, 268, 7, 9, 22, 56, 59, 79, 81, 104, 125, 127, 130, 153, 154, 160, 165, 172, 185, 189, 195, 201, 203, 214, 218, 247, 248, 257, 269, 273, 280, 294]\n",
      "Shape: (36452, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 120/120 [00:00<00:00, 255.72class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 0 - Intra-Class Similarity: 0.5428\n",
      "[  1   1   1 ... 297 297 297]\n",
      "Expert 3 learned classes: [4, 12, 31, 55, 61, 65, 69, 70, 77, 91, 100, 101, 119, 121, 132, 136, 150, 151, 163, 181, 184, 204, 236, 243, 256, 275, 276, 277, 283, 297, 1, 13, 35, 68, 98, 102, 109, 116, 122, 123, 134, 141, 144, 155, 167, 171, 174, 178, 188, 208, 209, 217, 229, 234, 242, 253, 255, 265, 267, 274, 19, 37, 40, 46, 51, 53, 63, 64, 73, 113, 118, 129, 146, 148, 158, 169, 186, 198, 205, 212, 220, 230, 237, 238, 251, 264, 279, 282, 290, 292]\n",
      "Shape: (26665, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 90/90 [00:00<00:00, 262.36class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 3 - Intra-Class Similarity: 0.5117\n",
      "[  6   6   6 ... 289 289 289]\n",
      "Expert 4 learned classes: [6, 16, 27, 33, 43, 49, 52, 57, 74, 84, 85, 95, 96, 105, 106, 117, 120, 162, 164, 170, 190, 192, 197, 221, 240, 260, 261, 270, 278, 289]\n",
      "Shape: (9021, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 30/30 [00:00<00:00, 257.41class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 4 - Intra-Class Similarity: 0.5876\n",
      "[  2   2   2 ... 287 287 287]\n",
      "Expert 2 learned classes: [2, 11, 15, 17, 39, 78, 80, 89, 94, 97, 108, 131, 137, 149, 159, 180, 182, 183, 191, 193, 200, 210, 222, 225, 250, 281, 284, 285, 286, 287]\n",
      "Shape: (8993, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 30/30 [00:00<00:00, 263.62class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 2 - Intra-Class Similarity: 0.5503\n",
      "[  5   5   5 ... 291 291 291]\n",
      "Expert 1 learned classes: [5, 8, 18, 24, 34, 36, 42, 48, 67, 72, 110, 114, 124, 135, 145, 152, 161, 173, 177, 207, 213, 215, 226, 227, 231, 244, 245, 254, 266, 291]\n",
      "Shape: (8566, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 30/30 [00:00<00:00, 275.26class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 1 - Intra-Class Similarity: 0.5190\n",
      "Average Expert Similarity: 0.5423\n",
      "Run 26v6mgz7 - Task Distribution: {3: [3], 4: [4], 1: [1, 5, 6], 2: [2, 7, 8, 9], 0: [0]}\n",
      "[  3   3   3 ... 292 292 292]\n",
      "Expert 3 learned classes: [3, 35, 47, 53, 76, 87, 108, 110, 115, 118, 122, 131, 132, 144, 148, 153, 154, 163, 170, 191, 226, 228, 229, 236, 239, 241, 277, 288, 289, 292]\n",
      "Shape: (8769, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 30/30 [00:00<00:00, 264.54class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 3 - Intra-Class Similarity: 0.5484\n",
      "[  7   7   7 ... 299 299 299]\n",
      "Expert 4 learned classes: [7, 10, 12, 13, 14, 40, 50, 57, 69, 79, 90, 106, 107, 116, 135, 142, 171, 184, 198, 203, 205, 210, 222, 223, 238, 262, 269, 270, 281, 299]\n",
      "Shape: (9158, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 30/30 [00:00<00:00, 252.93class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 4 - Intra-Class Similarity: 0.5375\n",
      "[  0   0   0 ... 298 298 298]\n",
      "Expert 1 learned classes: [6, 11, 15, 19, 20, 23, 27, 43, 51, 54, 80, 94, 100, 117, 119, 136, 179, 180, 183, 188, 196, 216, 217, 220, 232, 246, 251, 267, 279, 298, 17, 22, 37, 44, 61, 99, 104, 114, 138, 140, 160, 169, 173, 195, 199, 202, 207, 214, 218, 219, 233, 237, 247, 271, 274, 275, 278, 280, 282, 297, 0, 5, 45, 52, 58, 59, 63, 67, 68, 71, 74, 83, 93, 123, 130, 149, 151, 161, 162, 164, 172, 189, 190, 201, 213, 245, 263, 268, 276, 291]\n",
      "Shape: (27151, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 90/90 [00:00<00:00, 253.11class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 1 - Intra-Class Similarity: 0.5414\n",
      "[  1   1   1 ... 296 296 296]\n",
      "Expert 2 learned classes: [1, 18, 21, 36, 38, 55, 70, 77, 78, 81, 85, 88, 91, 92, 112, 113, 145, 158, 167, 182, 225, 227, 234, 243, 248, 258, 259, 261, 264, 294, 2, 25, 34, 64, 65, 75, 89, 95, 96, 109, 139, 152, 166, 176, 181, 185, 204, 212, 221, 231, 242, 254, 266, 272, 273, 284, 287, 290, 293, 296, 24, 28, 29, 31, 72, 73, 82, 84, 102, 103, 111, 124, 126, 127, 133, 134, 137, 143, 155, 174, 206, 211, 230, 235, 240, 249, 265, 283, 285, 286, 4, 16, 26, 39, 42, 48, 60, 62, 86, 101, 105, 120, 121, 128, 129, 141, 146, 147, 175, 177, 186, 197, 208, 224, 250, 253, 256, 257, 260, 295]\n",
      "Shape: (35984, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 120/120 [00:00<00:00, 256.68class/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 2 - Intra-Class Similarity: 0.5348\n",
      "[  8   8   8 ... 255 255 255]\n",
      "Expert 0 learned classes: [8, 9, 30, 32, 33, 41, 46, 49, 56, 66, 97, 98, 125, 150, 156, 157, 159, 165, 168, 178, 187, 192, 193, 194, 200, 209, 215, 244, 252, 255]\n",
      "Shape: (8635, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Intra-Class Similarity: 100%|██████████| 30/30 [00:00<00:00, 270.10class/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 0 - Intra-Class Similarity: 0.5143\n",
      "Average Expert Similarity: 0.5353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_42_adapter_performance = [\n",
    "    \"belaschindler-university-hamburg/0schindl-LayUp_sweeps_question1_results/6kim8tiu\", # DIL\n",
    "    \"belaschindler-university-hamburg/0schindl-LayUp_sweeps_question1_results/jdpa9z1x\", # Cars\n",
    "    \"belaschindler-university-hamburg/0schindl-LayUp_sweeps_question1_results/cjddpel4\", # Imagenet-a\n",
    "    \"belaschindler-university-hamburg/0schindl-LayUp_sweeps_question1_results/hxigp6ck\", # Imagenet-r\n",
    "    \"belaschindler-university-hamburg/0schindl-LayUp_sweeps_question1_results/p7zmthx9\", # CIL\n",
    "    ]\n",
    "datsets_in_CIL = [\"cifar100\", \"cub\", \"vtab\", \"omnibenchmark\"]\n",
    "\n",
    "loaded_labels = np.array(loaded_labels)\n",
    "loaded_features = np.array(loaded_features)\n",
    "\n",
    "table_421 = []\n",
    "for i, s in enumerate(_42_adapter_performance):\n",
    "    sweep = api.sweep(s)\n",
    "    runs = sweep.runs\n",
    "\n",
    "    attributes_config = [\"dataset\", \"selection_method\", \"seed\"]\n",
    "    attributes_summary = [\"task_mean/acc\"]\n",
    "\n",
    "    data = get_sweep_data(runs, attributes_config, attributes_summary, class_similarity=True)\n",
    "    for e in data:\n",
    "        if e[\"dataset\"] == DATASET:\n",
    "            if i == 4:\n",
    "                # CIL\n",
    "                if e[\"dataset\"] in datsets_in_CIL:\n",
    "                    table_421.append(e)\n",
    "            else:\n",
    "                table_421.append(e)\n",
    "    \n",
    "\n",
    "\n",
    "df_sweep = pd.DataFrame(table_421)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "40f7180b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     run_id        dataset selection_method  seed  task_mean/acc  \\\n",
      "0  6dwyk5lm  omnibenchmark       inv_ws_div  2000       0.573589   \n",
      "1  7z247qq2  omnibenchmark       inv_ws_div  2004       0.581586   \n",
      "2  5b5h2uzl  omnibenchmark       inv_ws_div  2003       0.597296   \n",
      "3  fs3coo40  omnibenchmark       inv_ws_div  2002       0.585258   \n",
      "4  26v6mgz7  omnibenchmark       inv_ws_div  2001       0.591332   \n",
      "\n",
      "                                  expert_similaritys  \\\n",
      "0  [0.5415225760440951, 0.5496745796154197, 0.537...   \n",
      "1  [0.4892236990547385, 0.5505489179248585, 0.536...   \n",
      "2  [0.4966600350786431, 0.5529010815099288, 0.557...   \n",
      "3  [0.5428289046490888, 0.5117119500755918, 0.587...   \n",
      "4  [0.5483583799959261, 0.5374967049866456, 0.541...   \n",
      "\n",
      "   average_expert_similarity  \n",
      "0                   0.531002  \n",
      "1                   0.533287  \n",
      "2                   0.533252  \n",
      "3                   0.542283  \n",
      "4                   0.535253  \n"
     ]
    }
   ],
   "source": [
    "data = df_sweep\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dbb137",
   "metadata": {},
   "source": [
    "### CARS\n",
    "\n",
    "   average_expert_similarity  \n",
    "0                   0.673711  \n",
    "1                   0.666535  \n",
    "2                   0.668896  \n",
    "3                   0.672675  \n",
    "4                   0.666224  \n",
    "\n",
    "### CIFAR\n",
    "0                   0.477616  \n",
    "1                   0.471247  \n",
    "2                   0.473173  \n",
    "3                   0.477304  \n",
    "4                   0.474774  \n",
    "\n",
    "\n",
    "### CUB\n",
    "0                   0.629865  \n",
    "1                   0.636929  \n",
    "2                   0.630805  \n",
    "3                   0.623717  \n",
    "4                   0.638992  \n",
    "\n",
    "### IN-A\n",
    "0                   0.179068  \n",
    "1                   0.181273  \n",
    "2                   0.184007  \n",
    "3                   0.185891  \n",
    "4                   0.184179  \n",
    "\n",
    "### IN-R\n",
    "0                   0.268898  \n",
    "1                   0.274224  \n",
    "2                   0.276426  \n",
    "3                   0.280183  \n",
    "4                   0.278103  \n",
    "\n",
    "## OB\n",
    "0                   0.531002  \n",
    "1                   0.533287  \n",
    "2                   0.533252  \n",
    "3                   0.542283  \n",
    "4                   0.535253  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c9a032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "999cd084",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab882a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da0abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                # Calculate inter-class similarity for each expert\n",
    "                expert_similaritys = list()\n",
    "                for expert_id, learned_classes in task_distribution.items():\n",
    "                    if len(learned_classes) > 1:\n",
    "                        label_mask = np.isin(loaded_labels, learned_classes)\n",
    "\n",
    "                        # Verwende die Maske, um die entsprechenden Features und Labels auszuwählen\n",
    "                        expert_features = loaded_features[label_mask]\n",
    "                        expert_labels = loaded_labels[label_mask]\n",
    "                        print(f\"Expert {expert_id} learned classes: {learned_classes}\")\n",
    "                        print(f\"Shape: {expert_features.shape}\")\n",
    "                        similarity = calculate_inter_class_similarity_vectorized(expert_features, expert_labels)\n",
    "                        print(f\"Expert {expert_id} - Inter-Class Similarity: {similarity:.4f}\")\n",
    "                        expert_similaritys.append(similarity)\n",
    "\n",
    "                run_data[\"expert_similaritys\"] = expert_similaritys\n",
    "                average_similarity = np.mean(expert_similaritys) if expert_similaritys else 0.0       \n",
    "                run_data[\"average_expert_similarity\"] = average_similarity\n",
    "                print(f\"Average Expert Similarity: {average_similarity:.4f}\")\n",
    "                print(task_distribution)\n",
    "\n",
    "\n",
    "                # inter-class similarity to all classes except the one where their expert only learned this one class\n",
    "                lonly_learned_classes = []\n",
    "                for expert_id, learned_classes in task_distribution.items():\n",
    "                    if len(learned_classes) == 1:\n",
    "                        lonly_learned_classes.append(learned_classes[0])\n",
    "                print(f\"Classes where expert only learned this class: {lonly_learned_classes}\")\n",
    "\n",
    "                # Calculate inter-class similarity to all other classes\n",
    "                other_classes = [label for label in loaded_labels if label != lonly_learned_classes]\n",
    "                label_mask = np.isin(loaded_labels, other_classes)\n",
    "\n",
    "                expert_features = loaded_features[label_mask]\n",
    "                expert_labels = loaded_labels[label_mask]\n",
    "                similarity = calculate_inter_class_similarity_vectorized(expert_features, expert_labels)\n",
    "                run_data[\"filtered_dataset_similarity\"] = similarity\n",
    "                print(f\"Dataset similarity: {similarity:.4f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
